{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":1,"renderable":1},{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"source/img/bak/hxh_n.jpg","path":"img/bak/hxh_n.jpg","modified":1,"renderable":0},{"_id":"source/img/bak/hxh_s.jpg","path":"img/bak/hxh_s.jpg","modified":1,"renderable":0},{"_id":"source/img/life/avatar.gif","path":"img/life/avatar.gif","modified":1,"renderable":0},{"_id":"source/img/life/favicon.ico","path":"img/life/favicon.ico","modified":1,"renderable":0},{"_id":"source/img/life/njupt-20170228.jpg","path":"img/life/njupt-20170228.jpg","modified":1,"renderable":0},{"_id":"source/img/life/天池20170228.jpg","path":"img/life/天池20170228.jpg","modified":1,"renderable":0},{"_id":"source/img/work/14853239880359.jpg","path":"img/work/14853239880359.jpg","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-clear-20170212.png","path":"img/work/Buffer-clear-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-flip-20170212.png","path":"img/work/Buffer-flip-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-get-20170212.png","path":"img/work/Buffer-get-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-process-20170212.jpeg","path":"img/work/Buffer-process-20170212.jpeg","modified":1,"renderable":0},{"_id":"source/img/work/Threadlife_20170208.jpg","path":"img/work/Threadlife_20170208.jpg","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-put-20170212.png","path":"img/work/Buffer-put-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/catalog_20170204.png","path":"img/work/catalog_20170204.png","modified":1,"renderable":0},{"_id":"source/img/work/channels-buffers-20170212.png","path":"img/work/channels-buffers-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka-producer-20170213.png.png","path":"img/work/kafka-producer-20170213.png.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka-tupe-20170213.png","path":"img/work/kafka-tupe-20170213.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka_log_anatomy_20170213.png","path":"img/work/kafka_log_anatomy_20170213.png","modified":1,"renderable":0},{"_id":"source/img/work/rpc-20170302.jpg","path":"img/work/rpc-20170302.jpg","modified":1,"renderable":0},{"_id":"source/img/work/selectors-20170212.png","path":"img/work/selectors-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/shuffle-write-no-consolidation_20170206.png","path":"img/work/shuffle-write-no-consolidation_20170206.png","modified":1,"renderable":0},{"_id":"source/img/work/sparkRdd_20170207.jpg","path":"img/work/sparkRdd_20170207.jpg","modified":1,"renderable":0},{"_id":"source/img/work/yarn-progress-20170216.png","path":"img/work/yarn-progress-20170216.png","modified":1,"renderable":0},{"_id":"source/img/work/yarn-structure-20170216.png","path":"img/work/yarn-structure-20170216.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/Java-NIO-ByteBuffer概述.md","hash":"4ff79b026674ad4d73091d989d8b45ad6b49cafe","modified":1712475775398},{"_id":"source/favicon.ico","hash":"dbf6bc9779822c62ba9ba55428b88f69aff93958","modified":1712475775405},{"_id":"source/_posts/Java-NIO简述.md","hash":"800544a653a076e32c809f4f90f0dede85ca3188","modified":1712475775398},{"_id":"source/_posts/Kafka-概述.md","hash":"a5bb703e7868a3fbfe91b4976a6a2eecec2f6fb7","modified":1712475775399},{"_id":"source/_posts/Java基础-JVM内存模型.md","hash":"f115aafd5c07e2b442d89c9afff5eb717efd1c3c","modified":1712476369495},{"_id":"source/_posts/Java基础-JVM类加载.md","hash":"8ab945f3748ff35e2eca904d3312194efda4fb9f","modified":1712476369495},{"_id":"source/_posts/Java基础-Thread概述.md","hash":"fde099a5e66376f33149dd4de90e8d33ed084ce6","modified":1712476369496},{"_id":"source/_posts/RPC原理-概述.md","hash":"aac95a0373575f2a96e77cae171a9310d91a7527","modified":1712475775400},{"_id":"source/_posts/Spark-RDD详解.md","hash":"d90ff4e64eae1741a27f5f6bb16a655c1ce042d3","modified":1712475775400},{"_id":"source/_posts/Spring-boot-MyBatis配置-1.md","hash":"e4f928aea74d9cb1eef04132bb940ed83fa9309b","modified":1712475775402},{"_id":"source/_posts/Spark-Shuffle基础.md","hash":"d4894c4694f78d8a61cc9870656b64659a2dbd3d","modified":1712475775400},{"_id":"source/_posts/SparkStream-函数详解-Transformations.md","hash":"b685bc6e6fc6989f31dd25c214fdcde8452645d3","modified":1712475775401},{"_id":"source/_posts/Spring-Boot-入门学习.md","hash":"e46b77e72d48c8a4fd30281d32f46a0d7792a1a0","modified":1712475775402},{"_id":"source/_posts/Spring-boot-MyBatis配置-2.md","hash":"11d8591b81b173172a44242a91f0914790520a7c","modified":1712475775403},{"_id":"source/_posts/TensorFlow-损失函数.md","hash":"e5f2dd1adcc1d8d8e1ed03bb83fe166a95326f1f","modified":1712476369496},{"_id":"source/_posts/TensorFlow-激活函数.md","hash":"68be26f8a75526afecbb827d272a9cb05ab7d336","modified":1712476369497},{"_id":"source/_posts/Tensorflow 入门-1.md","hash":"b8bcdeb036c92058cc4bdac9dbc4dbe4d9d66d80","modified":1712476369500},{"_id":"source/_posts/Tensorflow-卷积神经网络.md","hash":"c81cf0bdc7eb6aa78aaf75f89b83b4666203af0d","modified":1712476369501},{"_id":"source/_posts/go-ethereum-简单搭建私有链.md","hash":"4662c4c0637eed717d7c23ea7e9b29e7912623ab","modified":1712475775404},{"_id":"source/_posts/Yarn-概述.md","hash":"b239e5475c49ef2697c5658099c3b41e3c986e7d","modified":1712475775403},{"_id":"source/categories/index.md","hash":"2b17edd31de71c4666e55d343a5e8e6db7478e73","modified":1712476369501},{"_id":"source/about/index.md","hash":"b29831930eacd43a267a4cb2f6b1d3a21389a8b3","modified":1712475775404},{"_id":"source/tags/index.md","hash":"f66208f9d7965b40cdedfa200e132158e2321e2e","modified":1712476369502},{"_id":"source/img/life/avatar.gif","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712475775409},{"_id":"source/img/bak/hxh_s.jpg","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712475775409},{"_id":"source/img/life/favicon.ico","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712475775409},{"_id":"source/img/life/njupt-20170228.jpg","hash":"1ddcbb410ae2b1b8e0f9946927671fccd2bd3103","modified":1712475775411},{"_id":"source/img/work/Buffer-clear-20170212.png","hash":"cafb60b7e6a7516dd18dc38c1676f47572d034ff","modified":1712475775428},{"_id":"source/img/work/14853239880359.jpg","hash":"8ba6fcca9fc03ee5b99ec646b8d0d1ba61cc10dd","modified":1712475775427},{"_id":"source/img/work/Buffer-flip-20170212.png","hash":"d710f9be2e24d076190ada8280be3b2d419d94e2","modified":1712475775429},{"_id":"source/img/work/Buffer-get-20170212.png","hash":"f44111acd754300cc53329926190d5129648c4af","modified":1712475775430},{"_id":"source/img/work/Threadlife_20170208.jpg","hash":"27b8beef46f4b98e2913ae7c1196da40230763a3","modified":1712475775432},{"_id":"source/img/work/Buffer-process-20170212.jpeg","hash":"2610f8916944b59c7859aabf4df981007d42c2ef","modified":1712475775431},{"_id":"source/img/work/channels-buffers-20170212.png","hash":"91767a1f3c322cf4ff1936c24143724195b54fc9","modified":1712475775435},{"_id":"source/img/work/kafka-producer-20170213.png.png","hash":"d418cb89944102d0875ce7d0aaf1068ad616a651","modified":1712475775436},{"_id":"source/img/work/Buffer-put-20170212.png","hash":"177eadfa459694390c3d4accffa40e0304755243","modified":1712475775431},{"_id":"source/img/work/rpc-20170302.jpg","hash":"aa10f59910b269e4a0de75a828befb00a8f37ea7","modified":1712475775439},{"_id":"source/img/work/kafka_log_anatomy_20170213.png","hash":"7d387eec0de1ebfbcd98a3b0c87aa008b2dbe476","modified":1712475775438},{"_id":"source/img/work/selectors-20170212.png","hash":"d2019fc0847fad5488c199d4f6bb07a3dceba7ce","modified":1712475775439},{"_id":"source/img/work/sparkRdd_20170207.jpg","hash":"356667fdad339020fb9c94358ee46fe11deec498","modified":1712475775444},{"_id":"source/img/work/yarn-structure-20170216.png","hash":"811a3c06fe189b16a269b0cfc1236e9af6452748","modified":1712475775446},{"_id":"source/img/work/yarn-progress-20170216.png","hash":"2b5cc0b246b24e89a224a9fa2b5abdd1968a6847","modified":1712475775446},{"_id":"source/img/bak/hxh_n.jpg","hash":"251a9630d76739e2ef3489f3c9d2b4d922278f72","modified":1712475775408},{"_id":"source/img/work/catalog_20170204.png","hash":"af1d55a452c12677971ae42b643000d453e3804f","modified":1712475775435},{"_id":"source/img/work/kafka-tupe-20170213.png","hash":"24d5b6c8a7e0ecd183f6225a8038846aa8e3efaf","modified":1712475775438},{"_id":"source/img/work/shuffle-write-no-consolidation_20170206.png","hash":"6dd2872aa672f77e9b7e762f71f1733934e825cc","modified":1712475775443},{"_id":"themes/butterfly/package.json","hash":"314b0271ba3f668d0d6081b499b2d24e90dab25e","modified":1712479036730},{"_id":"themes/butterfly/.DS_Store","hash":"30101bd3277f4f223ba31df2ef0e2eac678160c3","modified":1712479812415},{"_id":"themes/butterfly/README_CN.md","hash":"148da187d16033624ceccce8b8561835296f5a5a","modified":1712479036729},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1712479036573},{"_id":"themes/butterfly/README.md","hash":"4e01b47448d9f3a02afc04eef644e2321253f6f4","modified":1712479036728},{"_id":"themes/butterfly/languages/default.yml","hash":"90a6dc361de67532437d819a55ec64945ca5404b","modified":1712479036723},{"_id":"themes/butterfly/plugins.yml","hash":"d807fbb62163bb6fc5a83a24ebd69ac14cf45f67","modified":1712479036731},{"_id":"themes/butterfly/_config.yml","hash":"39fbabbbf1e89dd82a6ea21517c6c24b49e4dd98","modified":1712479036731},{"_id":"themes/butterfly/.github/FUNDING.yml","hash":"da5e77f5e0cdb7e11b36546fb6796d10e3dfbe5d","modified":1712479036761},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"03629d1d13a7be09d4933aa5dc0dcbe45e79140c","modified":1712479036726},{"_id":"themes/butterfly/languages/en.yml","hash":"af5603b1a888f167dc80be6d53a19437b5cf6bef","modified":1712479036725},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"5004faee365139521f161babd66649a8107e4008","modified":1712479036727},{"_id":"themes/butterfly/layout/archive.pug","hash":"bb32c9c476372de747dfa563b83f77d7a917a77d","modified":1712479036575},{"_id":"themes/butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1712479036666},{"_id":"themes/butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1712479036577},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1712479036576},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1712479036666},{"_id":"themes/butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1712479036576},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"6e34b565ea013812d5e363b6de5fa1f9078d4e12","modified":1712479036765},{"_id":"themes/butterfly/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1712479812411},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":1712479036764},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1712479036762},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7b4831ae8f8f8c55dd1b856781210c517c63e6dd","modified":1712479036766},{"_id":"themes/butterfly/.github/workflows/stale.yml","hash":"ac62b989b5550c756e1986fcc68f243170705383","modified":1712479036763},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1712479036610},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1712479036600},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"ecec62305aaa596bb1dfbb46c13d06fb5a9628cf","modified":1712479036604},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"7fa9ae4b70b87fc97e992dde5944681f92b59bea","modified":1712479036611},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"aca0ec7ef69b21d1f242c62fed389468a0f0e1a2","modified":1712479036603},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"860f967ecf3c6a6ea785b560a7aae4d0757cd18a","modified":1712479036735},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"a8eef3f37428436554f58a2b6bac7c255fbdf38d","modified":1712479036735},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1712479036737},{"_id":"themes/butterfly/scripts/events/cdn.js","hash":"21fb5aabe043486d095c4c8cce361ed85ba88a26","modified":1712479036743},{"_id":"themes/butterfly/scripts/events/comment.js","hash":"5351e0bc09e6b5b3f6d30f333a2520626a28ca3a","modified":1712479036738},{"_id":"themes/butterfly/scripts/events/init.js","hash":"428b94c7b9e83f7ea36227dee66bfe3c23aee4a8","modified":1712479036739},{"_id":"themes/butterfly/scripts/events/merge_config.js","hash":"2ac43fd4103ba3c6897da7c13015cb05f39fd695","modified":1712479036742},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"8ad9911b755cba13dde2cc055c3f857a6b0dd20e","modified":1712479036740},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1712479036758},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"96f861151e3b889ef0ffe78821d489ad2625ee43","modified":1712479036758},{"_id":"themes/butterfly/scripts/events/stylus.js","hash":"e196a99733d7f90899bceed5d12488e8234817d5","modified":1712479036741},{"_id":"themes/butterfly/scripts/helpers/findArchiveLength.js","hash":"7caf549810f971c34196fb9deac2d992545bdff9","modified":1712479036757},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"d5c7e61257b08a9648404f6f48ce4d471cd5fa55","modified":1712479036759},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"e2a8a09bfe47da26eab242a36f516e6c452c799a","modified":1712479036755},{"_id":"themes/butterfly/scripts/helpers/series.js","hash":"821e973d41f7b3b64cde91e0e836ea49c43e3c06","modified":1712479036756},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"93229d44b35b9da92e647b89d6d3087085974a29","modified":1712479036751},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"db275f7fbe4438b54cd813b695f4834e10aa234f","modified":1712479036603},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"4677be4175da6800c0b3b8c1614e593f73df8831","modified":1712479036754},{"_id":"themes/butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1712479036746},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"418684993a3a3a2ac534257a2d9ecbcead6808c1","modified":1712479036748},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"9f0e9e039f304439007460fa0a7c8ac18e0ffd37","modified":1712479036661},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"4c85de4dea4dca4e5088097a79bd6d7009cbf8ef","modified":1712479036583},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"365db87ddfc582bf8c15cb440c48bed95106e4b1","modified":1712479036752},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"19773218877281ccffed921431e87148413a7c20","modified":1712479036753},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"5c2a07df5874b5377540884e4da14dd21489378f","modified":1712479036744},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"1acefc59ead75ebd8cafee36efc7da4fa426d088","modified":1712479036750},{"_id":"themes/butterfly/scripts/tag/score.js","hash":"5cb273e95846874e3a58074074c501df23c5e912","modified":1712479036749},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"512c68a22ae4a58d6a6b24b368a0c00c2ccb4fcb","modified":1712479036745},{"_id":"themes/butterfly/scripts/tag/series.js","hash":"830b1d592278b9f676df0cf9a91b1eeda2456aec","modified":1712479036746},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"ffc62222f8d7b4d44c1c0726c8a08824a2201039","modified":1712479036747},{"_id":"themes/butterfly/scripts/tag/timeline.js","hash":"4526c75e5bf84609d67e92b6af3524bcb278e852","modified":1712479036748},{"_id":"themes/butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1712479036720},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1712479036719},{"_id":"themes/butterfly/source/css/index.styl","hash":"755490867fd8afe47d5cce24faea2ca172b0c4dd","modified":1712479036686},{"_id":"themes/butterfly/source/css/var.styl","hash":"152b6bd4b6285165541a71f5a1c913f8ee6a602b","modified":1712479036681},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"f8d2e3f31468991a7f5171cbfdb157dfb86d3372","modified":1712479036716},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1712479036721},{"_id":"themes/butterfly/source/js/main.js","hash":"0dac585446445e0c419b86eec5580bc9b0657dc6","modified":1712479036714},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"8aa8d799aedbfd811195b84a451bc4b6e2647c12","modified":1712479036605},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"67e1c3b48e4ca7ee0b2c76d3ca7476b9883cf105","modified":1712479036608},{"_id":"themes/butterfly/source/js/utils.js","hash":"8e6b48d294e7aeaba8ff6348c43b2271cf865547","modified":1712479036715},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"63fed4548367a3663cdbaffa1df48167b0a2397b","modified":1712479036606},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1712479036609},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1712479036609},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1712479036607},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"5208fe1e75d97a05fd9bdd6cc53c59d8b741b94b","modified":1712479036607},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1712479036610},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1712479036664},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"f50e6a17073677933c5bc78481bf587a4a9e6da0","modified":1712479036664},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"944d6e9dd50df3395f3a2c7ad9db667d50dea4ed","modified":1712479036663},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"f61659aa457d1a2d1baa3a13157996cfac4d6609","modified":1712479036662},{"_id":"themes/butterfly/layout/includes/loading/fullpage-loading.pug","hash":"9e8c5788602b29a527ef35fe8a20076a5fa969bd","modified":1712479036589},{"_id":"themes/butterfly/layout/includes/loading/index.pug","hash":"131f344d68b4c241d6e03849b243ee792fcd3cea","modified":1712479036587},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"6f310ca7b392021632b987557607d5b6d18052bb","modified":1712479036602},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"5de9a82032cdad1db3b868b797460921cd775fc2","modified":1712479036665},{"_id":"themes/butterfly/layout/includes/loading/pace.pug","hash":"6ab4e301c92586505d6cddce1b3ad23b7c79010d","modified":1712479036588},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"90554c2ca5ba946f4c02e1bc5fe2859cef1b1594","modified":1712479036601},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"9621991359e22b14049346f1cf87bdedc94edf5a","modified":1712479036581},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1712479036580},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1712479036582},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"f9ce83978b217a71a2eb8761dc14b09866faa3f4","modified":1712479036580},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"5574804fdea5edf7fd52aad2caf030614d5e7f2f","modified":1712479036585},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"a096ff8eb6b2a22395be881f827ff2a686ba5596","modified":1712479036586},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1712479036625},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"1d39670ee6225f85f5c53bf5c84f3fd6e19290e8","modified":1712479036634},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1712479036648},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"8044b9c18b34b019ffe26b7383e7e80356b5e4b5","modified":1712479036633},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1712479036651},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"12e57491e94fa00d953bbda9db0b6d6169e2358c","modified":1712479036658},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1712479036596},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"03c6afabbf1ac729c7fb21c7ec06da0190b0fdc7","modified":1712479036598},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1712479036595},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1712479036592},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1712479036594},{"_id":"themes/butterfly/layout/includes/widget/card_post_series.pug","hash":"bd5ad01277f8c6ddf8a3a29af1518e5fe6eed23f","modified":1712479036593},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1712479036598},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"7834bf7c711e739fd33cfcd0b53d151013b3d449","modified":1712479036597},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1712479036599},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"e5aac7b28ed4123d75797263c64e74ac547945bc","modified":1712479036591},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"a658a274c5f7896ee5122725bee45548693bdd66","modified":1712479036590},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"eceb4420a64c720f0d2741e89d6229bbb3d87353","modified":1712479036596},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"35ce167c5a275211bfc1fa3d49adfde5b404d98f","modified":1712479036594},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"66f7a8b0cebc05c575ec3cb70b08d6854029d87a","modified":1712479036592},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"c8ff6ddd5bfe1190b7b8056b68ce41114fd79dcb","modified":1712479036680},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"f19694a42dbe28eda4b39a1696e8fbcd277bc76c","modified":1712479036680},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"4dcd468e4d11a0ac75406162678feffcd89fee00","modified":1712479036709},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1712479036710},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"134811b2d696f9ed2c0cd578f3886f1c60770c0a","modified":1712479036690},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"fad650f88778b33a6358e38cf50dfafc0974d28f","modified":1712479036694},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"f9a5d3f1fc5ed0ed2ee4c1eaa58ed650d11ddebd","modified":1712479036695},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"ac2aeee9926f75b2a0098efe1c114126987430f2","modified":1712479036691},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"18d08be0cd9b1f8c049d4b922e80f8163a55c947","modified":1712479036695},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"83a7a70eb0532ea9c4267939fe484af915fca01e","modified":1712479036691},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1712479036689},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1712479036689},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"d6cf26ffb8a0343eda1cde65b6b73b0ddbe8fcfc","modified":1712479036693},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"b7a6a585dbc38d177c9aba75df3a467415d0488a","modified":1712479036693},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"f845b9b4efdee750f70c023aab27432611f83059","modified":1712479036687},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"a2eb44fa5eaea1325319a2064439cf36d0f35a2f","modified":1712479036688},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"5556c9bf4f53a90cb9b4945cd76a8849bd67f3f3","modified":1712479036692},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1712479036670},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"c9e98027f2dd730ce389c2047f62ebb748955fcf","modified":1712479036674},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"0db591a1f4ed5adcb8668a549bbee5c9d62682cf","modified":1712479036678},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1712479036675},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"e549d24ad81a7d93326a509ff8dcfcc58c80729e","modified":1712479036677},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"d4ebc41b5c855dd75f47de7345d62f85ce7cf073","modified":1712479036672},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1712479036671},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1712479036673},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"4e320e16d49bc18085045937681f7331a1e243ca","modified":1712479036676},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"20a3134e1302b62bfc881f4ec43f398267111f22","modified":1712479036684},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1712479036698},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"649a054e73278b6724bd4dd9b94724791ec5c928","modified":1712479036685},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"961589da3c0a532c4709a4a4ea96bd579257f766","modified":1712479036683},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1712479036701},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"5cddbb5f4eae695a26685e415d821b523e0f17bf","modified":1712479036702},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1712479036700},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1712479036700},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"2d02e52b360f6e6cae47c293ae57ed78e2554663","modified":1712479036697},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"909bb5079b26b6ee68177919f522566503654058","modified":1712479036703},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1712479036699},{"_id":"themes/butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1712479036698},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"e1f60ebac53a3f596fd0a4769b4f9275c48c6542","modified":1712479036713},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1712479036711},{"_id":"themes/butterfly/layout/includes/third-party/abcjs/abcjs.pug","hash":"f7299f9fef5bf94bb58c8cd3be8ee660ad2f9cd4","modified":1712479036651},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"108988d046da9a4716148df43b3975217c8ceaae","modified":1712479036714},{"_id":"themes/butterfly/layout/includes/third-party/abcjs/index.pug","hash":"f58f1648d2d71311bafca4833f20b605bb5f18c8","modified":1712479036650},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"71af0b679e00290b0854384368b3c7e9b3e5f26a","modified":1712479036657},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"b2d274db84ef22fbd6d5ea8f4404821898934209","modified":1712479036653},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":1712479036656},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"0344477a2cf38698318ead2681c63ac12f01586e","modified":1712479036655},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"3a5ccfc69bd8ccb4b8f3ce3502023f7914f2a022","modified":1712479036654},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"39427e107230a10790972349c9dd4c4f31d55eb7","modified":1712479036654},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"ddce8352b371a1fb426bdb6c33f587eb37a69647","modified":1712479036627},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"56c028ba0ea8fac19f0125114d765dfc56ce2b48","modified":1712479036657},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"9b57a8e13de8fc51a5f550854e47164fd8ac1be8","modified":1712479036629},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"2fb098a7aa45010a8cd212dc0bd5308c6e7c63e3","modified":1712479036630},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"618e1b7f9204049b07beb9e1363c844a78a9ace3","modified":1712479036628},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"c5f7081ca29db8cc80f808dfc29e36d5fa22fd7e","modified":1712479036655},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"dd61eca6e9a45f63e09bdefba89fe285a81ba096","modified":1712479036632},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"3bc4c1b91568561f0491bdac65b75aa0bfd01f27","modified":1712479036616},{"_id":"themes/butterfly/layout/includes/third-party/chat/messenger.pug","hash":"e39a9c37adf4cb15a2ba3b2cc65542ffea88650d","modified":1712479036631},{"_id":"themes/butterfly/layout/includes/third-party/comments/artalk.pug","hash":"f77f0fdeac2bc8a72f71a58f9b75aa39f0a108c8","modified":1712479036621},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"62f16a602e57e5f7f7c5249dd37b42d436dc032a","modified":1712479036617},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"0d378ee8a671982a46213a4bfb223b4f3409aea9","modified":1712479036618},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"46aec6466959baec1c3d71a5dbc510fbeb00c91d","modified":1712479036623},{"_id":"themes/butterfly/layout/includes/third-party/comments/giscus.pug","hash":"2d7b0b09678adba09481e3152e0b32962677f650","modified":1712479036615},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"00ed91c52939b9675b316137f854d13684c895a6","modified":1712479036619},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"a9709905593d960954e2dd572f09f48a6c2b1ef7","modified":1712479036613},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"63cea2b5c8f7b59f5919379d61a2bb2ce8ed7623","modified":1712479036620},{"_id":"themes/butterfly/layout/includes/third-party/comments/remark42.pug","hash":"f15699abb8c7a255aabad0222ae53eee387c66a3","modified":1712479036620},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"1995a654ba7ad62775a0a6e2922209cd1a85f2e3","modified":1712479036616},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"5c29b5887e2e6cd81e1f13b32da53d9c139b788b","modified":1712479036624},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"46865e3f52096acb07d0212174b4e8751b123aea","modified":1712479036614},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"7aa443b4881448979b810864e206e58c9ed787e3","modified":1712479036614},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1712479036636},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"dfcbd9881be569ea420eff1a6b00e4f4dbe2138e","modified":1712479036637},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"6b67982bb7a3713b5bffd6a23ba2810425c504d0","modified":1712479036638},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"fc072ac839401174b5d3cf9acd3b694246c23a55","modified":1712479036636},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"4ec0642f2d5444acfab570a6f8c7868e7ff43fde","modified":1712479036643},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"17080aba1754478197ab089f7948ed900f116d2b","modified":1712479036647},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"2609bc2656aaaa9b59e8d575e711776512a62192","modified":1712479036648},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"0f0b46d637a9a1b6ae35148923abecc80b866276","modified":1712479036645},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"4104f96faa6040f111ebfb9a90eeb470857c3b86","modified":1712479036646},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"24804ab6da9727ed793655c1262fa3f1a9746f70","modified":1712479036643},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"de2c4d02b520dd49a0a59fc0f33295e5bbb2c624","modified":1712479036646},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"1f9f51023e9e33081c2add2ca73643c0edc5e9d5","modified":1712479036644},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"9c3c109a12d2b6916e8b4965cca12f521510ead9","modified":1712479036640},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"3335024ba91f55ccf3858571b7898f46881c455c","modified":1712479036641},{"_id":"themes/butterfly/layout/includes/third-party/search/docsearch.pug","hash":"b928be14d1b47a9fadb1bcc5f5072a7328752d4b","modified":1712479036641},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"a99a41334387ee9a46c6f8e8212331a29a10d159","modified":1712479036639},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1712479036660},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1712479036708},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"3ba49cfe186e9ca05faf9f0d0113611ec47e9b38","modified":1712479036659},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"c7dd2b2ae9b23aa0a60fffd7df9e9f76ef52033e","modified":1712479036661},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1712479036705},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1712479036708},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1712479036706},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"25914321762e30aacc610bc4dfb9de3e1cb556a3","modified":1712479036705},{"_id":"source/img/life/天池20170228.jpg","hash":"b18d8b0e72d86d4516582549a832d9deb9e8285e","modified":1712475775425},{"_id":"public/about/index.html","hash":"31bf9919bedef64866e15ecd803c0d636d65ccdd","modified":1712481396828},{"_id":"public/categories/index.html","hash":"635b5825542a18444ce2161b6647b77e0b403fa8","modified":1712481396828},{"_id":"public/tags/index.html","hash":"e76564f87cb9c0d400e352f5365431d039b6bff3","modified":1712481396828},{"_id":"public/2019/12/06/Tensorflow-卷积神经网络/index.html","hash":"ca6033883d8c55eea4859f13dd0bec998b147e54","modified":1712481396828},{"_id":"public/2018/10/16/TensorFlow-损失函数/index.html","hash":"a9fbbd123f4ab486cd668d879a984bb1ceb33f1b","modified":1712481396828},{"_id":"public/2018/09/22/TensorFlow-激活函数/index.html","hash":"6ba76991c07f94605a7f63848eb0463badc67c38","modified":1712481396828},{"_id":"public/2018/09/03/Tensorflow 入门-1/index.html","hash":"c21edd2e4d3b03ce0bf05ab9ce024b4b3b11633a","modified":1712481396828},{"_id":"public/2018/04/12/Java基础-JVM类加载/index.html","hash":"137c7fc59c3e3c5006ab11406abc355b0aa46d11","modified":1712481396828},{"_id":"public/2018/03/03/Java基础-JVM内存模型/index.html","hash":"ed09c162592570b0f3b979ce924b7e2778263f6c","modified":1712481396828},{"_id":"public/2017/12/05/go-ethereum-简单搭建私有链/index.html","hash":"7a9624d74c0bd896eaedfdbc2fc683c8af924717","modified":1712481396828},{"_id":"public/2017/03/08/Java基础-Thread概述/index.html","hash":"733d0cab6a1686204c1a5c5c16a7aec14854d389","modified":1712481396828},{"_id":"public/2017/03/02/RPC原理-概述/index.html","hash":"18da5f3694a192470ac8814f4e801c35cdc1f41b","modified":1712481396828},{"_id":"public/2017/02/16/Yarn-概述/index.html","hash":"9a13e7af5f7b4d59dd257df3dff2f0b25690737b","modified":1712481396828},{"_id":"public/2017/02/13/Kafka-概述/index.html","hash":"0a8c43a053ebb8c410512c236aa6875c729594f4","modified":1712481396828},{"_id":"public/2017/02/12/Java-NIO-ByteBuffer概述/index.html","hash":"9c0d2d658daf80ad1cd2df5f8bccca9e2db08782","modified":1712481396828},{"_id":"public/2017/02/12/Java-NIO简述/index.html","hash":"4dfde3cf1c08f0c55a5fd56bd7c6c14d11ee7ffa","modified":1712481396828},{"_id":"public/2017/02/07/Spark-RDD详解/index.html","hash":"e26884347645bec461e3be4b2d460bb08717a9e7","modified":1712481396828},{"_id":"public/2017/02/06/Spark-Shuffle基础/index.html","hash":"a0cea762ed8d005621c3d00c51d7f228ffc9f660","modified":1712481396828},{"_id":"public/2017/02/05/SparkStream-函数详解-Transformations/index.html","hash":"1af10d1773832f7a3f5a81be388438f6b222baf7","modified":1712481396828},{"_id":"public/2017/02/04/Spring-boot-MyBatis配置-2/index.html","hash":"a864942e052724e67fd54896f7b3f82d3bb1d23c","modified":1712481396828},{"_id":"public/2017/02/04/Spring-boot-MyBatis配置-1/index.html","hash":"771a11f457941cebae5e4d667878a1c2a027e9e1","modified":1712481396828},{"_id":"public/2017/01/25/Spring-Boot-入门学习/index.html","hash":"1e7b00901ad41e851e0263e731e985842297e163","modified":1712481396828},{"_id":"public/archives/index.html","hash":"d1842bd31a404cd7936d2be7c3e4fc973918771c","modified":1712481396828},{"_id":"public/archives/page/2/index.html","hash":"ee1bd1b7c8690939e4b2f97a1b9f7a5cc8a24158","modified":1712481396828},{"_id":"public/archives/2017/index.html","hash":"fece072d64af7f359b22e839af1779a08771c5e8","modified":1712481396828},{"_id":"public/archives/2017/page/2/index.html","hash":"d5526f2034a52151bcd74a3079083403d3ea411c","modified":1712481396828},{"_id":"public/archives/2017/01/index.html","hash":"662ebc2e90495d796419665ba6f985fad7ce45ec","modified":1712481396828},{"_id":"public/archives/2017/02/index.html","hash":"ef4ced58b42b93022643d4fecfd08583edf85118","modified":1712481396828},{"_id":"public/archives/2017/03/index.html","hash":"2be6e239b6c1a5184776feb4002c5b55a45beb75","modified":1712481396828},{"_id":"public/archives/2017/12/index.html","hash":"42fd37b76edee61d95b7e32c2b288c76f2925be0","modified":1712481396828},{"_id":"public/archives/2018/index.html","hash":"ab0c5b861900ea93638e5447eccec254a67c4f30","modified":1712481396828},{"_id":"public/archives/2018/03/index.html","hash":"e63332198a7395a247855040b7e12b91dc2b2b8c","modified":1712481396828},{"_id":"public/archives/2018/04/index.html","hash":"722b49c5bfbf3eee688fb4d697c29850ff7a4976","modified":1712481396828},{"_id":"public/archives/2018/09/index.html","hash":"f50820d996727f215c53cb70db7e8d570ba7f5f5","modified":1712481396828},{"_id":"public/archives/2019/index.html","hash":"1ec5ce0dc70bf6f935d28e69d890d1b14316303e","modified":1712481396828},{"_id":"public/archives/2018/10/index.html","hash":"10558fcb900db9eb7e27d110b44b37bb0587c930","modified":1712481396828},{"_id":"public/archives/2019/12/index.html","hash":"0a6d7cdbe568a2b7d13fe3921ea660aefd86e79a","modified":1712481396828},{"_id":"public/categories/java/index.html","hash":"d71a2da0983dc28766ce5d91d7ef45850b93144e","modified":1712481396828},{"_id":"public/categories/Java基础/index.html","hash":"c07a8a1f7dd642880053eeee01f47430cc78a375","modified":1712481396828},{"_id":"public/categories/kafka/index.html","hash":"10e1e229e6904c5f5111871f3343f4efcd07046f","modified":1712481396828},{"_id":"public/categories/spark/index.html","hash":"58f744a556d66ea714dbf6b172115437cfdeaa4c","modified":1712481396828},{"_id":"public/categories/tensorflow/index.html","hash":"6a2e531172307c1e97419a5e0ad1fe8244d26e96","modified":1712481396828},{"_id":"public/categories/Big-data/index.html","hash":"bc90b97f2481f39bdf7883140bfa8ff0f1b025f7","modified":1712481396828},{"_id":"public/categories/区块链/index.html","hash":"3e33c02e0050fe2cda584d31791e17d781b939ef","modified":1712481396828},{"_id":"public/index.html","hash":"3ddbf09a30cac2174cb915697ce28ca7b0bd0619","modified":1712481396828},{"_id":"public/page/2/index.html","hash":"c3dc091fe5c814974f88e4f43c85c5ee71a6c0ff","modified":1712481396828},{"_id":"public/tags/java-nio/index.html","hash":"067f36386fc9f4e0ea62f6b0e1c4f5ed3bff164e","modified":1712481396828},{"_id":"public/tags/java/index.html","hash":"b3f03405f4c5ddcc036b3a1ce33648a4eaa4d3f7","modified":1712481396828},{"_id":"public/tags/java基础/index.html","hash":"fa45d3e99c0345a0e367de5dec29b9d4975133ef","modified":1712481396828},{"_id":"public/tags/jvm/index.html","hash":"7b3e7d3d36fe514ddef49d95867ba2ecc98002a6","modified":1712481396828},{"_id":"public/tags/thread/index.html","hash":"416b7b9d9e2c16275374f767b44468e7773e486f","modified":1712481396828},{"_id":"public/tags/Java基础/index.html","hash":"f53b40f24b7bbcc8c2e16765cc59d79bc3f4e95d","modified":1712481396828},{"_id":"public/tags/kafka/index.html","hash":"a1f46bfed8c338fd3df4036319258e609f43508b","modified":1712481396828},{"_id":"public/tags/rpc/index.html","hash":"df72d9cbdfcff6ad61d767206ab4e288881e23f1","modified":1712481396828},{"_id":"public/tags/spark/index.html","hash":"78fae89a743138a2d33a62f9d4b290877a4140dc","modified":1712481396828},{"_id":"public/tags/spring-boot/index.html","hash":"fd5736abeee4c941e5fbd1d766fe409bdfe4cafa","modified":1712481396828},{"_id":"public/tags/spring/index.html","hash":"ffe1937667f13603534898fcc7672efb62365de0","modified":1712481396828},{"_id":"public/tags/tensorflow/index.html","hash":"5b93a9c9012626ee045e7f12ea12e2ae9c347713","modified":1712481396828},{"_id":"public/tags/yarn/index.html","hash":"9cac271caa84504eccdc3e1fa499bce8fb388b60","modified":1712481396828},{"_id":"public/tags/sparkstream/index.html","hash":"b76491ef233f7246a95761913250139d430aade7","modified":1712481396828},{"_id":"public/tags/hadoop/index.html","hash":"c216b13af6df7e3ae66978ae72441157235c8d97","modified":1712481396828},{"_id":"public/tags/以太坊/index.html","hash":"d4062f5ae85227f0d6a161f5bb907730395fbe2a","modified":1712481396828},{"_id":"public/tags/区块链/index.html","hash":"d7ff9257d9ae76e4a346da509b74083a0f21dfcc","modified":1712481396828},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1712481396828},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1712481396828},{"_id":"public/favicon.ico","hash":"dbf6bc9779822c62ba9ba55428b88f69aff93958","modified":1712481396828},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1712481396828},{"_id":"public/img/bak/hxh_s.jpg","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712481396828},{"_id":"public/img/life/avatar.gif","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712481396828},{"_id":"public/img/life/favicon.ico","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1712481396828},{"_id":"public/img/life/njupt-20170228.jpg","hash":"1ddcbb410ae2b1b8e0f9946927671fccd2bd3103","modified":1712481396828},{"_id":"public/img/work/14853239880359.jpg","hash":"8ba6fcca9fc03ee5b99ec646b8d0d1ba61cc10dd","modified":1712481396828},{"_id":"public/img/work/Buffer-clear-20170212.png","hash":"cafb60b7e6a7516dd18dc38c1676f47572d034ff","modified":1712481396828},{"_id":"public/img/work/Buffer-flip-20170212.png","hash":"d710f9be2e24d076190ada8280be3b2d419d94e2","modified":1712481396828},{"_id":"public/img/work/Buffer-process-20170212.jpeg","hash":"2610f8916944b59c7859aabf4df981007d42c2ef","modified":1712481396828},{"_id":"public/img/work/Buffer-get-20170212.png","hash":"f44111acd754300cc53329926190d5129648c4af","modified":1712481396828},{"_id":"public/img/work/Threadlife_20170208.jpg","hash":"27b8beef46f4b98e2913ae7c1196da40230763a3","modified":1712481396828},{"_id":"public/img/work/Buffer-put-20170212.png","hash":"177eadfa459694390c3d4accffa40e0304755243","modified":1712481396828},{"_id":"public/img/work/channels-buffers-20170212.png","hash":"91767a1f3c322cf4ff1936c24143724195b54fc9","modified":1712481396828},{"_id":"public/img/work/kafka-producer-20170213.png.png","hash":"d418cb89944102d0875ce7d0aaf1068ad616a651","modified":1712481396828},{"_id":"public/img/work/kafka_log_anatomy_20170213.png","hash":"7d387eec0de1ebfbcd98a3b0c87aa008b2dbe476","modified":1712481396828},{"_id":"public/img/work/rpc-20170302.jpg","hash":"aa10f59910b269e4a0de75a828befb00a8f37ea7","modified":1712481396828},{"_id":"public/img/work/selectors-20170212.png","hash":"d2019fc0847fad5488c199d4f6bb07a3dceba7ce","modified":1712481396828},{"_id":"public/img/work/sparkRdd_20170207.jpg","hash":"356667fdad339020fb9c94358ee46fe11deec498","modified":1712481396828},{"_id":"public/img/work/yarn-structure-20170216.png","hash":"811a3c06fe189b16a269b0cfc1236e9af6452748","modified":1712481396828},{"_id":"public/css/index.css","hash":"aec51d3ddd5cf94e4fe4771750edf500be15d523","modified":1712481396828},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1712481396828},{"_id":"public/js/main.js","hash":"0dac585446445e0c419b86eec5580bc9b0657dc6","modified":1712481396828},{"_id":"public/js/tw_cn.js","hash":"f8d2e3f31468991a7f5171cbfdb157dfb86d3372","modified":1712481396828},{"_id":"public/js/search/algolia.js","hash":"108988d046da9a4716148df43b3975217c8ceaae","modified":1712481396828},{"_id":"public/js/utils.js","hash":"8e6b48d294e7aeaba8ff6348c43b2271cf865547","modified":1712481396828},{"_id":"public/js/search/local-search.js","hash":"e1f60ebac53a3f596fd0a4769b4f9275c48c6542","modified":1712481396828},{"_id":"public/img/work/yarn-progress-20170216.png","hash":"2b5cc0b246b24e89a224a9fa2b5abdd1968a6847","modified":1712481396828},{"_id":"public/img/bak/hxh_n.jpg","hash":"251a9630d76739e2ef3489f3c9d2b4d922278f72","modified":1712481396828},{"_id":"public/img/work/catalog_20170204.png","hash":"af1d55a452c12677971ae42b643000d453e3804f","modified":1712481396828},{"_id":"public/img/work/kafka-tupe-20170213.png","hash":"24d5b6c8a7e0ecd183f6225a8038846aa8e3efaf","modified":1712481396828},{"_id":"public/img/work/shuffle-write-no-consolidation_20170206.png","hash":"6dd2872aa672f77e9b7e762f71f1733934e825cc","modified":1712481396828},{"_id":"public/img/life/天池20170228.jpg","hash":"b18d8b0e72d86d4516582549a832d9deb9e8285e","modified":1712481396828}],"Category":[{"name":"java","_id":"clupb7lrk0004obns4ah5fmjd"},{"name":"Java基础","_id":"clupb7lru000fobns46xb51vt"},{"name":"kafka","_id":"clupb7ls1000wobnsclhydulh"},{"name":"spark","_id":"clupb7ls30013obns4n8n8zbr"},{"name":"tensorflow","_id":"clupb7ls6001iobnsgepr34cr"},{"name":"Big data","_id":"clupb7lsh002uobns79ox7lxb"},{"name":"区块链","_id":"clupb7lsi002xobnsgu10c0by"}],"Data":[],"Page":[{"title":"","date":"2017-01-10T01:43:37.000Z","_content":"> 人生每一步都是选择\n\n## 严勇\nyany8060@gmail.com\n目前是一个北漂的程序员，生活比较自由。买自己想买的东西，去自己想去的地方；学自己想学的，做自己想做的。不需要太多的考虑，毕竟我还是单身🐶  ，一人吃饱全家不饿。\n\n喜欢去到处走走，目前北上路线已经完成：\n南京出发=》沈阳、漠河、北极村、哈尔滨、长春、长白山、沈阳\n![天池20170228.jpg](http://upload-images.jianshu.io/upload_images/1419542-03c951bc8de1b8de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500)\n\n### 母校\n南京邮电大学  软件学院— 本科，2011~2015\n大学唯一的遗憾就是单身汪过了四年，想想自己当年too yang too simple\n![njupt-20170228.jpg](http://upload-images.jianshu.io/upload_images/1419542-4d946d673cd5ebe4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500)\n\n\n### 技术栈（熟悉程度不一）：\njava，scala\nspringmvc，MyBatis\nhadoop，hbase，hive，impala，spark，kafka\n\n目前正在从事大数据的相关开发","source":"about/index.md","raw":"---\ntitle: \ndate: 2017-01-10 09:43:37\n---\n> 人生每一步都是选择\n\n## 严勇\nyany8060@gmail.com\n目前是一个北漂的程序员，生活比较自由。买自己想买的东西，去自己想去的地方；学自己想学的，做自己想做的。不需要太多的考虑，毕竟我还是单身🐶  ，一人吃饱全家不饿。\n\n喜欢去到处走走，目前北上路线已经完成：\n南京出发=》沈阳、漠河、北极村、哈尔滨、长春、长白山、沈阳\n![天池20170228.jpg](http://upload-images.jianshu.io/upload_images/1419542-03c951bc8de1b8de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500)\n\n### 母校\n南京邮电大学  软件学院— 本科，2011~2015\n大学唯一的遗憾就是单身汪过了四年，想想自己当年too yang too simple\n![njupt-20170228.jpg](http://upload-images.jianshu.io/upload_images/1419542-4d946d673cd5ebe4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500)\n\n\n### 技术栈（熟悉程度不一）：\njava，scala\nspringmvc，MyBatis\nhadoop，hbase，hive，impala，spark，kafka\n\n目前正在从事大数据的相关开发","updated":"2024-04-07T07:42:55.404Z","path":"about/index.html","comments":1,"layout":"page","_id":"clupb7lrb0000obnsfb7lhs6e","content":"<blockquote>\n<p>人生每一步都是选择</p>\n</blockquote>\n<h2 id=\"严勇\"><a href=\"#严勇\" class=\"headerlink\" title=\"严勇\"></a>严勇</h2><p><a href=\"mailto:&#x79;&#97;&#110;&#x79;&#56;&#x30;&#x36;&#x30;&#x40;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;\">&#x79;&#97;&#110;&#x79;&#56;&#x30;&#x36;&#x30;&#x40;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a><br>目前是一个北漂的程序员，生活比较自由。买自己想买的东西，去自己想去的地方；学自己想学的，做自己想做的。不需要太多的考虑，毕竟我还是单身🐶  ，一人吃饱全家不饿。</p>\n<p>喜欢去到处走走，目前北上路线已经完成：<br>南京出发&#x3D;》沈阳、漠河、北极村、哈尔滨、长春、长白山、沈阳<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-03c951bc8de1b8de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500\" alt=\"天池20170228.jpg\"></p>\n<h3 id=\"母校\"><a href=\"#母校\" class=\"headerlink\" title=\"母校\"></a>母校</h3><p>南京邮电大学  软件学院— 本科，2011~2015<br>大学唯一的遗憾就是单身汪过了四年，想想自己当年too yang too simple<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-4d946d673cd5ebe4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500\" alt=\"njupt-20170228.jpg\"></p>\n<h3 id=\"技术栈（熟悉程度不一）：\"><a href=\"#技术栈（熟悉程度不一）：\" class=\"headerlink\" title=\"技术栈（熟悉程度不一）：\"></a>技术栈（熟悉程度不一）：</h3><p>java，scala<br>springmvc，MyBatis<br>hadoop，hbase，hive，impala，spark，kafka</p>\n<p>目前正在从事大数据的相关开发</p>\n","cover":false,"excerpt":"","more":"<blockquote>\n<p>人生每一步都是选择</p>\n</blockquote>\n<h2 id=\"严勇\"><a href=\"#严勇\" class=\"headerlink\" title=\"严勇\"></a>严勇</h2><p><a href=\"mailto:&#x79;&#97;&#110;&#x79;&#56;&#x30;&#x36;&#x30;&#x40;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;\">&#x79;&#97;&#110;&#x79;&#56;&#x30;&#x36;&#x30;&#x40;&#103;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a><br>目前是一个北漂的程序员，生活比较自由。买自己想买的东西，去自己想去的地方；学自己想学的，做自己想做的。不需要太多的考虑，毕竟我还是单身🐶  ，一人吃饱全家不饿。</p>\n<p>喜欢去到处走走，目前北上路线已经完成：<br>南京出发&#x3D;》沈阳、漠河、北极村、哈尔滨、长春、长白山、沈阳<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-03c951bc8de1b8de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500\" alt=\"天池20170228.jpg\"></p>\n<h3 id=\"母校\"><a href=\"#母校\" class=\"headerlink\" title=\"母校\"></a>母校</h3><p>南京邮电大学  软件学院— 本科，2011~2015<br>大学唯一的遗憾就是单身汪过了四年，想想自己当年too yang too simple<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-4d946d673cd5ebe4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/500\" alt=\"njupt-20170228.jpg\"></p>\n<h3 id=\"技术栈（熟悉程度不一）：\"><a href=\"#技术栈（熟悉程度不一）：\" class=\"headerlink\" title=\"技术栈（熟悉程度不一）：\"></a>技术栈（熟悉程度不一）：</h3><p>java，scala<br>springmvc，MyBatis<br>hadoop，hbase，hive，impala，spark，kafka</p>\n<p>目前正在从事大数据的相关开发</p>\n"},{"title":"categories","date":"2018-09-29T09:07:39.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-09-29 17:07:39\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2024-04-07T07:52:49.501Z","path":"categories/index.html","comments":1,"_id":"clupb7lri0002obnse28w4x84","content":"","cover":false,"excerpt":"","more":""},{"title":"tags","date":"2018-09-29T10:22:35.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-09-29 18:22:35\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2024-04-07T07:52:49.502Z","path":"tags/index.html","comments":1,"_id":"clupb7lrm0006obnsbjfp32me","content":"","cover":false,"excerpt":"","more":""}],"Post":[{"title":"Java NIO ByteBuffer概述","date":"2017-02-12T09:29:27.000Z","_content":"### Buffer的基本用法\n使用Buffer读写数据一般遵循以下四个步骤：\n1、分配空间\n2、写入数据到Buffer\n3、调用flip()方法\n4、从Buffer中读取数据\n5、调用clear()方法或者compact()方法\n当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过`flip()`方法`将Buffer从写模式切换到读模式`。在读模式下，可以读取之前写入到buffer的所有数据。\n一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。`clear()方法会清空整个缓冲区`。`compact()方法只会清除已经读过的数据`。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。\n\n![Buffer-process.jpeg](http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 四个主要属性：\n* capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n* position：\n  * 当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。\n  * 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n* limit：\n  * 在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。\n  * 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\n* mark：为某一读过的位置做标记，便于某些时候回退到该位置。\n\n### Buffer基本函数\n#### Buffer的分配\n```\nByteBuffer buf = ByteBuffer.allocate(48);\n```\n\n#### 向Buffer中写数据\n从Channel写到Buffer的例子\n```\nint bytesRead = inChannel.read(buf); //read into buffer.\n```\n通过put方法写Buffer\n```\nbuf.put(127);\n```\n\n#### 从Buffer中读取数据\n从Buffer读取数据到Channel的例子：\n```\n//read from buffer into channel.\nint bytesWritten = inChannel.write(buf);\n```\n使用get()方法从Buffer中读取数据的例子\n```\nbyte aByte = buf.get();\n```\n#### flip()方法\nflip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。\n换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。\n#### clear()与compact()方法\n一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。\n如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。\ncompact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n\n### 图解ByteBuffer函数过程\n#### put\nWrites the given byte into this buffer at the current position, and then increments the position. \n写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。 \n![Buffer-put.png](http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### flip\nFlips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.\nAfter a sequence of channel-read or `put` operations, invoke this method to prepare for a sequence of channel-write or relative  `get` operations.（即读写模型切换）\n写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion\n```\npublic final Buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n }\n```\n![Buffer-flip.png](http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### get\nReads the byte at this buffer's current position, and then increments the position.\n从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置\n![Buffer-get.png](http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### clear\nClears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.\nInvoke this method before using a sequence of channel-read or `put` operations to fill this buffer.\n将position置为0，并不清除buffer内容。\n```java\n    public final Buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n```\n![Buffer-clear.png](http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n参考：\nhttp://ifeve.com/buffers/\n博客：\nhttp://yany8060.xyz/","source":"_posts/Java-NIO-ByteBuffer概述.md","raw":"---\ntitle: Java NIO ByteBuffer概述\ndate: 2017-02-12 17:29:27\ntags: [java-nio]\ncategories: [java]\n---\n### Buffer的基本用法\n使用Buffer读写数据一般遵循以下四个步骤：\n1、分配空间\n2、写入数据到Buffer\n3、调用flip()方法\n4、从Buffer中读取数据\n5、调用clear()方法或者compact()方法\n当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过`flip()`方法`将Buffer从写模式切换到读模式`。在读模式下，可以读取之前写入到buffer的所有数据。\n一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。`clear()方法会清空整个缓冲区`。`compact()方法只会清除已经读过的数据`。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。\n\n![Buffer-process.jpeg](http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 四个主要属性：\n* capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n* position：\n  * 当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。\n  * 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n* limit：\n  * 在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。\n  * 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\n* mark：为某一读过的位置做标记，便于某些时候回退到该位置。\n\n### Buffer基本函数\n#### Buffer的分配\n```\nByteBuffer buf = ByteBuffer.allocate(48);\n```\n\n#### 向Buffer中写数据\n从Channel写到Buffer的例子\n```\nint bytesRead = inChannel.read(buf); //read into buffer.\n```\n通过put方法写Buffer\n```\nbuf.put(127);\n```\n\n#### 从Buffer中读取数据\n从Buffer读取数据到Channel的例子：\n```\n//read from buffer into channel.\nint bytesWritten = inChannel.write(buf);\n```\n使用get()方法从Buffer中读取数据的例子\n```\nbyte aByte = buf.get();\n```\n#### flip()方法\nflip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。\n换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。\n#### clear()与compact()方法\n一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。\n如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。\ncompact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n\n### 图解ByteBuffer函数过程\n#### put\nWrites the given byte into this buffer at the current position, and then increments the position. \n写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。 \n![Buffer-put.png](http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### flip\nFlips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.\nAfter a sequence of channel-read or `put` operations, invoke this method to prepare for a sequence of channel-write or relative  `get` operations.（即读写模型切换）\n写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion\n```\npublic final Buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n }\n```\n![Buffer-flip.png](http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### get\nReads the byte at this buffer's current position, and then increments the position.\n从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置\n![Buffer-get.png](http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### clear\nClears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.\nInvoke this method before using a sequence of channel-read or `put` operations to fill this buffer.\n将position置为0，并不清除buffer内容。\n```java\n    public final Buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n```\n![Buffer-clear.png](http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n参考：\nhttp://ifeve.com/buffers/\n博客：\nhttp://yany8060.xyz/","slug":"Java-NIO-ByteBuffer概述","published":1,"updated":"2024-04-07T07:42:55.398Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lre0001obnsfxtzhvki","content":"<h3 id=\"Buffer的基本用法\"><a href=\"#Buffer的基本用法\" class=\"headerlink\" title=\"Buffer的基本用法\"></a>Buffer的基本用法</h3><p>使用Buffer读写数据一般遵循以下四个步骤：<br>1、分配空间<br>2、写入数据到Buffer<br>3、调用flip()方法<br>4、从Buffer中读取数据<br>5、调用clear()方法或者compact()方法<br>当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过<code>flip()</code>方法<code>将Buffer从写模式切换到读模式</code>。在读模式下，可以读取之前写入到buffer的所有数据。<br>一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。<code>clear()方法会清空整个缓冲区</code>。<code>compact()方法只会清除已经读过的数据</code>。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-process.jpeg\"></p>\n<h3 id=\"四个主要属性：\"><a href=\"#四个主要属性：\" class=\"headerlink\" title=\"四个主要属性：\"></a>四个主要属性：</h3><ul>\n<li>capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。</li>\n<li>position：<ul>\n<li>当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。</li>\n<li>当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。</li>\n</ul>\n</li>\n<li>limit：<ul>\n<li>在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。</li>\n<li>当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）</li>\n</ul>\n</li>\n<li>mark：为某一读过的位置做标记，便于某些时候回退到该位置。</li>\n</ul>\n<h3 id=\"Buffer基本函数\"><a href=\"#Buffer基本函数\" class=\"headerlink\" title=\"Buffer基本函数\"></a>Buffer基本函数</h3><h4 id=\"Buffer的分配\"><a href=\"#Buffer的分配\" class=\"headerlink\" title=\"Buffer的分配\"></a>Buffer的分配</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuffer buf = ByteBuffer.allocate(48);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"向Buffer中写数据\"><a href=\"#向Buffer中写数据\" class=\"headerlink\" title=\"向Buffer中写数据\"></a>向Buffer中写数据</h4><p>从Channel写到Buffer的例子</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int bytesRead = inChannel.read(buf); //read into buffer.</span><br></pre></td></tr></table></figure>\n<p>通过put方法写Buffer</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">buf.put(127);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"从Buffer中读取数据\"><a href=\"#从Buffer中读取数据\" class=\"headerlink\" title=\"从Buffer中读取数据\"></a>从Buffer中读取数据</h4><p>从Buffer读取数据到Channel的例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//read from buffer into channel.</span><br><span class=\"line\">int bytesWritten = inChannel.write(buf);</span><br></pre></td></tr></table></figure>\n<p>使用get()方法从Buffer中读取数据的例子</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">byte aByte = buf.get();</span><br></pre></td></tr></table></figure>\n<h4 id=\"flip-方法\"><a href=\"#flip-方法\" class=\"headerlink\" title=\"flip()方法\"></a>flip()方法</h4><p>flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。<br>换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。</p>\n<h4 id=\"clear-与compact-方法\"><a href=\"#clear-与compact-方法\" class=\"headerlink\" title=\"clear()与compact()方法\"></a>clear()与compact()方法</h4><p>一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。<br>如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。<br>如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。<br>如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。<br>compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。</p>\n<h3 id=\"图解ByteBuffer函数过程\"><a href=\"#图解ByteBuffer函数过程\" class=\"headerlink\" title=\"图解ByteBuffer函数过程\"></a>图解ByteBuffer函数过程</h3><h4 id=\"put\"><a href=\"#put\" class=\"headerlink\" title=\"put\"></a>put</h4><p>Writes the given byte into this buffer at the current position, and then increments the position.<br>写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-put.png\"></p>\n<h4 id=\"flip\"><a href=\"#flip\" class=\"headerlink\" title=\"flip\"></a>flip</h4><p>Flips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.<br>After a sequence of channel-read or <code>put</code> operations, invoke this method to prepare for a sequence of channel-write or relative  <code>get</code> operations.（即读写模型切换）<br>写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final Buffer flip() &#123;</span><br><span class=\"line\">        limit = position;</span><br><span class=\"line\">        position = 0;</span><br><span class=\"line\">        mark = -1;</span><br><span class=\"line\">        return this;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-flip.png\"></p>\n<h4 id=\"get\"><a href=\"#get\" class=\"headerlink\" title=\"get\"></a>get</h4><p>Reads the byte at this buffer’s current position, and then increments the position.<br>从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-get.png\"></p>\n<h4 id=\"clear\"><a href=\"#clear\" class=\"headerlink\" title=\"clear\"></a>clear</h4><p>Clears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.<br>Invoke this method before using a sequence of channel-read or <code>put</code> operations to fill this buffer.<br>将position置为0，并不清除buffer内容。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Buffer <span class=\"title function_\">clear</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    position = <span class=\"number\">0</span>;</span><br><span class=\"line\">    limit = capacity;</span><br><span class=\"line\">    mark = -<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-clear.png\"></p>\n<p>参考：<br><a href=\"http://ifeve.com/buffers/\">http://ifeve.com/buffers/</a><br>博客：<br><a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a></p>\n","cover":false,"excerpt":"","more":"<h3 id=\"Buffer的基本用法\"><a href=\"#Buffer的基本用法\" class=\"headerlink\" title=\"Buffer的基本用法\"></a>Buffer的基本用法</h3><p>使用Buffer读写数据一般遵循以下四个步骤：<br>1、分配空间<br>2、写入数据到Buffer<br>3、调用flip()方法<br>4、从Buffer中读取数据<br>5、调用clear()方法或者compact()方法<br>当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过<code>flip()</code>方法<code>将Buffer从写模式切换到读模式</code>。在读模式下，可以读取之前写入到buffer的所有数据。<br>一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。<code>clear()方法会清空整个缓冲区</code>。<code>compact()方法只会清除已经读过的数据</code>。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-process.jpeg\"></p>\n<h3 id=\"四个主要属性：\"><a href=\"#四个主要属性：\" class=\"headerlink\" title=\"四个主要属性：\"></a>四个主要属性：</h3><ul>\n<li>capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。</li>\n<li>position：<ul>\n<li>当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。</li>\n<li>当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。</li>\n</ul>\n</li>\n<li>limit：<ul>\n<li>在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。</li>\n<li>当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）</li>\n</ul>\n</li>\n<li>mark：为某一读过的位置做标记，便于某些时候回退到该位置。</li>\n</ul>\n<h3 id=\"Buffer基本函数\"><a href=\"#Buffer基本函数\" class=\"headerlink\" title=\"Buffer基本函数\"></a>Buffer基本函数</h3><h4 id=\"Buffer的分配\"><a href=\"#Buffer的分配\" class=\"headerlink\" title=\"Buffer的分配\"></a>Buffer的分配</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ByteBuffer buf = ByteBuffer.allocate(48);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"向Buffer中写数据\"><a href=\"#向Buffer中写数据\" class=\"headerlink\" title=\"向Buffer中写数据\"></a>向Buffer中写数据</h4><p>从Channel写到Buffer的例子</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int bytesRead = inChannel.read(buf); //read into buffer.</span><br></pre></td></tr></table></figure>\n<p>通过put方法写Buffer</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">buf.put(127);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"从Buffer中读取数据\"><a href=\"#从Buffer中读取数据\" class=\"headerlink\" title=\"从Buffer中读取数据\"></a>从Buffer中读取数据</h4><p>从Buffer读取数据到Channel的例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//read from buffer into channel.</span><br><span class=\"line\">int bytesWritten = inChannel.write(buf);</span><br></pre></td></tr></table></figure>\n<p>使用get()方法从Buffer中读取数据的例子</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">byte aByte = buf.get();</span><br></pre></td></tr></table></figure>\n<h4 id=\"flip-方法\"><a href=\"#flip-方法\" class=\"headerlink\" title=\"flip()方法\"></a>flip()方法</h4><p>flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。<br>换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。</p>\n<h4 id=\"clear-与compact-方法\"><a href=\"#clear-与compact-方法\" class=\"headerlink\" title=\"clear()与compact()方法\"></a>clear()与compact()方法</h4><p>一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。<br>如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。<br>如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。<br>如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。<br>compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。</p>\n<h3 id=\"图解ByteBuffer函数过程\"><a href=\"#图解ByteBuffer函数过程\" class=\"headerlink\" title=\"图解ByteBuffer函数过程\"></a>图解ByteBuffer函数过程</h3><h4 id=\"put\"><a href=\"#put\" class=\"headerlink\" title=\"put\"></a>put</h4><p>Writes the given byte into this buffer at the current position, and then increments the position.<br>写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-put.png\"></p>\n<h4 id=\"flip\"><a href=\"#flip\" class=\"headerlink\" title=\"flip\"></a>flip</h4><p>Flips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.<br>After a sequence of channel-read or <code>put</code> operations, invoke this method to prepare for a sequence of channel-write or relative  <code>get</code> operations.（即读写模型切换）<br>写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final Buffer flip() &#123;</span><br><span class=\"line\">        limit = position;</span><br><span class=\"line\">        position = 0;</span><br><span class=\"line\">        mark = -1;</span><br><span class=\"line\">        return this;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-flip.png\"></p>\n<h4 id=\"get\"><a href=\"#get\" class=\"headerlink\" title=\"get\"></a>get</h4><p>Reads the byte at this buffer’s current position, and then increments the position.<br>从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-get.png\"></p>\n<h4 id=\"clear\"><a href=\"#clear\" class=\"headerlink\" title=\"clear\"></a>clear</h4><p>Clears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.<br>Invoke this method before using a sequence of channel-read or <code>put</code> operations to fill this buffer.<br>将position置为0，并不清除buffer内容。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Buffer <span class=\"title function_\">clear</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    position = <span class=\"number\">0</span>;</span><br><span class=\"line\">    limit = capacity;</span><br><span class=\"line\">    mark = -<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">this</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-clear.png\"></p>\n<p>参考：<br><a href=\"http://ifeve.com/buffers/\">http://ifeve.com/buffers/</a><br>博客：<br><a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a></p>\n"},{"title":"Java NIO简述","date":"2017-02-12T08:45:16.000Z","_content":"### 概述\nJava NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。\n\n### I/O模型\n* 同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！\n* 同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。\n* 异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！\n* 异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 \n\n### BIO、NIO、AIO\n* Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。\n* Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I/O请求时才启动一个线程进行处理。\n* Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。\n\n### Java NIO由以下几个核心部分组成：\n* Channel（通道）\n* Buffer（缓冲区）\n* Selector（选择器）\n\n#### channel\nJava NIO的通道类似流，但又有些不同：\n* 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。\n* 通道可以异步地读写。\n* 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入\n![overview-channels-buffers.png](http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nChannel的实现\n* `FileChannel` 从文件中读写数据\n* `DatagramChannel`能通过UDP读写网络中的数据\n* `SocketChannel` 能通过TCP读写网络中的数据\n* `ServerSocketChannel` 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。\n\n#### Buffer\nJava NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。\n缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。\n\n#### Selector\nSelector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。\n![selectors.png](http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。\n\n\nSelector的创建\n```\nSelector selector = Selector.open();\n```\n向Selector注册通道\n```\n// 与Selector一起使用时，Channel必须处于非阻塞模式下\nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector,Selectionkey.OP_READ);\n```\n通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道\n```\nint select() //阻塞到至少有一个通道在你注册的事件上就绪了\nint select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)\nint selectNow()  //不会阻塞，不管什么通道就绪都立刻返回\n```","source":"_posts/Java-NIO简述.md","raw":"---\ntitle: Java NIO简述\ndate: 2017-02-12 16:45:16\ntags: [java-nio]\ncategories: [java]\n---\n### 概述\nJava NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。\n\n### I/O模型\n* 同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！\n* 同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。\n* 异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！\n* 异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 \n\n### BIO、NIO、AIO\n* Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。\n* Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I/O请求时才启动一个线程进行处理。\n* Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。\n\n### Java NIO由以下几个核心部分组成：\n* Channel（通道）\n* Buffer（缓冲区）\n* Selector（选择器）\n\n#### channel\nJava NIO的通道类似流，但又有些不同：\n* 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。\n* 通道可以异步地读写。\n* 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入\n![overview-channels-buffers.png](http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nChannel的实现\n* `FileChannel` 从文件中读写数据\n* `DatagramChannel`能通过UDP读写网络中的数据\n* `SocketChannel` 能通过TCP读写网络中的数据\n* `ServerSocketChannel` 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。\n\n#### Buffer\nJava NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。\n缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。\n\n#### Selector\nSelector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。\n![selectors.png](http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。\n\n\nSelector的创建\n```\nSelector selector = Selector.open();\n```\n向Selector注册通道\n```\n// 与Selector一起使用时，Channel必须处于非阻塞模式下\nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector,Selectionkey.OP_READ);\n```\n通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道\n```\nint select() //阻塞到至少有一个通道在你注册的事件上就绪了\nint select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)\nint selectNow()  //不会阻塞，不管什么通道就绪都立刻返回\n```","slug":"Java-NIO简述","published":1,"updated":"2024-04-07T07:42:55.398Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lri0003obnsbsiv4x2c","content":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。</p>\n<h3 id=\"I-O模型\"><a href=\"#I-O模型\" class=\"headerlink\" title=\"I&#x2F;O模型\"></a>I&#x2F;O模型</h3><ul>\n<li>同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！</li>\n<li>同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。</li>\n<li>异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！</li>\n<li>异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。</li>\n</ul>\n<h3 id=\"BIO、NIO、AIO\"><a href=\"#BIO、NIO、AIO\" class=\"headerlink\" title=\"BIO、NIO、AIO\"></a>BIO、NIO、AIO</h3><ul>\n<li>Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。</li>\n<li>Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I&#x2F;O请求时才启动一个线程进行处理。</li>\n<li>Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I&#x2F;O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。</li>\n</ul>\n<h3 id=\"Java-NIO由以下几个核心部分组成：\"><a href=\"#Java-NIO由以下几个核心部分组成：\" class=\"headerlink\" title=\"Java NIO由以下几个核心部分组成：\"></a>Java NIO由以下几个核心部分组成：</h3><ul>\n<li>Channel（通道）</li>\n<li>Buffer（缓冲区）</li>\n<li>Selector（选择器）</li>\n</ul>\n<h4 id=\"channel\"><a href=\"#channel\" class=\"headerlink\" title=\"channel\"></a>channel</h4><p>Java NIO的通道类似流，但又有些不同：</p>\n<ul>\n<li>既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。</li>\n<li>通道可以异步地读写。</li>\n<li>通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"overview-channels-buffers.png\"></li>\n</ul>\n<p>Channel的实现</p>\n<ul>\n<li><code>FileChannel</code> 从文件中读写数据</li>\n<li><code>DatagramChannel</code>能通过UDP读写网络中的数据</li>\n<li><code>SocketChannel</code> 能通过TCP读写网络中的数据</li>\n<li><code>ServerSocketChannel</code> 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。</li>\n</ul>\n<h4 id=\"Buffer\"><a href=\"#Buffer\" class=\"headerlink\" title=\"Buffer\"></a>Buffer</h4><p>Java NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。<br>缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。</p>\n<h4 id=\"Selector\"><a href=\"#Selector\" class=\"headerlink\" title=\"Selector\"></a>Selector</h4><p>Selector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"selectors.png\"><br>要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。</p>\n<p>Selector的创建</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Selector selector = Selector.open();</span><br></pre></td></tr></table></figure>\n<p>向Selector注册通道</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 与Selector一起使用时，Channel必须处于非阻塞模式下</span><br><span class=\"line\">channel.configureBlocking(false);</span><br><span class=\"line\">SelectionKey key = channel.register(selector,Selectionkey.OP_READ);</span><br></pre></td></tr></table></figure>\n<p>通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int select() //阻塞到至少有一个通道在你注册的事件上就绪了</span><br><span class=\"line\">int select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)</span><br><span class=\"line\">int selectNow()  //不会阻塞，不管什么通道就绪都立刻返回</span><br></pre></td></tr></table></figure>","cover":false,"excerpt":"","more":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。</p>\n<h3 id=\"I-O模型\"><a href=\"#I-O模型\" class=\"headerlink\" title=\"I&#x2F;O模型\"></a>I&#x2F;O模型</h3><ul>\n<li>同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！</li>\n<li>同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。</li>\n<li>异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！</li>\n<li>异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。</li>\n</ul>\n<h3 id=\"BIO、NIO、AIO\"><a href=\"#BIO、NIO、AIO\" class=\"headerlink\" title=\"BIO、NIO、AIO\"></a>BIO、NIO、AIO</h3><ul>\n<li>Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。</li>\n<li>Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I&#x2F;O请求时才启动一个线程进行处理。</li>\n<li>Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I&#x2F;O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。</li>\n</ul>\n<h3 id=\"Java-NIO由以下几个核心部分组成：\"><a href=\"#Java-NIO由以下几个核心部分组成：\" class=\"headerlink\" title=\"Java NIO由以下几个核心部分组成：\"></a>Java NIO由以下几个核心部分组成：</h3><ul>\n<li>Channel（通道）</li>\n<li>Buffer（缓冲区）</li>\n<li>Selector（选择器）</li>\n</ul>\n<h4 id=\"channel\"><a href=\"#channel\" class=\"headerlink\" title=\"channel\"></a>channel</h4><p>Java NIO的通道类似流，但又有些不同：</p>\n<ul>\n<li>既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。</li>\n<li>通道可以异步地读写。</li>\n<li>通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"overview-channels-buffers.png\"></li>\n</ul>\n<p>Channel的实现</p>\n<ul>\n<li><code>FileChannel</code> 从文件中读写数据</li>\n<li><code>DatagramChannel</code>能通过UDP读写网络中的数据</li>\n<li><code>SocketChannel</code> 能通过TCP读写网络中的数据</li>\n<li><code>ServerSocketChannel</code> 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。</li>\n</ul>\n<h4 id=\"Buffer\"><a href=\"#Buffer\" class=\"headerlink\" title=\"Buffer\"></a>Buffer</h4><p>Java NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。<br>缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。</p>\n<h4 id=\"Selector\"><a href=\"#Selector\" class=\"headerlink\" title=\"Selector\"></a>Selector</h4><p>Selector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"selectors.png\"><br>要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。</p>\n<p>Selector的创建</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Selector selector = Selector.open();</span><br></pre></td></tr></table></figure>\n<p>向Selector注册通道</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 与Selector一起使用时，Channel必须处于非阻塞模式下</span><br><span class=\"line\">channel.configureBlocking(false);</span><br><span class=\"line\">SelectionKey key = channel.register(selector,Selectionkey.OP_READ);</span><br></pre></td></tr></table></figure>\n<p>通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int select() //阻塞到至少有一个通道在你注册的事件上就绪了</span><br><span class=\"line\">int select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)</span><br><span class=\"line\">int selectNow()  //不会阻塞，不管什么通道就绪都立刻返回</span><br></pre></td></tr></table></figure>"},{"title":"Java基础-JVM内存模型","date":"2018-03-03T11:59:55.000Z","_content":"\n* JRE（JavaRuntimeEnvironment，Java运行环境）也就是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。\n* JDK（Java Development Kit）是程序开发者用来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。\n* JVM（JavaVirtualMachine，Java虚拟机）是JRE的一部分。它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。\n\n\n![avatar](http://wx2.sinaimg.cn/large/007h1WTYly1fysn56thu0j30sg0h9tdz.jpg)\n\n\n### 内存模型\n\n#### 程序计数器\n&#160; &#160; &#160; &#160;**程序计数器是一块较小的内存空间，可以看作是当前线程所执行的（正在执行）字节码的行号指示器**。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。\n\n&#160; &#160; &#160; &#160;由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个**独立**的程序计数器，**各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。它的生命周期与线程相同**\n\n&#160; &#160; &#160; &#160;如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError 情况的区域。\n\n#### 栈\n##### Java虚拟机栈\n与程序计数器一样，Java虚拟机栈也是**线程私有的**，**它的生命周期和线程相同**，**描述的是Java方法执行的内存模型**：每一个方法执行的同时都会创建一个栈帧（Stack Frame），用来存储局部变量表、操作数栈，动态链接，方法出口等信息。每个方法从调用到执行完成的过程，都对应一个栈帧在虚拟机栈中从入栈到出栈的过程。\n\nJava内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。\n\n**局部变量表主要存放了编译器可知的各种基本数据类型、对象引用**。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。\n\nStackOverflowError：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常\n\n##### 本地方法栈\n\n本地方法栈为虚拟使用到Native方法服务。\n\n#### 堆\n\nJava堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程所共享的一块内存区域，在虚拟机启动是创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。**\n\nJava 堆是垃圾收集器管理的主要区域，因此也被称作**GC堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**\n\n堆模型图\n![avatar](http://wx4.sinaimg.cn/large/007h1WTYly1fysn51qwu5j30s30azmyo.jpg)\n\n#### 方法区\n\n方法区与 Java 堆一样，是**各个线程共享的内存区域**，它用于**存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据**。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。\n\n**相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。**\n\n**注**：**在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制）。**\n\n#### 运行时常量池\n\n运行时常量池时方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）\n\n#### 直接内存\n\n直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致OutOfMemoryError异常出现。\n\n本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\n\n\n\n### JVM常用参数的含义\n\n| 参数名称        | 含义                        |\n| --------------- | --------------------------- |\n| -Xms            | 初始堆大小                  |\n| -Xmx            | 最大堆                      |\n| -Xmn            | 年轻代大小（1.4 or later）  |\n| -XX:NewSize     | 设置年轻代大小(for 1.3/1.4) |\n| -XX:MaxNewSize  | 年轻代最大值(for 1.3/1.4)   |\n| -XX:PermSize    | 设置持久代(perm gen)初始值  |\n| -XX:MaxPermSize | 设置持久代最大值            |\n| -Xss            | 每个线程的堆栈大小          |\n\n\n\n","source":"_posts/Java基础-JVM内存模型.md","raw":"---\ntitle: Java基础-JVM内存模型\ndate: 2018-03-03 19:59:55\ntags: [java,java基础,jvm]\ncategories: [Java基础]\n---\n\n* JRE（JavaRuntimeEnvironment，Java运行环境）也就是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。\n* JDK（Java Development Kit）是程序开发者用来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。\n* JVM（JavaVirtualMachine，Java虚拟机）是JRE的一部分。它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。\n\n\n![avatar](http://wx2.sinaimg.cn/large/007h1WTYly1fysn56thu0j30sg0h9tdz.jpg)\n\n\n### 内存模型\n\n#### 程序计数器\n&#160; &#160; &#160; &#160;**程序计数器是一块较小的内存空间，可以看作是当前线程所执行的（正在执行）字节码的行号指示器**。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。\n\n&#160; &#160; &#160; &#160;由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个**独立**的程序计数器，**各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。它的生命周期与线程相同**\n\n&#160; &#160; &#160; &#160;如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError 情况的区域。\n\n#### 栈\n##### Java虚拟机栈\n与程序计数器一样，Java虚拟机栈也是**线程私有的**，**它的生命周期和线程相同**，**描述的是Java方法执行的内存模型**：每一个方法执行的同时都会创建一个栈帧（Stack Frame），用来存储局部变量表、操作数栈，动态链接，方法出口等信息。每个方法从调用到执行完成的过程，都对应一个栈帧在虚拟机栈中从入栈到出栈的过程。\n\nJava内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。\n\n**局部变量表主要存放了编译器可知的各种基本数据类型、对象引用**。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。\n\nStackOverflowError：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常\n\n##### 本地方法栈\n\n本地方法栈为虚拟使用到Native方法服务。\n\n#### 堆\n\nJava堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程所共享的一块内存区域，在虚拟机启动是创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。**\n\nJava 堆是垃圾收集器管理的主要区域，因此也被称作**GC堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**\n\n堆模型图\n![avatar](http://wx4.sinaimg.cn/large/007h1WTYly1fysn51qwu5j30s30azmyo.jpg)\n\n#### 方法区\n\n方法区与 Java 堆一样，是**各个线程共享的内存区域**，它用于**存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据**。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。\n\n**相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。**\n\n**注**：**在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制）。**\n\n#### 运行时常量池\n\n运行时常量池时方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）\n\n#### 直接内存\n\n直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致OutOfMemoryError异常出现。\n\n本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\n\n\n\n### JVM常用参数的含义\n\n| 参数名称        | 含义                        |\n| --------------- | --------------------------- |\n| -Xms            | 初始堆大小                  |\n| -Xmx            | 最大堆                      |\n| -Xmn            | 年轻代大小（1.4 or later）  |\n| -XX:NewSize     | 设置年轻代大小(for 1.3/1.4) |\n| -XX:MaxNewSize  | 年轻代最大值(for 1.3/1.4)   |\n| -XX:PermSize    | 设置持久代(perm gen)初始值  |\n| -XX:MaxPermSize | 设置持久代最大值            |\n| -Xss            | 每个线程的堆栈大小          |\n\n\n\n","slug":"Java基础-JVM内存模型","published":1,"updated":"2024-04-07T07:52:49.495Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrn0007obns1r2bgsz0","content":"<ul>\n<li>JRE（JavaRuntimeEnvironment，Java运行环境）也就是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。</li>\n<li>JDK（Java Development Kit）是程序开发者用来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。</li>\n<li>JVM（JavaVirtualMachine，Java虚拟机）是JRE的一部分。它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。</li>\n</ul>\n<p><img src=\"http://wx2.sinaimg.cn/large/007h1WTYly1fysn56thu0j30sg0h9tdz.jpg\" alt=\"avatar\"></p>\n<h3 id=\"内存模型\"><a href=\"#内存模型\" class=\"headerlink\" title=\"内存模型\"></a>内存模型</h3><h4 id=\"程序计数器\"><a href=\"#程序计数器\" class=\"headerlink\" title=\"程序计数器\"></a>程序计数器</h4><p>&#160; &#160; &#160; &#160;<strong>程序计数器是一块较小的内存空间，可以看作是当前线程所执行的（正在执行）字节码的行号指示器</strong>。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。</p>\n<p>&#160; &#160; &#160; &#160;由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个<strong>独立</strong>的程序计数器，<strong>各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。它的生命周期与线程相同</strong></p>\n<p>&#160; &#160; &#160; &#160;如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError 情况的区域。</p>\n<h4 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h4><h5 id=\"Java虚拟机栈\"><a href=\"#Java虚拟机栈\" class=\"headerlink\" title=\"Java虚拟机栈\"></a>Java虚拟机栈</h5><p>与程序计数器一样，Java虚拟机栈也是<strong>线程私有的</strong>，<strong>它的生命周期和线程相同</strong>，<strong>描述的是Java方法执行的内存模型</strong>：每一个方法执行的同时都会创建一个栈帧（Stack Frame），用来存储局部变量表、操作数栈，动态链接，方法出口等信息。每个方法从调用到执行完成的过程，都对应一个栈帧在虚拟机栈中从入栈到出栈的过程。</p>\n<p>Java内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。</p>\n<p><strong>局部变量表主要存放了编译器可知的各种基本数据类型、对象引用</strong>。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。</p>\n<p>StackOverflowError：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常</p>\n<h5 id=\"本地方法栈\"><a href=\"#本地方法栈\" class=\"headerlink\" title=\"本地方法栈\"></a>本地方法栈</h5><p>本地方法栈为虚拟使用到Native方法服务。</p>\n<h4 id=\"堆\"><a href=\"#堆\" class=\"headerlink\" title=\"堆\"></a>堆</h4><p>Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程所共享的一块内存区域，在虚拟机启动是创建。<strong>此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</strong></p>\n<p>Java 堆是垃圾收集器管理的主要区域，因此也被称作<strong>GC堆（Garbage Collected Heap）</strong>.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。<strong>进一步划分的目的是更好地回收内存，或者更快地分配内存。</strong></p>\n<p>堆模型图<br><img src=\"http://wx4.sinaimg.cn/large/007h1WTYly1fysn51qwu5j30s30azmyo.jpg\" alt=\"avatar\"></p>\n<h4 id=\"方法区\"><a href=\"#方法区\" class=\"headerlink\" title=\"方法区\"></a>方法区</h4><p>方法区与 Java 堆一样，是<strong>各个线程共享的内存区域</strong>，它用于<strong>存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据</strong>。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。</p>\n<p><strong>相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。</strong></p>\n<p><strong>注</strong>：<strong>在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制）。</strong></p>\n<h4 id=\"运行时常量池\"><a href=\"#运行时常量池\" class=\"headerlink\" title=\"运行时常量池\"></a>运行时常量池</h4><p>运行时常量池时方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）</p>\n<h4 id=\"直接内存\"><a href=\"#直接内存\" class=\"headerlink\" title=\"直接内存\"></a>直接内存</h4><p>直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致OutOfMemoryError异常出现。</p>\n<p>本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。</p>\n<h3 id=\"JVM常用参数的含义\"><a href=\"#JVM常用参数的含义\" class=\"headerlink\" title=\"JVM常用参数的含义\"></a>JVM常用参数的含义</h3><table>\n<thead>\n<tr>\n<th>参数名称</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-Xms</td>\n<td>初始堆大小</td>\n</tr>\n<tr>\n<td>-Xmx</td>\n<td>最大堆</td>\n</tr>\n<tr>\n<td>-Xmn</td>\n<td>年轻代大小（1.4 or later）</td>\n</tr>\n<tr>\n<td>-XX:NewSize</td>\n<td>设置年轻代大小(for 1.3&#x2F;1.4)</td>\n</tr>\n<tr>\n<td>-XX:MaxNewSize</td>\n<td>年轻代最大值(for 1.3&#x2F;1.4)</td>\n</tr>\n<tr>\n<td>-XX:PermSize</td>\n<td>设置持久代(perm gen)初始值</td>\n</tr>\n<tr>\n<td>-XX:MaxPermSize</td>\n<td>设置持久代最大值</td>\n</tr>\n<tr>\n<td>-Xss</td>\n<td>每个线程的堆栈大小</td>\n</tr>\n</tbody></table>\n","cover":false,"excerpt":"","more":"<ul>\n<li>JRE（JavaRuntimeEnvironment，Java运行环境）也就是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。</li>\n<li>JDK（Java Development Kit）是程序开发者用来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。</li>\n<li>JVM（JavaVirtualMachine，Java虚拟机）是JRE的一部分。它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。</li>\n</ul>\n<p><img src=\"http://wx2.sinaimg.cn/large/007h1WTYly1fysn56thu0j30sg0h9tdz.jpg\" alt=\"avatar\"></p>\n<h3 id=\"内存模型\"><a href=\"#内存模型\" class=\"headerlink\" title=\"内存模型\"></a>内存模型</h3><h4 id=\"程序计数器\"><a href=\"#程序计数器\" class=\"headerlink\" title=\"程序计数器\"></a>程序计数器</h4><p>&#160; &#160; &#160; &#160;<strong>程序计数器是一块较小的内存空间，可以看作是当前线程所执行的（正在执行）字节码的行号指示器</strong>。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。</p>\n<p>&#160; &#160; &#160; &#160;由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个<strong>独立</strong>的程序计数器，<strong>各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。它的生命周期与线程相同</strong></p>\n<p>&#160; &#160; &#160; &#160;如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError 情况的区域。</p>\n<h4 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h4><h5 id=\"Java虚拟机栈\"><a href=\"#Java虚拟机栈\" class=\"headerlink\" title=\"Java虚拟机栈\"></a>Java虚拟机栈</h5><p>与程序计数器一样，Java虚拟机栈也是<strong>线程私有的</strong>，<strong>它的生命周期和线程相同</strong>，<strong>描述的是Java方法执行的内存模型</strong>：每一个方法执行的同时都会创建一个栈帧（Stack Frame），用来存储局部变量表、操作数栈，动态链接，方法出口等信息。每个方法从调用到执行完成的过程，都对应一个栈帧在虚拟机栈中从入栈到出栈的过程。</p>\n<p>Java内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。</p>\n<p><strong>局部变量表主要存放了编译器可知的各种基本数据类型、对象引用</strong>。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。</p>\n<p>StackOverflowError：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常</p>\n<h5 id=\"本地方法栈\"><a href=\"#本地方法栈\" class=\"headerlink\" title=\"本地方法栈\"></a>本地方法栈</h5><p>本地方法栈为虚拟使用到Native方法服务。</p>\n<h4 id=\"堆\"><a href=\"#堆\" class=\"headerlink\" title=\"堆\"></a>堆</h4><p>Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程所共享的一块内存区域，在虚拟机启动是创建。<strong>此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</strong></p>\n<p>Java 堆是垃圾收集器管理的主要区域，因此也被称作<strong>GC堆（Garbage Collected Heap）</strong>.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。<strong>进一步划分的目的是更好地回收内存，或者更快地分配内存。</strong></p>\n<p>堆模型图<br><img src=\"http://wx4.sinaimg.cn/large/007h1WTYly1fysn51qwu5j30s30azmyo.jpg\" alt=\"avatar\"></p>\n<h4 id=\"方法区\"><a href=\"#方法区\" class=\"headerlink\" title=\"方法区\"></a>方法区</h4><p>方法区与 Java 堆一样，是<strong>各个线程共享的内存区域</strong>，它用于<strong>存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据</strong>。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。</p>\n<p><strong>相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。</strong></p>\n<p><strong>注</strong>：<strong>在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制）。</strong></p>\n<h4 id=\"运行时常量池\"><a href=\"#运行时常量池\" class=\"headerlink\" title=\"运行时常量池\"></a>运行时常量池</h4><p>运行时常量池时方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）</p>\n<h4 id=\"直接内存\"><a href=\"#直接内存\" class=\"headerlink\" title=\"直接内存\"></a>直接内存</h4><p>直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致OutOfMemoryError异常出现。</p>\n<p>本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。</p>\n<h3 id=\"JVM常用参数的含义\"><a href=\"#JVM常用参数的含义\" class=\"headerlink\" title=\"JVM常用参数的含义\"></a>JVM常用参数的含义</h3><table>\n<thead>\n<tr>\n<th>参数名称</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-Xms</td>\n<td>初始堆大小</td>\n</tr>\n<tr>\n<td>-Xmx</td>\n<td>最大堆</td>\n</tr>\n<tr>\n<td>-Xmn</td>\n<td>年轻代大小（1.4 or later）</td>\n</tr>\n<tr>\n<td>-XX:NewSize</td>\n<td>设置年轻代大小(for 1.3&#x2F;1.4)</td>\n</tr>\n<tr>\n<td>-XX:MaxNewSize</td>\n<td>年轻代最大值(for 1.3&#x2F;1.4)</td>\n</tr>\n<tr>\n<td>-XX:PermSize</td>\n<td>设置持久代(perm gen)初始值</td>\n</tr>\n<tr>\n<td>-XX:MaxPermSize</td>\n<td>设置持久代最大值</td>\n</tr>\n<tr>\n<td>-Xss</td>\n<td>每个线程的堆栈大小</td>\n</tr>\n</tbody></table>\n"},{"title":"Java基础-JVM类加载","date":"2018-04-12T12:52:17.000Z","_content":"\n\n\n### 类加载过程\n> **JVM类加载机制分为五个部分：加载，验证，准备，解析，初始化**\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dp4k541j30w00astab.jpg)\n\n#### 加载\n\n1. 通过一个类的全限定名来获取其定义的二进制字节流。\n2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。\n3. 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。\n\n\n> JVM中的ClassLoader类加载器加载Class发生在此阶段\n> \n> 这里不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也**可以在运行时计算生成（动态代理）**，也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。\n\n\n#### 验证\n验证的目的是为了**确保Class文件中的字节流包含的信息符合当前虚拟机的要求**，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：**文件格式的验证、元数据的验证、字节码验证和符号引用验证**。\n\n* 文件格式的验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。\n* 元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合Java语法规范的元数据信息。\n* 字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。\n* 符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。\n\n\n#### 准备\n准备阶段是正式为**类变量（static变量）**分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。\n\n1. 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。\n2. 这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。\n\n假设一个类变量的定义为：\n\n```java\npublic static int value = 3；\n```\n那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器<clinit\\>（）方法之中的，**所以把value赋值为3的动作将在初始化阶段才会执行。**\n\n\n\n> final、static、static final修饰的字段赋值的区别：\n>\n> - static修饰的字段在类加载过程中的准备阶段被初始化为0或null等默认值，而后在初始化阶段（触发类构造器<clinit\\>）才会被赋予代码中设定的值，如果没有设定值，那么它的值就为默认值。\n> - final修饰的字段在运行时被初始化（可以直接赋值，也可以在实例构造器中赋值），一旦赋值便不可更改；\n> - static final修饰的字段在Javac时生成ConstantValue属性，在类加载的准备阶段根据ConstantValue的值为该字段赋值，它没有默认值，必须显式地赋值，否则Javac时会报错。可以理解为在编译期即把结果放入了常量池中。\n\n\n\n#### 解析\n\n**把类中的符号引用转换为直接引用**。解析阶段是虚拟机将常量池的符号引用替换为直接引用的过程。\n\n解析动作主要：\n1. 类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。\n2. 字段解析：字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会**按照继承关系从上往下递归搜索**该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。\n3. 类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。\n4. 接口方法解析：与类方法解析步骤类似，知识接口不会有父类，因此，只递归向上搜索父接口就行了。\n\n\n\n>- 符号引用：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到了内存中。\n>\n>- 直接引用：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那说明引用的目标必定已经存在于内存之中了。\n\n#### 初始化\n\n* 初始化阶段是类加载最后一个阶段，前面的类加载阶段之后，除了在加载阶段可以自定义类加载器以外，其它操作都由JVM主导。到了初始阶段，才开始真正执行类中定义的Java程序代码。\n* 初始化阶段是执行类构造器<client\\>方法的过程。<client\\>方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证<client\\>方法执行之前，父类的<client\\>方法已经执行完毕。\n\n**<clinit\\>（）方法的执行规则:**\n\n1. <clinit>（）方法是由编译器自动收集类中的**所有类变量的赋值动作和静态语句块中的语句**合并产生的，编译器收集的顺序是由语句在**源文件中出现的顺序**所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句中可以赋值，但是不能访问。\n2.  <clinit\\>（）方法与实例构造器<init\\>（）方法（类的构造函数）不同，它不需要显式地调用父类构造器，**虚拟机会保证在子类的<clinit\\>（）方法执行之前，父类的<clinit\\>（）方法已经执行完毕**。因此，在虚拟机中第一个被执行的<clinit\\>（）方法的类肯定是java.lang.Object。\n3. <clinit\\>（）方法对于类或接口来说并不是必须的，如果一个类中没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成<clinit\\>（）方法。\n4. 接口中不能使用静态语句块，但仍然有类变量（final static）初始化的赋值操作，因此**接口与类一样会生成<clinit\\>（）方法**。但是接口类不同的是：执行接口的<clinit\\>（）方法不需要先执行父接口的<clinit\\>（）方法，只有当父接口中定义的变量被使用时，父接口才会被初始化。另外，接口的实现类在初始化时也一样不会执行接口的<clinit\\>（）方法。\n5. **虚拟机会保证一个类的<clinit\\>（）方法在多线程环境中被正确地加锁和同步**，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的<clinit\\>（）方法，其他线程都需要阻塞等待，直到活动线程执行<clinit\\>（）方法完毕。如果在一个类的<clinit\\>（）方法中有耗时很长的操作，那就可能造成多个线程阻塞，在实际应用中这种阻塞往往是很隐蔽的\n\n### 类加载器\n* 启动类加载器（Bootstrap ClassLoader）\n* 扩展类加载器（Extension ClassLoader）\n* 应用程序类加载器（Application ClassLoader）\n* 自定义类加载器\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fz0dozllubj30ge0db3zn.jpg)\n\n#### 双亲委派模型\n#### 工作流程\n如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dovr832j305w0c6glq.jpg)\n\n#### 机制\n\n1. 当ApplicationClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtensionClassLoader去完成。\n2. 当ExtensionClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。\n3. 如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtensionClassLoader来尝试加载；\n4. 若ExtensionClassLoader也加载失败，则会使用ApplicationClassLoader来加载，如果ApplicationClassLoader也加载失败，则会报出异常ClassNotFoundException。\n\n\n\n#### 系统安全性\n\nJava类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java中的Object类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，**最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类**。如果不采用双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object类。","source":"_posts/Java基础-JVM类加载.md","raw":"---\ntitle: Java基础-JVM类加载\ndate: 2018-04-12 20:52:17\ntags: [java,java基础,jvm]\ncategories: [Java基础]\n---\n\n\n\n### 类加载过程\n> **JVM类加载机制分为五个部分：加载，验证，准备，解析，初始化**\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dp4k541j30w00astab.jpg)\n\n#### 加载\n\n1. 通过一个类的全限定名来获取其定义的二进制字节流。\n2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。\n3. 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。\n\n\n> JVM中的ClassLoader类加载器加载Class发生在此阶段\n> \n> 这里不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也**可以在运行时计算生成（动态代理）**，也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。\n\n\n#### 验证\n验证的目的是为了**确保Class文件中的字节流包含的信息符合当前虚拟机的要求**，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：**文件格式的验证、元数据的验证、字节码验证和符号引用验证**。\n\n* 文件格式的验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。\n* 元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合Java语法规范的元数据信息。\n* 字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。\n* 符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。\n\n\n#### 准备\n准备阶段是正式为**类变量（static变量）**分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。\n\n1. 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。\n2. 这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。\n\n假设一个类变量的定义为：\n\n```java\npublic static int value = 3；\n```\n那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器<clinit\\>（）方法之中的，**所以把value赋值为3的动作将在初始化阶段才会执行。**\n\n\n\n> final、static、static final修饰的字段赋值的区别：\n>\n> - static修饰的字段在类加载过程中的准备阶段被初始化为0或null等默认值，而后在初始化阶段（触发类构造器<clinit\\>）才会被赋予代码中设定的值，如果没有设定值，那么它的值就为默认值。\n> - final修饰的字段在运行时被初始化（可以直接赋值，也可以在实例构造器中赋值），一旦赋值便不可更改；\n> - static final修饰的字段在Javac时生成ConstantValue属性，在类加载的准备阶段根据ConstantValue的值为该字段赋值，它没有默认值，必须显式地赋值，否则Javac时会报错。可以理解为在编译期即把结果放入了常量池中。\n\n\n\n#### 解析\n\n**把类中的符号引用转换为直接引用**。解析阶段是虚拟机将常量池的符号引用替换为直接引用的过程。\n\n解析动作主要：\n1. 类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。\n2. 字段解析：字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会**按照继承关系从上往下递归搜索**该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。\n3. 类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。\n4. 接口方法解析：与类方法解析步骤类似，知识接口不会有父类，因此，只递归向上搜索父接口就行了。\n\n\n\n>- 符号引用：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到了内存中。\n>\n>- 直接引用：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那说明引用的目标必定已经存在于内存之中了。\n\n#### 初始化\n\n* 初始化阶段是类加载最后一个阶段，前面的类加载阶段之后，除了在加载阶段可以自定义类加载器以外，其它操作都由JVM主导。到了初始阶段，才开始真正执行类中定义的Java程序代码。\n* 初始化阶段是执行类构造器<client\\>方法的过程。<client\\>方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证<client\\>方法执行之前，父类的<client\\>方法已经执行完毕。\n\n**<clinit\\>（）方法的执行规则:**\n\n1. <clinit>（）方法是由编译器自动收集类中的**所有类变量的赋值动作和静态语句块中的语句**合并产生的，编译器收集的顺序是由语句在**源文件中出现的顺序**所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句中可以赋值，但是不能访问。\n2.  <clinit\\>（）方法与实例构造器<init\\>（）方法（类的构造函数）不同，它不需要显式地调用父类构造器，**虚拟机会保证在子类的<clinit\\>（）方法执行之前，父类的<clinit\\>（）方法已经执行完毕**。因此，在虚拟机中第一个被执行的<clinit\\>（）方法的类肯定是java.lang.Object。\n3. <clinit\\>（）方法对于类或接口来说并不是必须的，如果一个类中没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成<clinit\\>（）方法。\n4. 接口中不能使用静态语句块，但仍然有类变量（final static）初始化的赋值操作，因此**接口与类一样会生成<clinit\\>（）方法**。但是接口类不同的是：执行接口的<clinit\\>（）方法不需要先执行父接口的<clinit\\>（）方法，只有当父接口中定义的变量被使用时，父接口才会被初始化。另外，接口的实现类在初始化时也一样不会执行接口的<clinit\\>（）方法。\n5. **虚拟机会保证一个类的<clinit\\>（）方法在多线程环境中被正确地加锁和同步**，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的<clinit\\>（）方法，其他线程都需要阻塞等待，直到活动线程执行<clinit\\>（）方法完毕。如果在一个类的<clinit\\>（）方法中有耗时很长的操作，那就可能造成多个线程阻塞，在实际应用中这种阻塞往往是很隐蔽的\n\n### 类加载器\n* 启动类加载器（Bootstrap ClassLoader）\n* 扩展类加载器（Extension ClassLoader）\n* 应用程序类加载器（Application ClassLoader）\n* 自定义类加载器\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fz0dozllubj30ge0db3zn.jpg)\n\n#### 双亲委派模型\n#### 工作流程\n如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dovr832j305w0c6glq.jpg)\n\n#### 机制\n\n1. 当ApplicationClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtensionClassLoader去完成。\n2. 当ExtensionClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。\n3. 如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtensionClassLoader来尝试加载；\n4. 若ExtensionClassLoader也加载失败，则会使用ApplicationClassLoader来加载，如果ApplicationClassLoader也加载失败，则会报出异常ClassNotFoundException。\n\n\n\n#### 系统安全性\n\nJava类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java中的Object类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，**最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类**。如果不采用双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object类。","slug":"Java基础-JVM类加载","published":1,"updated":"2024-04-07T07:52:49.495Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lro0008obns94uf3cn1","content":"<h3 id=\"类加载过程\"><a href=\"#类加载过程\" class=\"headerlink\" title=\"类加载过程\"></a>类加载过程</h3><blockquote>\n<p><strong>JVM类加载机制分为五个部分：加载，验证，准备，解析，初始化</strong></p>\n</blockquote>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dp4k541j30w00astab.jpg\" alt=\"avatar\"></p>\n<h4 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h4><ol>\n<li>通过一个类的全限定名来获取其定义的二进制字节流。</li>\n<li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</li>\n<li>在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。</li>\n</ol>\n<blockquote>\n<p>JVM中的ClassLoader类加载器加载Class发生在此阶段</p>\n<p>这里不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也<strong>可以在运行时计算生成（动态代理）</strong>，也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。</p>\n</blockquote>\n<h4 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h4><p>验证的目的是为了<strong>确保Class文件中的字节流包含的信息符合当前虚拟机的要求</strong>，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：<strong>文件格式的验证、元数据的验证、字节码验证和符号引用验证</strong>。</p>\n<ul>\n<li>文件格式的验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。</li>\n<li>元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合Java语法规范的元数据信息。</li>\n<li>字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。</li>\n<li>符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。</li>\n</ul>\n<h4 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h4><p>准备阶段是正式为<strong>类变量（static变量）</strong>分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。</p>\n<ol>\n<li>这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。</li>\n<li>这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。</li>\n</ol>\n<p>假设一个类变量的定义为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"variable\">value</span> <span class=\"operator\">=</span> <span class=\"number\">3</span>；</span><br></pre></td></tr></table></figure>\n<p>那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器&lt;clinit&gt;（）方法之中的，<strong>所以把value赋值为3的动作将在初始化阶段才会执行。</strong></p>\n<blockquote>\n<p>final、static、static final修饰的字段赋值的区别：</p>\n<ul>\n<li>static修饰的字段在类加载过程中的准备阶段被初始化为0或null等默认值，而后在初始化阶段（触发类构造器&lt;clinit&gt;）才会被赋予代码中设定的值，如果没有设定值，那么它的值就为默认值。</li>\n<li>final修饰的字段在运行时被初始化（可以直接赋值，也可以在实例构造器中赋值），一旦赋值便不可更改；</li>\n<li>static final修饰的字段在Javac时生成ConstantValue属性，在类加载的准备阶段根据ConstantValue的值为该字段赋值，它没有默认值，必须显式地赋值，否则Javac时会报错。可以理解为在编译期即把结果放入了常量池中。</li>\n</ul>\n</blockquote>\n<h4 id=\"解析\"><a href=\"#解析\" class=\"headerlink\" title=\"解析\"></a>解析</h4><p><strong>把类中的符号引用转换为直接引用</strong>。解析阶段是虚拟机将常量池的符号引用替换为直接引用的过程。</p>\n<p>解析动作主要：</p>\n<ol>\n<li>类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。</li>\n<li>字段解析：字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会<strong>按照继承关系从上往下递归搜索</strong>该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。</li>\n<li>类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。</li>\n<li>接口方法解析：与类方法解析步骤类似，知识接口不会有父类，因此，只递归向上搜索父接口就行了。</li>\n</ol>\n<blockquote>\n<ul>\n<li><p>符号引用：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到了内存中。</p>\n</li>\n<li><p>直接引用：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那说明引用的目标必定已经存在于内存之中了。</p>\n</li>\n</ul>\n</blockquote>\n<h4 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h4><ul>\n<li>初始化阶段是类加载最后一个阶段，前面的类加载阶段之后，除了在加载阶段可以自定义类加载器以外，其它操作都由JVM主导。到了初始阶段，才开始真正执行类中定义的Java程序代码。</li>\n<li>初始化阶段是执行类构造器&lt;client&gt;方法的过程。&lt;client&gt;方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证&lt;client&gt;方法执行之前，父类的&lt;client&gt;方法已经执行完毕。</li>\n</ul>\n<p><strong>&lt;clinit&gt;（）方法的执行规则:</strong></p>\n<ol>\n<li><clinit>（）方法是由编译器自动收集类中的<strong>所有类变量的赋值动作和静态语句块中的语句</strong>合并产生的，编译器收集的顺序是由语句在<strong>源文件中出现的顺序</strong>所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句中可以赋值，但是不能访问。</li>\n<li>&lt;clinit&gt;（）方法与实例构造器&lt;init&gt;（）方法（类的构造函数）不同，它不需要显式地调用父类构造器，<strong>虚拟机会保证在子类的&lt;clinit&gt;（）方法执行之前，父类的&lt;clinit&gt;（）方法已经执行完毕</strong>。因此，在虚拟机中第一个被执行的&lt;clinit&gt;（）方法的类肯定是java.lang.Object。</li>\n<li>&lt;clinit&gt;（）方法对于类或接口来说并不是必须的，如果一个类中没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成&lt;clinit&gt;（）方法。</li>\n<li>接口中不能使用静态语句块，但仍然有类变量（final static）初始化的赋值操作，因此<strong>接口与类一样会生成&lt;clinit&gt;（）方法</strong>。但是接口类不同的是：执行接口的&lt;clinit&gt;（）方法不需要先执行父接口的&lt;clinit&gt;（）方法，只有当父接口中定义的变量被使用时，父接口才会被初始化。另外，接口的实现类在初始化时也一样不会执行接口的&lt;clinit&gt;（）方法。</li>\n<li><strong>虚拟机会保证一个类的&lt;clinit&gt;（）方法在多线程环境中被正确地加锁和同步</strong>，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;（）方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;（）方法完毕。如果在一个类的&lt;clinit&gt;（）方法中有耗时很长的操作，那就可能造成多个线程阻塞，在实际应用中这种阻塞往往是很隐蔽的</li>\n</ol>\n<h3 id=\"类加载器\"><a href=\"#类加载器\" class=\"headerlink\" title=\"类加载器\"></a>类加载器</h3><ul>\n<li>启动类加载器（Bootstrap ClassLoader）</li>\n<li>扩展类加载器（Extension ClassLoader）</li>\n<li>应用程序类加载器（Application ClassLoader）</li>\n<li>自定义类加载器</li>\n</ul>\n<p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fz0dozllubj30ge0db3zn.jpg\" alt=\"avatar\"></p>\n<h4 id=\"双亲委派模型\"><a href=\"#双亲委派模型\" class=\"headerlink\" title=\"双亲委派模型\"></a>双亲委派模型</h4><h4 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h4><p>如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。</p>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dovr832j305w0c6glq.jpg\" alt=\"avatar\"></p>\n<h4 id=\"机制\"><a href=\"#机制\" class=\"headerlink\" title=\"机制\"></a>机制</h4><ol>\n<li>当ApplicationClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtensionClassLoader去完成。</li>\n<li>当ExtensionClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。</li>\n<li>如果BootStrapClassLoader加载失败（例如在$JAVA_HOME&#x2F;jre&#x2F;lib里未查找到该class），会使用ExtensionClassLoader来尝试加载；</li>\n<li>若ExtensionClassLoader也加载失败，则会使用ApplicationClassLoader来加载，如果ApplicationClassLoader也加载失败，则会报出异常ClassNotFoundException。</li>\n</ol>\n<h4 id=\"系统安全性\"><a href=\"#系统安全性\" class=\"headerlink\" title=\"系统安全性\"></a>系统安全性</h4><p>Java类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java中的Object类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，<strong>最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类</strong>。如果不采用双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object类。</p>\n","cover":false,"excerpt":"","more":"<h3 id=\"类加载过程\"><a href=\"#类加载过程\" class=\"headerlink\" title=\"类加载过程\"></a>类加载过程</h3><blockquote>\n<p><strong>JVM类加载机制分为五个部分：加载，验证，准备，解析，初始化</strong></p>\n</blockquote>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dp4k541j30w00astab.jpg\" alt=\"avatar\"></p>\n<h4 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h4><ol>\n<li>通过一个类的全限定名来获取其定义的二进制字节流。</li>\n<li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</li>\n<li>在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。</li>\n</ol>\n<blockquote>\n<p>JVM中的ClassLoader类加载器加载Class发生在此阶段</p>\n<p>这里不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也<strong>可以在运行时计算生成（动态代理）</strong>，也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。</p>\n</blockquote>\n<h4 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h4><p>验证的目的是为了<strong>确保Class文件中的字节流包含的信息符合当前虚拟机的要求</strong>，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：<strong>文件格式的验证、元数据的验证、字节码验证和符号引用验证</strong>。</p>\n<ul>\n<li>文件格式的验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。</li>\n<li>元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合Java语法规范的元数据信息。</li>\n<li>字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。</li>\n<li>符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。</li>\n</ul>\n<h4 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h4><p>准备阶段是正式为<strong>类变量（static变量）</strong>分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。</p>\n<ol>\n<li>这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。</li>\n<li>这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。</li>\n</ol>\n<p>假设一个类变量的定义为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"variable\">value</span> <span class=\"operator\">=</span> <span class=\"number\">3</span>；</span><br></pre></td></tr></table></figure>\n<p>那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器&lt;clinit&gt;（）方法之中的，<strong>所以把value赋值为3的动作将在初始化阶段才会执行。</strong></p>\n<blockquote>\n<p>final、static、static final修饰的字段赋值的区别：</p>\n<ul>\n<li>static修饰的字段在类加载过程中的准备阶段被初始化为0或null等默认值，而后在初始化阶段（触发类构造器&lt;clinit&gt;）才会被赋予代码中设定的值，如果没有设定值，那么它的值就为默认值。</li>\n<li>final修饰的字段在运行时被初始化（可以直接赋值，也可以在实例构造器中赋值），一旦赋值便不可更改；</li>\n<li>static final修饰的字段在Javac时生成ConstantValue属性，在类加载的准备阶段根据ConstantValue的值为该字段赋值，它没有默认值，必须显式地赋值，否则Javac时会报错。可以理解为在编译期即把结果放入了常量池中。</li>\n</ul>\n</blockquote>\n<h4 id=\"解析\"><a href=\"#解析\" class=\"headerlink\" title=\"解析\"></a>解析</h4><p><strong>把类中的符号引用转换为直接引用</strong>。解析阶段是虚拟机将常量池的符号引用替换为直接引用的过程。</p>\n<p>解析动作主要：</p>\n<ol>\n<li>类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。</li>\n<li>字段解析：字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会<strong>按照继承关系从上往下递归搜索</strong>该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。</li>\n<li>类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。</li>\n<li>接口方法解析：与类方法解析步骤类似，知识接口不会有父类，因此，只递归向上搜索父接口就行了。</li>\n</ol>\n<blockquote>\n<ul>\n<li><p>符号引用：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到了内存中。</p>\n</li>\n<li><p>直接引用：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那说明引用的目标必定已经存在于内存之中了。</p>\n</li>\n</ul>\n</blockquote>\n<h4 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h4><ul>\n<li>初始化阶段是类加载最后一个阶段，前面的类加载阶段之后，除了在加载阶段可以自定义类加载器以外，其它操作都由JVM主导。到了初始阶段，才开始真正执行类中定义的Java程序代码。</li>\n<li>初始化阶段是执行类构造器&lt;client&gt;方法的过程。&lt;client&gt;方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证&lt;client&gt;方法执行之前，父类的&lt;client&gt;方法已经执行完毕。</li>\n</ul>\n<p><strong>&lt;clinit&gt;（）方法的执行规则:</strong></p>\n<ol>\n<li><clinit>（）方法是由编译器自动收集类中的<strong>所有类变量的赋值动作和静态语句块中的语句</strong>合并产生的，编译器收集的顺序是由语句在<strong>源文件中出现的顺序</strong>所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句中可以赋值，但是不能访问。</li>\n<li>&lt;clinit&gt;（）方法与实例构造器&lt;init&gt;（）方法（类的构造函数）不同，它不需要显式地调用父类构造器，<strong>虚拟机会保证在子类的&lt;clinit&gt;（）方法执行之前，父类的&lt;clinit&gt;（）方法已经执行完毕</strong>。因此，在虚拟机中第一个被执行的&lt;clinit&gt;（）方法的类肯定是java.lang.Object。</li>\n<li>&lt;clinit&gt;（）方法对于类或接口来说并不是必须的，如果一个类中没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成&lt;clinit&gt;（）方法。</li>\n<li>接口中不能使用静态语句块，但仍然有类变量（final static）初始化的赋值操作，因此<strong>接口与类一样会生成&lt;clinit&gt;（）方法</strong>。但是接口类不同的是：执行接口的&lt;clinit&gt;（）方法不需要先执行父接口的&lt;clinit&gt;（）方法，只有当父接口中定义的变量被使用时，父接口才会被初始化。另外，接口的实现类在初始化时也一样不会执行接口的&lt;clinit&gt;（）方法。</li>\n<li><strong>虚拟机会保证一个类的&lt;clinit&gt;（）方法在多线程环境中被正确地加锁和同步</strong>，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;（）方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;（）方法完毕。如果在一个类的&lt;clinit&gt;（）方法中有耗时很长的操作，那就可能造成多个线程阻塞，在实际应用中这种阻塞往往是很隐蔽的</li>\n</ol>\n<h3 id=\"类加载器\"><a href=\"#类加载器\" class=\"headerlink\" title=\"类加载器\"></a>类加载器</h3><ul>\n<li>启动类加载器（Bootstrap ClassLoader）</li>\n<li>扩展类加载器（Extension ClassLoader）</li>\n<li>应用程序类加载器（Application ClassLoader）</li>\n<li>自定义类加载器</li>\n</ul>\n<p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fz0dozllubj30ge0db3zn.jpg\" alt=\"avatar\"></p>\n<h4 id=\"双亲委派模型\"><a href=\"#双亲委派模型\" class=\"headerlink\" title=\"双亲委派模型\"></a>双亲委派模型</h4><h4 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h4><p>如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。</p>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fz0dovr832j305w0c6glq.jpg\" alt=\"avatar\"></p>\n<h4 id=\"机制\"><a href=\"#机制\" class=\"headerlink\" title=\"机制\"></a>机制</h4><ol>\n<li>当ApplicationClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtensionClassLoader去完成。</li>\n<li>当ExtensionClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。</li>\n<li>如果BootStrapClassLoader加载失败（例如在$JAVA_HOME&#x2F;jre&#x2F;lib里未查找到该class），会使用ExtensionClassLoader来尝试加载；</li>\n<li>若ExtensionClassLoader也加载失败，则会使用ApplicationClassLoader来加载，如果ApplicationClassLoader也加载失败，则会报出异常ClassNotFoundException。</li>\n</ol>\n<h4 id=\"系统安全性\"><a href=\"#系统安全性\" class=\"headerlink\" title=\"系统安全性\"></a>系统安全性</h4><p>Java类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java中的Object类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，<strong>最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类</strong>。如果不采用双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object类。</p>\n"},{"title":"Java基础-Thread概述","date":"2017-03-08T11:33:39.000Z","_content":"### 线程生命周期\n\n![Threadlife_20170208.jpg](http://upload-images.jianshu.io/upload_images/1419542-7b0fe1fa20076f52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620)\n线程状态：\n* 新建状态：使用new关键字和Thread类或其子类建立一个线程对象后\n* 就绪状态：当线程对象调用了start()方法之后，该线程进入就绪状态\n* 运行状态：如果就绪状态的线程获取CPU资源，就可以执行run()\n* 阻塞状态：\n  * 等待阻塞：运行状态中的线程执行wait()方法\n  * 同步阻塞：线程在获取 synchronized 同步锁失败\n  * 其他阻塞：通过调用线程的sleep()或join()发出I/O请求时\n* 死亡状态：一个运行状态的线程完成人文或者其他终止条件发送\n\n### 创建线程\n1、继承Thread类\n```java\nclass ThreadDemo extends Thread\n```\n2、实现Runable接口\n```java\npublic class RunTest implements Runnable\n   \nThread thread1 = new Thread(new RunTest());\n```\n3、通过Callable、Future、FutureTask创建线程\n```\n// 继承实现Callable接口，声明返回类型\nclass CallTest implements Callable<Long> \n    \nFutureTask<Long> futureTask = new FutureTask<Long>(new CallTest(searchVo));\n```\n\n### 线程内置方法\n1、sleep：使当前线程（即调用该方法的线程）暂停执行一段时间，让其它线程有机会继续执行，但它不会释放对象锁。\n2、yield：__暂停__当前正在执行的线程对象，并执行其他线程。yield的目的是让相同优先级的线程之间能适当的__轮转执行__\n3、join：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如线程B中调用了线程A的JOIN()方法，直到线程A执行完毕后，才会继续执行线程B。\n```\n    //主线程等待子线程thread执行结束才会输出结果\n    public static void main(String[] args) throws InterruptedException {\n        Thread thread = new Thread(new JoinTest());\n        thread.start();\n        thread.join(); //加入join()\n        System.out.println(\"主线程结束\");\n    }\n```\n4、setDaemon：\nJava中线程分为两种类型：用户线程和守护线程。通过Thread.setDaemon(false)设置为用户线程；通过Thread.setDaemon(true)设置为守护线程。如果不设置次属性，默认为用户线程。\n```java\n  public class DaemonTest extends Thread {\n    public void run() {            //永真循环线程\n        for (int i = 0; ; i++) {\n            try {\n                Thread.sleep(1000);\n            } catch (InterruptedException ex) {\n            }\n            System.out.println(i);\n        }\n    }\n\n    public static void main(String[] args) {\n        Thread daemonTest = new DaemonTest();\n        daemonTest.setDaemon(true);    //调试时可以设置为false，那么这个程序是个死循环，没有退出条件。设置为true，即可主线程结束，test线程也结束。\n        daemonTest.start();\n        System.out.println(\"isDaemon = \" + daemonTest.isDaemon());\n        try {\n            System.in.read();   // 接受输入，使程序在此停顿，一旦接收到用户输入，main线程结束，守护线程自动结束\n        } catch (IOException ex) {\n        }\n    }\n}\n```\n如果线程设置为Thread.setDaemon(true)，则主线程结束该程序不会结束，必须等待子线程执行结束整个程序才能结束；如果线程设置为Thread.setDaemon(false)，则主线程结束整个程序就结束。\n\n5、wait、notify、notifyAll\n当一个线程进入wait之后，就必须等待其他线程notify/notifyAll，使用notifyAll可以唤醒所有处于wait状态的线程，使其重新进入锁的争夺队列中，而notify只能唤醒一个。\nnotify被唤醒的线程是随机的，所以通常是没办法指定是谁被唤醒。\n\n### 线程关键字\n* volatile\n可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入\n原子性：对任意单个volatile变量的读/写具有原子性，但是类似于volatile++这种复合操作不具有原子性\n* synchronized\n当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。\nsynchronized 关键字，它包括两种用法：synchronized 方法和 synchronized 块。  \n\t* synchronized 方法：通过在方法声明中加入 synchronized关键字来声明 synchronized 方法\n```java\npublic synchronized void accessVal(int newVal);  \n```\n\t* synchronized 块：通过 synchronized关键字来声明synchronized 块\n```java\nsynchronized(syncObject) {  \n//允许访问控制的代码  \n}  \n```\n\n### ThreadLocal\nThreadLocal是一个关于创建线程局部变量的类。通常情况下，我们创建的变量是可以被任何一个线程访问修改的。而使用ThreadLocal创建的变量只能被当前线程访问，其他线程无法访问和修改。\n> 在Java中，栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存。而堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。\n\nThreadLocal并不是存放在栈上。ThreadLocal实例实际上也是被其创建的类持有（更顶端应该是被线程持有）。而ThreadLocal的值其实也是被线程实例持有。它们都是位于堆上，只是通过一些技巧将可见性修改成线程可见。\n```java\n//使用ThreadLocal保存Connection变量 \nThreadLocal<Connection> connThreadLocal = new ThreadLocal<Connection>(); \n//保存到线程本地变量中\nconnThreadLocal.set(conn);  \n// 直接返回线程本地变量  \nconnThreadLocal.get();\n\n```\n","source":"_posts/Java基础-Thread概述.md","raw":"---\ntitle: Java基础-Thread概述\ndate: 2017-03-08 19:33:39\ntags: [java,thread,Java基础]\ncategories: [Java基础]\n---\n### 线程生命周期\n\n![Threadlife_20170208.jpg](http://upload-images.jianshu.io/upload_images/1419542-7b0fe1fa20076f52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620)\n线程状态：\n* 新建状态：使用new关键字和Thread类或其子类建立一个线程对象后\n* 就绪状态：当线程对象调用了start()方法之后，该线程进入就绪状态\n* 运行状态：如果就绪状态的线程获取CPU资源，就可以执行run()\n* 阻塞状态：\n  * 等待阻塞：运行状态中的线程执行wait()方法\n  * 同步阻塞：线程在获取 synchronized 同步锁失败\n  * 其他阻塞：通过调用线程的sleep()或join()发出I/O请求时\n* 死亡状态：一个运行状态的线程完成人文或者其他终止条件发送\n\n### 创建线程\n1、继承Thread类\n```java\nclass ThreadDemo extends Thread\n```\n2、实现Runable接口\n```java\npublic class RunTest implements Runnable\n   \nThread thread1 = new Thread(new RunTest());\n```\n3、通过Callable、Future、FutureTask创建线程\n```\n// 继承实现Callable接口，声明返回类型\nclass CallTest implements Callable<Long> \n    \nFutureTask<Long> futureTask = new FutureTask<Long>(new CallTest(searchVo));\n```\n\n### 线程内置方法\n1、sleep：使当前线程（即调用该方法的线程）暂停执行一段时间，让其它线程有机会继续执行，但它不会释放对象锁。\n2、yield：__暂停__当前正在执行的线程对象，并执行其他线程。yield的目的是让相同优先级的线程之间能适当的__轮转执行__\n3、join：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如线程B中调用了线程A的JOIN()方法，直到线程A执行完毕后，才会继续执行线程B。\n```\n    //主线程等待子线程thread执行结束才会输出结果\n    public static void main(String[] args) throws InterruptedException {\n        Thread thread = new Thread(new JoinTest());\n        thread.start();\n        thread.join(); //加入join()\n        System.out.println(\"主线程结束\");\n    }\n```\n4、setDaemon：\nJava中线程分为两种类型：用户线程和守护线程。通过Thread.setDaemon(false)设置为用户线程；通过Thread.setDaemon(true)设置为守护线程。如果不设置次属性，默认为用户线程。\n```java\n  public class DaemonTest extends Thread {\n    public void run() {            //永真循环线程\n        for (int i = 0; ; i++) {\n            try {\n                Thread.sleep(1000);\n            } catch (InterruptedException ex) {\n            }\n            System.out.println(i);\n        }\n    }\n\n    public static void main(String[] args) {\n        Thread daemonTest = new DaemonTest();\n        daemonTest.setDaemon(true);    //调试时可以设置为false，那么这个程序是个死循环，没有退出条件。设置为true，即可主线程结束，test线程也结束。\n        daemonTest.start();\n        System.out.println(\"isDaemon = \" + daemonTest.isDaemon());\n        try {\n            System.in.read();   // 接受输入，使程序在此停顿，一旦接收到用户输入，main线程结束，守护线程自动结束\n        } catch (IOException ex) {\n        }\n    }\n}\n```\n如果线程设置为Thread.setDaemon(true)，则主线程结束该程序不会结束，必须等待子线程执行结束整个程序才能结束；如果线程设置为Thread.setDaemon(false)，则主线程结束整个程序就结束。\n\n5、wait、notify、notifyAll\n当一个线程进入wait之后，就必须等待其他线程notify/notifyAll，使用notifyAll可以唤醒所有处于wait状态的线程，使其重新进入锁的争夺队列中，而notify只能唤醒一个。\nnotify被唤醒的线程是随机的，所以通常是没办法指定是谁被唤醒。\n\n### 线程关键字\n* volatile\n可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入\n原子性：对任意单个volatile变量的读/写具有原子性，但是类似于volatile++这种复合操作不具有原子性\n* synchronized\n当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。\nsynchronized 关键字，它包括两种用法：synchronized 方法和 synchronized 块。  \n\t* synchronized 方法：通过在方法声明中加入 synchronized关键字来声明 synchronized 方法\n```java\npublic synchronized void accessVal(int newVal);  \n```\n\t* synchronized 块：通过 synchronized关键字来声明synchronized 块\n```java\nsynchronized(syncObject) {  \n//允许访问控制的代码  \n}  \n```\n\n### ThreadLocal\nThreadLocal是一个关于创建线程局部变量的类。通常情况下，我们创建的变量是可以被任何一个线程访问修改的。而使用ThreadLocal创建的变量只能被当前线程访问，其他线程无法访问和修改。\n> 在Java中，栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存。而堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。\n\nThreadLocal并不是存放在栈上。ThreadLocal实例实际上也是被其创建的类持有（更顶端应该是被线程持有）。而ThreadLocal的值其实也是被线程实例持有。它们都是位于堆上，只是通过一些技巧将可见性修改成线程可见。\n```java\n//使用ThreadLocal保存Connection变量 \nThreadLocal<Connection> connThreadLocal = new ThreadLocal<Connection>(); \n//保存到线程本地变量中\nconnThreadLocal.set(conn);  \n// 直接返回线程本地变量  \nconnThreadLocal.get();\n\n```\n","slug":"Java基础-Thread概述","published":1,"updated":"2024-04-07T07:52:49.496Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrp0009obns5xdx727j","content":"<h3 id=\"线程生命周期\"><a href=\"#线程生命周期\" class=\"headerlink\" title=\"线程生命周期\"></a>线程生命周期</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-7b0fe1fa20076f52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620\" alt=\"Threadlife_20170208.jpg\"><br>线程状态：</p>\n<ul>\n<li>新建状态：使用new关键字和Thread类或其子类建立一个线程对象后</li>\n<li>就绪状态：当线程对象调用了start()方法之后，该线程进入就绪状态</li>\n<li>运行状态：如果就绪状态的线程获取CPU资源，就可以执行run()</li>\n<li>阻塞状态：<ul>\n<li>等待阻塞：运行状态中的线程执行wait()方法</li>\n<li>同步阻塞：线程在获取 synchronized 同步锁失败</li>\n<li>其他阻塞：通过调用线程的sleep()或join()发出I&#x2F;O请求时</li>\n</ul>\n</li>\n<li>死亡状态：一个运行状态的线程完成人文或者其他终止条件发送</li>\n</ul>\n<h3 id=\"创建线程\"><a href=\"#创建线程\" class=\"headerlink\" title=\"创建线程\"></a>创建线程</h3><p>1、继承Thread类</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ThreadDemo</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Thread</span></span><br></pre></td></tr></table></figure>\n<p>2、实现Runable接口</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RunTest</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Runnable</span></span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"type\">Thread</span> <span class=\"variable\">thread1</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">RunTest</span>());</span><br></pre></td></tr></table></figure>\n<p>3、通过Callable、Future、FutureTask创建线程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 继承实现Callable接口，声明返回类型</span><br><span class=\"line\">class CallTest implements Callable&lt;Long&gt; </span><br><span class=\"line\">    </span><br><span class=\"line\">FutureTask&lt;Long&gt; futureTask = new FutureTask&lt;Long&gt;(new CallTest(searchVo));</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"线程内置方法\"><a href=\"#线程内置方法\" class=\"headerlink\" title=\"线程内置方法\"></a>线程内置方法</h3><p>1、sleep：使当前线程（即调用该方法的线程）暂停执行一段时间，让其它线程有机会继续执行，但它不会释放对象锁。<br>2、yield：<strong>暂停__当前正在执行的线程对象，并执行其他线程。yield的目的是让相同优先级的线程之间能适当的__轮转执行</strong><br>3、join：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如线程B中调用了线程A的JOIN()方法，直到线程A执行完毕后，才会继续执行线程B。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//主线程等待子线程thread执行结束才会输出结果</span><br><span class=\"line\">public static void main(String[] args) throws InterruptedException &#123;</span><br><span class=\"line\">    Thread thread = new Thread(new JoinTest());</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">    thread.join(); //加入join()</span><br><span class=\"line\">    System.out.println(&quot;主线程结束&quot;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>4、setDaemon：<br>Java中线程分为两种类型：用户线程和守护线程。通过Thread.setDaemon(false)设置为用户线程；通过Thread.setDaemon(true)设置为守护线程。如果不设置次属性，默认为用户线程。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DaemonTest</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Thread</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;            <span class=\"comment\">//永真循环线程</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>; ; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                Thread.sleep(<span class=\"number\">1000</span>);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (InterruptedException ex) &#123;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            System.out.println(i);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Thread</span> <span class=\"variable\">daemonTest</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DaemonTest</span>();</span><br><span class=\"line\">        daemonTest.setDaemon(<span class=\"literal\">true</span>);    <span class=\"comment\">//调试时可以设置为false，那么这个程序是个死循环，没有退出条件。设置为true，即可主线程结束，test线程也结束。</span></span><br><span class=\"line\">        daemonTest.start();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;isDaemon = &quot;</span> + daemonTest.isDaemon());</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            System.in.read();   <span class=\"comment\">// 接受输入，使程序在此停顿，一旦接收到用户输入，main线程结束，守护线程自动结束</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException ex) &#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果线程设置为Thread.setDaemon(true)，则主线程结束该程序不会结束，必须等待子线程执行结束整个程序才能结束；如果线程设置为Thread.setDaemon(false)，则主线程结束整个程序就结束。</p>\n<p>5、wait、notify、notifyAll<br>当一个线程进入wait之后，就必须等待其他线程notify&#x2F;notifyAll，使用notifyAll可以唤醒所有处于wait状态的线程，使其重新进入锁的争夺队列中，而notify只能唤醒一个。<br>notify被唤醒的线程是随机的，所以通常是没办法指定是谁被唤醒。</p>\n<h3 id=\"线程关键字\"><a href=\"#线程关键字\" class=\"headerlink\" title=\"线程关键字\"></a>线程关键字</h3><ul>\n<li>volatile<br>可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入<br>原子性：对任意单个volatile变量的读&#x2F;写具有原子性，但是类似于volatile++这种复合操作不具有原子性</li>\n<li>synchronized<br>当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。<br>synchronized 关键字，它包括两种用法：synchronized 方法和 synchronized 块。  <ul>\n<li>synchronized 方法：通过在方法声明中加入 synchronized关键字来声明 synchronized 方法<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title function_\">accessVal</span><span class=\"params\">(<span class=\"type\">int</span> newVal)</span>;  </span><br></pre></td></tr></table></figure></li>\n<li>synchronized 块：通过 synchronized关键字来声明synchronized 块<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">synchronized</span>(syncObject) &#123;  </span><br><span class=\"line\"><span class=\"comment\">//允许访问控制的代码  </span></span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"ThreadLocal\"><a href=\"#ThreadLocal\" class=\"headerlink\" title=\"ThreadLocal\"></a>ThreadLocal</h3><p>ThreadLocal是一个关于创建线程局部变量的类。通常情况下，我们创建的变量是可以被任何一个线程访问修改的。而使用ThreadLocal创建的变量只能被当前线程访问，其他线程无法访问和修改。</p>\n<blockquote>\n<p>在Java中，栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存。而堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。</p>\n</blockquote>\n<p>ThreadLocal并不是存放在栈上。ThreadLocal实例实际上也是被其创建的类持有（更顶端应该是被线程持有）。而ThreadLocal的值其实也是被线程实例持有。它们都是位于堆上，只是通过一些技巧将可见性修改成线程可见。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//使用ThreadLocal保存Connection变量 </span></span><br><span class=\"line\">ThreadLocal&lt;Connection&gt; connThreadLocal = <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadLocal</span>&lt;Connection&gt;(); </span><br><span class=\"line\"><span class=\"comment\">//保存到线程本地变量中</span></span><br><span class=\"line\">connThreadLocal.set(conn);  </span><br><span class=\"line\"><span class=\"comment\">// 直接返回线程本地变量  </span></span><br><span class=\"line\">connThreadLocal.get();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n","cover":false,"excerpt":"","more":"<h3 id=\"线程生命周期\"><a href=\"#线程生命周期\" class=\"headerlink\" title=\"线程生命周期\"></a>线程生命周期</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-7b0fe1fa20076f52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620\" alt=\"Threadlife_20170208.jpg\"><br>线程状态：</p>\n<ul>\n<li>新建状态：使用new关键字和Thread类或其子类建立一个线程对象后</li>\n<li>就绪状态：当线程对象调用了start()方法之后，该线程进入就绪状态</li>\n<li>运行状态：如果就绪状态的线程获取CPU资源，就可以执行run()</li>\n<li>阻塞状态：<ul>\n<li>等待阻塞：运行状态中的线程执行wait()方法</li>\n<li>同步阻塞：线程在获取 synchronized 同步锁失败</li>\n<li>其他阻塞：通过调用线程的sleep()或join()发出I&#x2F;O请求时</li>\n</ul>\n</li>\n<li>死亡状态：一个运行状态的线程完成人文或者其他终止条件发送</li>\n</ul>\n<h3 id=\"创建线程\"><a href=\"#创建线程\" class=\"headerlink\" title=\"创建线程\"></a>创建线程</h3><p>1、继承Thread类</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ThreadDemo</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Thread</span></span><br></pre></td></tr></table></figure>\n<p>2、实现Runable接口</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">RunTest</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">Runnable</span></span><br><span class=\"line\">   </span><br><span class=\"line\"><span class=\"type\">Thread</span> <span class=\"variable\">thread1</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Thread</span>(<span class=\"keyword\">new</span> <span class=\"title class_\">RunTest</span>());</span><br></pre></td></tr></table></figure>\n<p>3、通过Callable、Future、FutureTask创建线程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 继承实现Callable接口，声明返回类型</span><br><span class=\"line\">class CallTest implements Callable&lt;Long&gt; </span><br><span class=\"line\">    </span><br><span class=\"line\">FutureTask&lt;Long&gt; futureTask = new FutureTask&lt;Long&gt;(new CallTest(searchVo));</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"线程内置方法\"><a href=\"#线程内置方法\" class=\"headerlink\" title=\"线程内置方法\"></a>线程内置方法</h3><p>1、sleep：使当前线程（即调用该方法的线程）暂停执行一段时间，让其它线程有机会继续执行，但它不会释放对象锁。<br>2、yield：<strong>暂停__当前正在执行的线程对象，并执行其他线程。yield的目的是让相同优先级的线程之间能适当的__轮转执行</strong><br>3、join：把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如线程B中调用了线程A的JOIN()方法，直到线程A执行完毕后，才会继续执行线程B。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//主线程等待子线程thread执行结束才会输出结果</span><br><span class=\"line\">public static void main(String[] args) throws InterruptedException &#123;</span><br><span class=\"line\">    Thread thread = new Thread(new JoinTest());</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">    thread.join(); //加入join()</span><br><span class=\"line\">    System.out.println(&quot;主线程结束&quot;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>4、setDaemon：<br>Java中线程分为两种类型：用户线程和守护线程。通过Thread.setDaemon(false)设置为用户线程；通过Thread.setDaemon(true)设置为守护线程。如果不设置次属性，默认为用户线程。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  <span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DaemonTest</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">Thread</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;            <span class=\"comment\">//永真循环线程</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>; ; i++) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                Thread.sleep(<span class=\"number\">1000</span>);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (InterruptedException ex) &#123;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            System.out.println(i);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Thread</span> <span class=\"variable\">daemonTest</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DaemonTest</span>();</span><br><span class=\"line\">        daemonTest.setDaemon(<span class=\"literal\">true</span>);    <span class=\"comment\">//调试时可以设置为false，那么这个程序是个死循环，没有退出条件。设置为true，即可主线程结束，test线程也结束。</span></span><br><span class=\"line\">        daemonTest.start();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;isDaemon = &quot;</span> + daemonTest.isDaemon());</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            System.in.read();   <span class=\"comment\">// 接受输入，使程序在此停顿，一旦接收到用户输入，main线程结束，守护线程自动结束</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException ex) &#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果线程设置为Thread.setDaemon(true)，则主线程结束该程序不会结束，必须等待子线程执行结束整个程序才能结束；如果线程设置为Thread.setDaemon(false)，则主线程结束整个程序就结束。</p>\n<p>5、wait、notify、notifyAll<br>当一个线程进入wait之后，就必须等待其他线程notify&#x2F;notifyAll，使用notifyAll可以唤醒所有处于wait状态的线程，使其重新进入锁的争夺队列中，而notify只能唤醒一个。<br>notify被唤醒的线程是随机的，所以通常是没办法指定是谁被唤醒。</p>\n<h3 id=\"线程关键字\"><a href=\"#线程关键字\" class=\"headerlink\" title=\"线程关键字\"></a>线程关键字</h3><ul>\n<li>volatile<br>可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入<br>原子性：对任意单个volatile变量的读&#x2F;写具有原子性，但是类似于volatile++这种复合操作不具有原子性</li>\n<li>synchronized<br>当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。<br>synchronized 关键字，它包括两种用法：synchronized 方法和 synchronized 块。  <ul>\n<li>synchronized 方法：通过在方法声明中加入 synchronized关键字来声明 synchronized 方法<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">synchronized</span> <span class=\"keyword\">void</span> <span class=\"title function_\">accessVal</span><span class=\"params\">(<span class=\"type\">int</span> newVal)</span>;  </span><br></pre></td></tr></table></figure></li>\n<li>synchronized 块：通过 synchronized关键字来声明synchronized 块<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">synchronized</span>(syncObject) &#123;  </span><br><span class=\"line\"><span class=\"comment\">//允许访问控制的代码  </span></span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"ThreadLocal\"><a href=\"#ThreadLocal\" class=\"headerlink\" title=\"ThreadLocal\"></a>ThreadLocal</h3><p>ThreadLocal是一个关于创建线程局部变量的类。通常情况下，我们创建的变量是可以被任何一个线程访问修改的。而使用ThreadLocal创建的变量只能被当前线程访问，其他线程无法访问和修改。</p>\n<blockquote>\n<p>在Java中，栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存。而堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。</p>\n</blockquote>\n<p>ThreadLocal并不是存放在栈上。ThreadLocal实例实际上也是被其创建的类持有（更顶端应该是被线程持有）。而ThreadLocal的值其实也是被线程实例持有。它们都是位于堆上，只是通过一些技巧将可见性修改成线程可见。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//使用ThreadLocal保存Connection变量 </span></span><br><span class=\"line\">ThreadLocal&lt;Connection&gt; connThreadLocal = <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadLocal</span>&lt;Connection&gt;(); </span><br><span class=\"line\"><span class=\"comment\">//保存到线程本地变量中</span></span><br><span class=\"line\">connThreadLocal.set(conn);  </span><br><span class=\"line\"><span class=\"comment\">// 直接返回线程本地变量  </span></span><br><span class=\"line\">connThreadLocal.get();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n"},{"title":"Kafka 概述","date":"2017-02-13T12:11:17.000Z","_content":"### Kafka架构\n* Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker\n* Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）\n\n* Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。\n* Producer：负责发布消息到Kafka broker\n* Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。\n* Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nKafka拓扑结构：\n\n![kafka-tupe-20170213.png](http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### Kafka的Topic & Partition\nTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。\n消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：\n![kafka_log_anatomy_20170213.png](http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。\n\n### Kafka 分区机制\nKafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有**消息（.log）**和**索引文件（.index）**，这使得Kafka的吞吐率可以水平扩展。\n生产者在生产数据的时候，可以**为每条消息指定Key**，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。\n默认kakfa.producer.Partitioner接口的类\n```\nclass DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner {\n  def partition(key: Any, numPartitions: Int): Int = {\n    Utils.abs(key.hashCode) % numPartitions\n  }\n}\n```\n\n### Consumer Group\nKafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。\n* 单播：所有Consumer在同一个Croup里\n* 广播：每个Consumer有一个独立的Group\n\n使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。\n\n### Producer\nProducer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。\nproducer 写入消息序列图:\n\n![kafka-producer-20170213.png.png](http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n流程说明:\n```\n1. producer 先从 zookeeper 的 \"/brokers/.../state\" 节点找到该 partition 的 leader\n2. producer 将消息发送给该 leader\n3. leader 将消息写入本地 log\n4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK\n5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK\n```\n\n### Kafka Shell\n启动：\n```\n ./kafka-server-start.sh ../config/server.properties > /dev/null &\n```\n查看topic：\n```\n./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list\n```\n创建topic：\n```\n./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY\n#  replication-factor 副本参数\n#  partitions 分区参数\n\n```\n删除topic:\n```\n./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete\n```\n消费者：\n```\n./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning\n```\n生产者：\n```\n./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY\n```\n    \n\n参考：\nhttp://www.infoq.com/cn/articles/kafka-analysis-part-1/\nhttp://www.cnblogs.com/cyfonly/p/5954614.html\n\n","source":"_posts/Kafka-概述.md","raw":"---\ntitle: Kafka 概述\ndate: 2017-02-13 20:11:17\ntags: [kafka]\ncategories: [kafka]\n---\n### Kafka架构\n* Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker\n* Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）\n\n* Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。\n* Producer：负责发布消息到Kafka broker\n* Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。\n* Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nKafka拓扑结构：\n\n![kafka-tupe-20170213.png](http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### Kafka的Topic & Partition\nTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。\n消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：\n![kafka_log_anatomy_20170213.png](http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。\n\n### Kafka 分区机制\nKafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有**消息（.log）**和**索引文件（.index）**，这使得Kafka的吞吐率可以水平扩展。\n生产者在生产数据的时候，可以**为每条消息指定Key**，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。\n默认kakfa.producer.Partitioner接口的类\n```\nclass DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner {\n  def partition(key: Any, numPartitions: Int): Int = {\n    Utils.abs(key.hashCode) % numPartitions\n  }\n}\n```\n\n### Consumer Group\nKafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。\n* 单播：所有Consumer在同一个Croup里\n* 广播：每个Consumer有一个独立的Group\n\n使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。\n\n### Producer\nProducer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。\nproducer 写入消息序列图:\n\n![kafka-producer-20170213.png.png](http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n流程说明:\n```\n1. producer 先从 zookeeper 的 \"/brokers/.../state\" 节点找到该 partition 的 leader\n2. producer 将消息发送给该 leader\n3. leader 将消息写入本地 log\n4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK\n5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK\n```\n\n### Kafka Shell\n启动：\n```\n ./kafka-server-start.sh ../config/server.properties > /dev/null &\n```\n查看topic：\n```\n./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list\n```\n创建topic：\n```\n./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY\n#  replication-factor 副本参数\n#  partitions 分区参数\n\n```\n删除topic:\n```\n./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete\n```\n消费者：\n```\n./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning\n```\n生产者：\n```\n./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY\n```\n    \n\n参考：\nhttp://www.infoq.com/cn/articles/kafka-analysis-part-1/\nhttp://www.cnblogs.com/cyfonly/p/5954614.html\n\n","slug":"Kafka-概述","published":1,"updated":"2024-04-07T07:42:55.399Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrs000dobns5ndb9ou6","content":"<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><ul>\n<li><p>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</p>\n</li>\n<li><p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>\n</li>\n<li><p>Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。</p>\n</li>\n<li><p>Producer：负责发布消息到Kafka broker</p>\n</li>\n<li><p>Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。</p>\n</li>\n<li><p>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>\n</li>\n</ul>\n<p>Kafka拓扑结构：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-tupe-20170213.png\"></p>\n<h3 id=\"Kafka的Topic-Partition\"><a href=\"#Kafka的Topic-Partition\" class=\"headerlink\" title=\"Kafka的Topic &amp; Partition\"></a>Kafka的Topic &amp; Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。<br>消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka_log_anatomy_20170213.png\"><br>因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>\n<h3 id=\"Kafka-分区机制\"><a href=\"#Kafka-分区机制\" class=\"headerlink\" title=\"Kafka 分区机制\"></a>Kafka 分区机制</h3><p>Kafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有<strong>消息（.log）</strong>和<strong>索引文件（.index）</strong>，这使得Kafka的吞吐率可以水平扩展。<br>生产者在生产数据的时候，可以<strong>为每条消息指定Key</strong>，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。<br>默认kakfa.producer.Partitioner接口的类</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner &#123;</span><br><span class=\"line\">  def partition(key: Any, numPartitions: Int): Int = &#123;</span><br><span class=\"line\">    Utils.abs(key.hashCode) % numPartitions</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Consumer-Group\"><a href=\"#Consumer-Group\" class=\"headerlink\" title=\"Consumer Group\"></a>Consumer Group</h3><p>Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>\n<ul>\n<li>单播：所有Consumer在同一个Croup里</li>\n<li>广播：每个Consumer有一个独立的Group</li>\n</ul>\n<p>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。</p>\n<h3 id=\"Producer\"><a href=\"#Producer\" class=\"headerlink\" title=\"Producer\"></a>Producer</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。<br>producer 写入消息序列图:</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-producer-20170213.png.png\"></p>\n<p>流程说明:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader</span><br><span class=\"line\">2. producer 将消息发送给该 leader</span><br><span class=\"line\">3. leader 将消息写入本地 log</span><br><span class=\"line\">4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</span><br><span class=\"line\">5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Kafka-Shell\"><a href=\"#Kafka-Shell\" class=\"headerlink\" title=\"Kafka Shell\"></a>Kafka Shell</h3><p>启动：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-server-start.sh ../config/server.properties &gt; /dev/null &amp;</span><br></pre></td></tr></table></figure>\n<p>查看topic：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list</span><br></pre></td></tr></table></figure>\n<p>创建topic：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY</span><br><span class=\"line\">#  replication-factor 副本参数</span><br><span class=\"line\">#  partitions 分区参数</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>删除topic:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete</span><br></pre></td></tr></table></figure>\n<p>消费者：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning</span><br></pre></td></tr></table></figure>\n<p>生产者：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY</span><br></pre></td></tr></table></figure>\n<p>参考：<br><a href=\"http://www.infoq.com/cn/articles/kafka-analysis-part-1/\">http://www.infoq.com/cn/articles/kafka-analysis-part-1/</a><br><a href=\"http://www.cnblogs.com/cyfonly/p/5954614.html\">http://www.cnblogs.com/cyfonly/p/5954614.html</a></p>\n","cover":false,"excerpt":"","more":"<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><ul>\n<li><p>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</p>\n</li>\n<li><p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p>\n</li>\n<li><p>Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。</p>\n</li>\n<li><p>Producer：负责发布消息到Kafka broker</p>\n</li>\n<li><p>Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。</p>\n</li>\n<li><p>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p>\n</li>\n</ul>\n<p>Kafka拓扑结构：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-tupe-20170213.png\"></p>\n<h3 id=\"Kafka的Topic-Partition\"><a href=\"#Kafka的Topic-Partition\" class=\"headerlink\" title=\"Kafka的Topic &amp; Partition\"></a>Kafka的Topic &amp; Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。<br>消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka_log_anatomy_20170213.png\"><br>因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>\n<h3 id=\"Kafka-分区机制\"><a href=\"#Kafka-分区机制\" class=\"headerlink\" title=\"Kafka 分区机制\"></a>Kafka 分区机制</h3><p>Kafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有<strong>消息（.log）</strong>和<strong>索引文件（.index）</strong>，这使得Kafka的吞吐率可以水平扩展。<br>生产者在生产数据的时候，可以<strong>为每条消息指定Key</strong>，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。<br>默认kakfa.producer.Partitioner接口的类</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner &#123;</span><br><span class=\"line\">  def partition(key: Any, numPartitions: Int): Int = &#123;</span><br><span class=\"line\">    Utils.abs(key.hashCode) % numPartitions</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Consumer-Group\"><a href=\"#Consumer-Group\" class=\"headerlink\" title=\"Consumer Group\"></a>Consumer Group</h3><p>Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>\n<ul>\n<li>单播：所有Consumer在同一个Croup里</li>\n<li>广播：每个Consumer有一个独立的Group</li>\n</ul>\n<p>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。</p>\n<h3 id=\"Producer\"><a href=\"#Producer\" class=\"headerlink\" title=\"Producer\"></a>Producer</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。<br>producer 写入消息序列图:</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-producer-20170213.png.png\"></p>\n<p>流程说明:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader</span><br><span class=\"line\">2. producer 将消息发送给该 leader</span><br><span class=\"line\">3. leader 将消息写入本地 log</span><br><span class=\"line\">4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</span><br><span class=\"line\">5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Kafka-Shell\"><a href=\"#Kafka-Shell\" class=\"headerlink\" title=\"Kafka Shell\"></a>Kafka Shell</h3><p>启动：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-server-start.sh ../config/server.properties &gt; /dev/null &amp;</span><br></pre></td></tr></table></figure>\n<p>查看topic：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list</span><br></pre></td></tr></table></figure>\n<p>创建topic：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY</span><br><span class=\"line\">#  replication-factor 副本参数</span><br><span class=\"line\">#  partitions 分区参数</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>删除topic:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete</span><br></pre></td></tr></table></figure>\n<p>消费者：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning</span><br></pre></td></tr></table></figure>\n<p>生产者：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY</span><br></pre></td></tr></table></figure>\n<p>参考：<br><a href=\"http://www.infoq.com/cn/articles/kafka-analysis-part-1/\">http://www.infoq.com/cn/articles/kafka-analysis-part-1/</a><br><a href=\"http://www.cnblogs.com/cyfonly/p/5954614.html\">http://www.cnblogs.com/cyfonly/p/5954614.html</a></p>\n"},{"title":"RPC原理 概述","date":"2017-03-02T01:40:18.000Z","_content":"远程过程调用（Remote Procedure Call，缩写为RPC）是一种计算机通信协议。该协议运行运行与一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。——维基百科\n\n### RPC调用分类\n同步调用：客户方等待调用执行完成并返回结果\n异步调用：客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。 若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果\n\n### RPC结构\n\n![rpc-20170302.jpg](http://upload-images.jianshu.io/upload_images/1419542-29bfe0b7d38005c1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/960)\nRPC 服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。 客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 RpcProxy 。 代理封装调用信息并将调用转交给 RpcInvoker 去实际执行。 在客户端的 RpcInvoker 通过连接器 RpcConnector 去维持与服务端的通道 RpcChannel， 并使用 RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。\nRPC 服务端接收器 RpcAcceptor 接收客户端的调用请求，同样使用 RpcProtocol 执行协议解码（decode）。 解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给 RpcInvoker 去实际执行并返回调用结果。\n\n### RPC组件职责\n1. RpcServer：负责导出（export）远程接口\n2. RpcClient：负责导入（import）远程接口的代理实现\n3. RpcProxy：远程接口的代理实现\n4. RpcInvoker：\n    1. 客户端实现方式：负责编码调用信息和发送调用请求到服务方并等待调用结果返回\n    2. 服务端实现方式：负责调用服务端接口的具体实现并返回调用结果\n5. RpcProtocol：负责协议编/解码\n6. RpcConnector：负责维持客户方和服务方的连接通道和发送数据到服务方\n7. RpcAcceptor：负责接收客户方请求并返回请求结果\n8. RpcProcessor：负责在服务方控制调用过程，包括管理调用线程池、超时时间等\n9. RpcChannel：数据传输通道\n\n参考：\nhttp://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&utm_medium=referral\nhttp://www.cnblogs.com/LBSer/p/4853234.html","source":"_posts/RPC原理-概述.md","raw":"---\ntitle: RPC原理 概述\ndate: 2017-03-02 09:40:18\ntags: [rpc]\n---\n远程过程调用（Remote Procedure Call，缩写为RPC）是一种计算机通信协议。该协议运行运行与一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。——维基百科\n\n### RPC调用分类\n同步调用：客户方等待调用执行完成并返回结果\n异步调用：客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。 若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果\n\n### RPC结构\n\n![rpc-20170302.jpg](http://upload-images.jianshu.io/upload_images/1419542-29bfe0b7d38005c1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/960)\nRPC 服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。 客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 RpcProxy 。 代理封装调用信息并将调用转交给 RpcInvoker 去实际执行。 在客户端的 RpcInvoker 通过连接器 RpcConnector 去维持与服务端的通道 RpcChannel， 并使用 RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。\nRPC 服务端接收器 RpcAcceptor 接收客户端的调用请求，同样使用 RpcProtocol 执行协议解码（decode）。 解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给 RpcInvoker 去实际执行并返回调用结果。\n\n### RPC组件职责\n1. RpcServer：负责导出（export）远程接口\n2. RpcClient：负责导入（import）远程接口的代理实现\n3. RpcProxy：远程接口的代理实现\n4. RpcInvoker：\n    1. 客户端实现方式：负责编码调用信息和发送调用请求到服务方并等待调用结果返回\n    2. 服务端实现方式：负责调用服务端接口的具体实现并返回调用结果\n5. RpcProtocol：负责协议编/解码\n6. RpcConnector：负责维持客户方和服务方的连接通道和发送数据到服务方\n7. RpcAcceptor：负责接收客户方请求并返回请求结果\n8. RpcProcessor：负责在服务方控制调用过程，包括管理调用线程池、超时时间等\n9. RpcChannel：数据传输通道\n\n参考：\nhttp://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&utm_medium=referral\nhttp://www.cnblogs.com/LBSer/p/4853234.html","slug":"RPC原理-概述","published":1,"updated":"2024-04-07T07:42:55.400Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrt000eobns4hs7edfn","content":"<p>远程过程调用（Remote Procedure Call，缩写为RPC）是一种计算机通信协议。该协议运行运行与一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。——维基百科</p>\n<h3 id=\"RPC调用分类\"><a href=\"#RPC调用分类\" class=\"headerlink\" title=\"RPC调用分类\"></a>RPC调用分类</h3><p>同步调用：客户方等待调用执行完成并返回结果<br>异步调用：客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。 若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果</p>\n<h3 id=\"RPC结构\"><a href=\"#RPC结构\" class=\"headerlink\" title=\"RPC结构\"></a>RPC结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-29bfe0b7d38005c1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/960\" alt=\"rpc-20170302.jpg\"><br>RPC 服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。 客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 RpcProxy 。 代理封装调用信息并将调用转交给 RpcInvoker 去实际执行。 在客户端的 RpcInvoker 通过连接器 RpcConnector 去维持与服务端的通道 RpcChannel， 并使用 RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。<br>RPC 服务端接收器 RpcAcceptor 接收客户端的调用请求，同样使用 RpcProtocol 执行协议解码（decode）。 解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给 RpcInvoker 去实际执行并返回调用结果。</p>\n<h3 id=\"RPC组件职责\"><a href=\"#RPC组件职责\" class=\"headerlink\" title=\"RPC组件职责\"></a>RPC组件职责</h3><ol>\n<li>RpcServer：负责导出（export）远程接口</li>\n<li>RpcClient：负责导入（import）远程接口的代理实现</li>\n<li>RpcProxy：远程接口的代理实现</li>\n<li>RpcInvoker：<ol>\n<li>客户端实现方式：负责编码调用信息和发送调用请求到服务方并等待调用结果返回</li>\n<li>服务端实现方式：负责调用服务端接口的具体实现并返回调用结果</li>\n</ol>\n</li>\n<li>RpcProtocol：负责协议编&#x2F;解码</li>\n<li>RpcConnector：负责维持客户方和服务方的连接通道和发送数据到服务方</li>\n<li>RpcAcceptor：负责接收客户方请求并返回请求结果</li>\n<li>RpcProcessor：负责在服务方控制调用过程，包括管理调用线程池、超时时间等</li>\n<li>RpcChannel：数据传输通道</li>\n</ol>\n<p>参考：<br><a href=\"http://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&utm_medium=referral\">http://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&amp;utm_medium=referral</a><br><a href=\"http://www.cnblogs.com/LBSer/p/4853234.html\">http://www.cnblogs.com/LBSer/p/4853234.html</a></p>\n","cover":false,"excerpt":"","more":"<p>远程过程调用（Remote Procedure Call，缩写为RPC）是一种计算机通信协议。该协议运行运行与一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。——维基百科</p>\n<h3 id=\"RPC调用分类\"><a href=\"#RPC调用分类\" class=\"headerlink\" title=\"RPC调用分类\"></a>RPC调用分类</h3><p>同步调用：客户方等待调用执行完成并返回结果<br>异步调用：客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。 若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果</p>\n<h3 id=\"RPC结构\"><a href=\"#RPC结构\" class=\"headerlink\" title=\"RPC结构\"></a>RPC结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-29bfe0b7d38005c1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/960\" alt=\"rpc-20170302.jpg\"><br>RPC 服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。 客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 RpcProxy 。 代理封装调用信息并将调用转交给 RpcInvoker 去实际执行。 在客户端的 RpcInvoker 通过连接器 RpcConnector 去维持与服务端的通道 RpcChannel， 并使用 RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。<br>RPC 服务端接收器 RpcAcceptor 接收客户端的调用请求，同样使用 RpcProtocol 执行协议解码（decode）。 解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给 RpcInvoker 去实际执行并返回调用结果。</p>\n<h3 id=\"RPC组件职责\"><a href=\"#RPC组件职责\" class=\"headerlink\" title=\"RPC组件职责\"></a>RPC组件职责</h3><ol>\n<li>RpcServer：负责导出（export）远程接口</li>\n<li>RpcClient：负责导入（import）远程接口的代理实现</li>\n<li>RpcProxy：远程接口的代理实现</li>\n<li>RpcInvoker：<ol>\n<li>客户端实现方式：负责编码调用信息和发送调用请求到服务方并等待调用结果返回</li>\n<li>服务端实现方式：负责调用服务端接口的具体实现并返回调用结果</li>\n</ol>\n</li>\n<li>RpcProtocol：负责协议编&#x2F;解码</li>\n<li>RpcConnector：负责维持客户方和服务方的连接通道和发送数据到服务方</li>\n<li>RpcAcceptor：负责接收客户方请求并返回请求结果</li>\n<li>RpcProcessor：负责在服务方控制调用过程，包括管理调用线程池、超时时间等</li>\n<li>RpcChannel：数据传输通道</li>\n</ol>\n<p>参考：<br><a href=\"http://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&utm_medium=referral\">http://www.cnblogs.com/metoy/p/4321311.html?utm_source=tuicool&amp;utm_medium=referral</a><br><a href=\"http://www.cnblogs.com/LBSer/p/4853234.html\">http://www.cnblogs.com/LBSer/p/4853234.html</a></p>\n"},{"title":"Spark RDD详解","date":"2017-02-07T11:58:41.000Z","_content":"### RDD基本概念：\n* RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。\n* RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                \n* DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖\n* 窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data\n* 宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖\n\n<!-- more -->\n\n### RDD构建图\n![sparkRdd.jpg](http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n### RDD来源：\n*  parallelizing an existing collection in your driver program（程序内部已经存在的数据集）\n* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）\n\n\n\n### RDD特点：\n1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。\n2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。\n3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。\n4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。\n```\n// return the set of partitions in this RDD\nprotected def getPartitions: Array[Partition]\n/* 分片列表 */\n\n// compute a given partition\ndef compute(split: Partition, context: TaskContext): Iterator[T]\n/* compute 对分片进行计算,得出一个可遍历的结果 */\n\n// return how this RDD depends on parent RDDs\nprotected def getDependencies: Seq[Dependency[_]] = deps\n/* 只计算一次，计算RDD对父RDD的依赖 */\n\n@transient val partitioner: Option[Partitioner] = None\n /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */\n\nprotected def getPreferredLocations(split: Partition): Seq[String] = Nil\n/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/\n```\n\n\n### RDD操作\n* transformations：接受RDD并返回RDD\nTransformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。\n* action：接受RDD但是返回非RDD\nAction调用会直接计算。\n\n### RDD优化技巧\n* RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。\n可以通过`rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)`或`rdd.cache()`（就是调用persist）来缓存数据\n* 转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并\n* 减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。\n\n### RDD运行过程（具体在Spark任务调度中详细说明）\n1、创建RDD对象\n2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG\n3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销\n\n\n\n参考：\nhttp://www.cnblogs.com/bourneli/p/4394271.html\nhttp://www.jianshu.com/p/4ff6afbbafe4\nhttp://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\n\n### 附\nTransformation具体内容\n`map(func)` :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成\n`filter(func) `: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成\n`flatMap(func) `: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）\n`sample(withReplacement, frac, seed) `:根据给定的随机种子seed，随机抽样出数量为frac的数据\n`union(otherDataset)` : 返回一个新的数据集，由原数据集和参数联合而成\n`groupByKey([numTasks])` :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task\n`reduceByKey(func, [numTasks]) `: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。\n`join(otherDataset, [numTasks])` ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集\n`groupWith(otherDataset, [numTasks])` ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup\n`cartesian(otherDataset) `: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。\n\nActions具体内容\n`reduce(func) `: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行\n`collect()` : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM\n`count()` : 返回数据集的元素个数\n`take(n)` : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）\n`first()` : 返回数据集的第一个元素（类似于take(1)）\n`saveAsTextFile(path) `: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本\n`saveAsSequenceFile(path)` : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）\n`foreach(func)` : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互\n","source":"_posts/Spark-RDD详解.md","raw":"---\ntitle: Spark RDD详解\ndate: 2017-02-07 19:58:41\ntags: [spark]\ncategories: [spark]\n---\n### RDD基本概念：\n* RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。\n* RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                \n* DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖\n* 窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data\n* 宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖\n\n<!-- more -->\n\n### RDD构建图\n![sparkRdd.jpg](http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n### RDD来源：\n*  parallelizing an existing collection in your driver program（程序内部已经存在的数据集）\n* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）\n\n\n\n### RDD特点：\n1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。\n2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。\n3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。\n4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。\n```\n// return the set of partitions in this RDD\nprotected def getPartitions: Array[Partition]\n/* 分片列表 */\n\n// compute a given partition\ndef compute(split: Partition, context: TaskContext): Iterator[T]\n/* compute 对分片进行计算,得出一个可遍历的结果 */\n\n// return how this RDD depends on parent RDDs\nprotected def getDependencies: Seq[Dependency[_]] = deps\n/* 只计算一次，计算RDD对父RDD的依赖 */\n\n@transient val partitioner: Option[Partitioner] = None\n /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */\n\nprotected def getPreferredLocations(split: Partition): Seq[String] = Nil\n/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/\n```\n\n\n### RDD操作\n* transformations：接受RDD并返回RDD\nTransformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。\n* action：接受RDD但是返回非RDD\nAction调用会直接计算。\n\n### RDD优化技巧\n* RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。\n可以通过`rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)`或`rdd.cache()`（就是调用persist）来缓存数据\n* 转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并\n* 减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。\n\n### RDD运行过程（具体在Spark任务调度中详细说明）\n1、创建RDD对象\n2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG\n3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销\n\n\n\n参考：\nhttp://www.cnblogs.com/bourneli/p/4394271.html\nhttp://www.jianshu.com/p/4ff6afbbafe4\nhttp://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\n\n### 附\nTransformation具体内容\n`map(func)` :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成\n`filter(func) `: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成\n`flatMap(func) `: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）\n`sample(withReplacement, frac, seed) `:根据给定的随机种子seed，随机抽样出数量为frac的数据\n`union(otherDataset)` : 返回一个新的数据集，由原数据集和参数联合而成\n`groupByKey([numTasks])` :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task\n`reduceByKey(func, [numTasks]) `: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。\n`join(otherDataset, [numTasks])` ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集\n`groupWith(otherDataset, [numTasks])` ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup\n`cartesian(otherDataset) `: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。\n\nActions具体内容\n`reduce(func) `: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行\n`collect()` : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM\n`count()` : 返回数据集的元素个数\n`take(n)` : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）\n`first()` : 返回数据集的第一个元素（类似于take(1)）\n`saveAsTextFile(path) `: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本\n`saveAsSequenceFile(path)` : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）\n`foreach(func)` : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互\n","slug":"Spark-RDD详解","published":1,"updated":"2024-04-07T07:42:55.400Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrv000jobnsgtmf10h7","content":"<h3 id=\"RDD基本概念：\"><a href=\"#RDD基本概念：\" class=\"headerlink\" title=\"RDD基本概念：\"></a>RDD基本概念：</h3><ul>\n<li>RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。</li>\n<li>RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                </li>\n<li>DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖</li>\n<li>窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data</li>\n<li>宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"RDD构建图\"><a href=\"#RDD构建图\" class=\"headerlink\" title=\"RDD构建图\"></a>RDD构建图</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"sparkRdd.jpg\"></p>\n<h3 id=\"RDD来源：\"><a href=\"#RDD来源：\" class=\"headerlink\" title=\"RDD来源：\"></a>RDD来源：</h3><ul>\n<li>parallelizing an existing collection in your driver program（程序内部已经存在的数据集）</li>\n<li>referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）</li>\n</ul>\n<h3 id=\"RDD特点：\"><a href=\"#RDD特点：\" class=\"headerlink\" title=\"RDD特点：\"></a>RDD特点：</h3><p>1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。<br>2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。<br>3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。<br>4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。<br>5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// return the set of partitions in this RDD</span><br><span class=\"line\">protected def getPartitions: Array[Partition]</span><br><span class=\"line\">/* 分片列表 */</span><br><span class=\"line\"></span><br><span class=\"line\">// compute a given partition</span><br><span class=\"line\">def compute(split: Partition, context: TaskContext): Iterator[T]</span><br><span class=\"line\">/* compute 对分片进行计算,得出一个可遍历的结果 */</span><br><span class=\"line\"></span><br><span class=\"line\">// return how this RDD depends on parent RDDs</span><br><span class=\"line\">protected def getDependencies: Seq[Dependency[_]] = deps</span><br><span class=\"line\">/* 只计算一次，计算RDD对父RDD的依赖 */</span><br><span class=\"line\"></span><br><span class=\"line\">@transient val partitioner: Option[Partitioner] = None</span><br><span class=\"line\"> /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */</span><br><span class=\"line\"></span><br><span class=\"line\">protected def getPreferredLocations(split: Partition): Seq[String] = Nil</span><br><span class=\"line\">/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"RDD操作\"><a href=\"#RDD操作\" class=\"headerlink\" title=\"RDD操作\"></a>RDD操作</h3><ul>\n<li>transformations：接受RDD并返回RDD<br>Transformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。</li>\n<li>action：接受RDD但是返回非RDD<br>Action调用会直接计算。</li>\n</ul>\n<h3 id=\"RDD优化技巧\"><a href=\"#RDD优化技巧\" class=\"headerlink\" title=\"RDD优化技巧\"></a>RDD优化技巧</h3><ul>\n<li>RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。<br>可以通过<code>rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)</code>或<code>rdd.cache()</code>（就是调用persist）来缓存数据</li>\n<li>转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并</li>\n<li>减少shuffle网络传输：网络I&#x2F;O开销是很大的，减少网络开销，可以显著加快计算效率。</li>\n</ul>\n<h3 id=\"RDD运行过程（具体在Spark任务调度中详细说明）\"><a href=\"#RDD运行过程（具体在Spark任务调度中详细说明）\" class=\"headerlink\" title=\"RDD运行过程（具体在Spark任务调度中详细说明）\"></a>RDD运行过程（具体在Spark任务调度中详细说明）</h3><p>1、创建RDD对象<br>2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG<br>3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销</p>\n<p>参考：<br><a href=\"http://www.cnblogs.com/bourneli/p/4394271.html\">http://www.cnblogs.com/bourneli/p/4394271.html</a><br><a href=\"http://www.jianshu.com/p/4ff6afbbafe4\">http://www.jianshu.com/p/4ff6afbbafe4</a><br><a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds</a></p>\n<h3 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h3><p>Transformation具体内容<br><code>map(func)</code> :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成<br><code>filter(func) </code>: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成<br><code>flatMap(func) </code>: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）<br><code>sample(withReplacement, frac, seed) </code>:根据给定的随机种子seed，随机抽样出数量为frac的数据<br><code>union(otherDataset)</code> : 返回一个新的数据集，由原数据集和参数联合而成<br><code>groupByKey([numTasks])</code> :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task<br><code>reduceByKey(func, [numTasks]) </code>: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。<br><code>join(otherDataset, [numTasks])</code> ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集<br><code>groupWith(otherDataset, [numTasks])</code> ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup<br><code>cartesian(otherDataset) </code>: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。</p>\n<p>Actions具体内容<br><code>reduce(func) </code>: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行<br><code>collect()</code> : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM<br><code>count()</code> : 返回数据集的元素个数<br><code>take(n)</code> : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）<br><code>first()</code> : 返回数据集的第一个元素（类似于take(1)）<br><code>saveAsTextFile(path) </code>: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本<br><code>saveAsSequenceFile(path)</code> : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）<br><code>foreach(func)</code> : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p>\n","cover":false,"excerpt":"<h3 id=\"RDD基本概念：\"><a href=\"#RDD基本概念：\" class=\"headerlink\" title=\"RDD基本概念：\"></a>RDD基本概念：</h3><ul>\n<li>RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。</li>\n<li>RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                </li>\n<li>DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖</li>\n<li>窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data</li>\n<li>宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖</li>\n</ul>","more":"<h3 id=\"RDD构建图\"><a href=\"#RDD构建图\" class=\"headerlink\" title=\"RDD构建图\"></a>RDD构建图</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"sparkRdd.jpg\"></p>\n<h3 id=\"RDD来源：\"><a href=\"#RDD来源：\" class=\"headerlink\" title=\"RDD来源：\"></a>RDD来源：</h3><ul>\n<li>parallelizing an existing collection in your driver program（程序内部已经存在的数据集）</li>\n<li>referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）</li>\n</ul>\n<h3 id=\"RDD特点：\"><a href=\"#RDD特点：\" class=\"headerlink\" title=\"RDD特点：\"></a>RDD特点：</h3><p>1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。<br>2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。<br>3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。<br>4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。<br>5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// return the set of partitions in this RDD</span><br><span class=\"line\">protected def getPartitions: Array[Partition]</span><br><span class=\"line\">/* 分片列表 */</span><br><span class=\"line\"></span><br><span class=\"line\">// compute a given partition</span><br><span class=\"line\">def compute(split: Partition, context: TaskContext): Iterator[T]</span><br><span class=\"line\">/* compute 对分片进行计算,得出一个可遍历的结果 */</span><br><span class=\"line\"></span><br><span class=\"line\">// return how this RDD depends on parent RDDs</span><br><span class=\"line\">protected def getDependencies: Seq[Dependency[_]] = deps</span><br><span class=\"line\">/* 只计算一次，计算RDD对父RDD的依赖 */</span><br><span class=\"line\"></span><br><span class=\"line\">@transient val partitioner: Option[Partitioner] = None</span><br><span class=\"line\"> /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */</span><br><span class=\"line\"></span><br><span class=\"line\">protected def getPreferredLocations(split: Partition): Seq[String] = Nil</span><br><span class=\"line\">/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"RDD操作\"><a href=\"#RDD操作\" class=\"headerlink\" title=\"RDD操作\"></a>RDD操作</h3><ul>\n<li>transformations：接受RDD并返回RDD<br>Transformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。</li>\n<li>action：接受RDD但是返回非RDD<br>Action调用会直接计算。</li>\n</ul>\n<h3 id=\"RDD优化技巧\"><a href=\"#RDD优化技巧\" class=\"headerlink\" title=\"RDD优化技巧\"></a>RDD优化技巧</h3><ul>\n<li>RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。<br>可以通过<code>rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)</code>或<code>rdd.cache()</code>（就是调用persist）来缓存数据</li>\n<li>转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并</li>\n<li>减少shuffle网络传输：网络I&#x2F;O开销是很大的，减少网络开销，可以显著加快计算效率。</li>\n</ul>\n<h3 id=\"RDD运行过程（具体在Spark任务调度中详细说明）\"><a href=\"#RDD运行过程（具体在Spark任务调度中详细说明）\" class=\"headerlink\" title=\"RDD运行过程（具体在Spark任务调度中详细说明）\"></a>RDD运行过程（具体在Spark任务调度中详细说明）</h3><p>1、创建RDD对象<br>2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG<br>3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销</p>\n<p>参考：<br><a href=\"http://www.cnblogs.com/bourneli/p/4394271.html\">http://www.cnblogs.com/bourneli/p/4394271.html</a><br><a href=\"http://www.jianshu.com/p/4ff6afbbafe4\">http://www.jianshu.com/p/4ff6afbbafe4</a><br><a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds</a></p>\n<h3 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h3><p>Transformation具体内容<br><code>map(func)</code> :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成<br><code>filter(func) </code>: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成<br><code>flatMap(func) </code>: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）<br><code>sample(withReplacement, frac, seed) </code>:根据给定的随机种子seed，随机抽样出数量为frac的数据<br><code>union(otherDataset)</code> : 返回一个新的数据集，由原数据集和参数联合而成<br><code>groupByKey([numTasks])</code> :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task<br><code>reduceByKey(func, [numTasks]) </code>: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。<br><code>join(otherDataset, [numTasks])</code> ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集<br><code>groupWith(otherDataset, [numTasks])</code> ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup<br><code>cartesian(otherDataset) </code>: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。</p>\n<p>Actions具体内容<br><code>reduce(func) </code>: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行<br><code>collect()</code> : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM<br><code>count()</code> : 返回数据集的元素个数<br><code>take(n)</code> : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）<br><code>first()</code> : 返回数据集的第一个元素（类似于take(1)）<br><code>saveAsTextFile(path) </code>: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本<br><code>saveAsSequenceFile(path)</code> : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）<br><code>foreach(func)</code> : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p>"},{"title":"Spark Shuffle基础","date":"2017-02-06T07:45:36.000Z","_content":"### Shuffle 基本概念\n#### 概述：\n* Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。\n* 当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。\n* 由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。\n<!--more-->\n#### Spark 的Shuffle 分为 Write，Read 两阶段\n* Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责\n* Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的\n* 所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。\n\nShuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。\n\n### Spark的Shuffle机制\n> Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。\n\nShuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。\nShuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。\n\n\n### Shuffle的基本流程\n> bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等\n\n![shuffle-write-no-consolidation.png](http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n* 首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。\n* 其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。\n* 当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。\n\n### Spark中Shuffle类型\n* Hash Shuffle：\n第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。\n* Sort Shuffle：\n每个map产生一个文件，彻底解决了扩展性问题\n\n\n本文只是对Shuffle作了初步的描述，了解基本概念\n\n\n### 问题\n今天遇到如下问题，特来了解一下。\n```\n17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)\njava.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)\n     at java.io.FileOutputStream.open(Native Method)\n     at java.io.FileOutputStream.<init>(FileOutputStream.java:212)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n     at java.lang.Thread.run(Thread.java:722)\n```\n一般造成此问题的是系统资源不够用\n参考网上的解决方案,修改启动参数：\n* 添加：--conf spark.shuffle.manager=SORT\n Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销\n* 是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。\n     \n\n\n参考：\nhttp://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\nhttp://www.jianshu.com/p/c83bb237caa8\nhttps://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\n","source":"_posts/Spark-Shuffle基础.md","raw":"---\ntitle: Spark Shuffle基础\ndate: 2017-02-06 15:45:36\ntags: [spark]\ncategories: [spark]\n---\n### Shuffle 基本概念\n#### 概述：\n* Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。\n* 当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。\n* 由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。\n<!--more-->\n#### Spark 的Shuffle 分为 Write，Read 两阶段\n* Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责\n* Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的\n* 所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。\n\nShuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。\n\n### Spark的Shuffle机制\n> Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。\n\nShuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。\nShuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。\n\n\n### Shuffle的基本流程\n> bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等\n\n![shuffle-write-no-consolidation.png](http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n* 首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。\n* 其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。\n* 当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。\n\n### Spark中Shuffle类型\n* Hash Shuffle：\n第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。\n* Sort Shuffle：\n每个map产生一个文件，彻底解决了扩展性问题\n\n\n本文只是对Shuffle作了初步的描述，了解基本概念\n\n\n### 问题\n今天遇到如下问题，特来了解一下。\n```\n17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)\njava.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)\n     at java.io.FileOutputStream.open(Native Method)\n     at java.io.FileOutputStream.<init>(FileOutputStream.java:212)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n     at java.lang.Thread.run(Thread.java:722)\n```\n一般造成此问题的是系统资源不够用\n参考网上的解决方案,修改启动参数：\n* 添加：--conf spark.shuffle.manager=SORT\n Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销\n* 是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。\n     \n\n\n参考：\nhttp://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\nhttp://www.jianshu.com/p/c83bb237caa8\nhttps://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\n","slug":"Spark-Shuffle基础","published":1,"updated":"2024-04-07T07:42:55.400Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrw000kobnsbcdc335m","content":"<h3 id=\"Shuffle-基本概念\"><a href=\"#Shuffle-基本概念\" class=\"headerlink\" title=\"Shuffle 基本概念\"></a>Shuffle 基本概念</h3><h4 id=\"概述：\"><a href=\"#概述：\" class=\"headerlink\" title=\"概述：\"></a>概述：</h4><ul>\n<li>Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。</li>\n<li>当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</li>\n<li>由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。<span id=\"more\"></span></li>\n</ul>\n<h4 id=\"Spark-的Shuffle-分为-Write，Read-两阶段\"><a href=\"#Spark-的Shuffle-分为-Write，Read-两阶段\" class=\"headerlink\" title=\"Spark 的Shuffle 分为 Write，Read 两阶段\"></a>Spark 的Shuffle 分为 Write，Read 两阶段</h4><ul>\n<li>Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责</li>\n<li>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</li>\n<li>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</li>\n</ul>\n<p>Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>\n<h3 id=\"Spark的Shuffle机制\"><a href=\"#Spark的Shuffle机制\" class=\"headerlink\" title=\"Spark的Shuffle机制\"></a>Spark的Shuffle机制</h3><blockquote>\n<p>Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>\n</blockquote>\n<p>Shuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。<br>Shuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。</p>\n<h3 id=\"Shuffle的基本流程\"><a href=\"#Shuffle的基本流程\" class=\"headerlink\" title=\"Shuffle的基本流程\"></a>Shuffle的基本流程</h3><blockquote>\n<p>bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等</p>\n</blockquote>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"shuffle-write-no-consolidation.png\"></p>\n<ul>\n<li>首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</li>\n<li>其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。</li>\n<li>当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</li>\n</ul>\n<h3 id=\"Spark中Shuffle类型\"><a href=\"#Spark中Shuffle类型\" class=\"headerlink\" title=\"Spark中Shuffle类型\"></a>Spark中Shuffle类型</h3><ul>\n<li>Hash Shuffle：<br>第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。</li>\n<li>Sort Shuffle：<br>每个map产生一个文件，彻底解决了扩展性问题</li>\n</ul>\n<p>本文只是对Shuffle作了初步的描述，了解基本概念</p>\n<h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>今天遇到如下问题，特来了解一下。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)</span><br><span class=\"line\">java.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)</span><br><span class=\"line\">     at java.io.FileOutputStream.open(Native Method)</span><br><span class=\"line\">     at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)</span><br><span class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)</span><br><span class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)</span><br><span class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)</span><br><span class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)</span><br><span class=\"line\">     at org.apache.spark.scheduler.Task.run(Task.scala:85)</span><br><span class=\"line\">     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)</span><br><span class=\"line\">     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</span><br><span class=\"line\">     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</span><br><span class=\"line\">     at java.lang.Thread.run(Thread.java:722)</span><br></pre></td></tr></table></figure>\n<p>一般造成此问题的是系统资源不够用<br>参考网上的解决方案,修改启动参数：</p>\n<ul>\n<li>添加：–conf spark.shuffle.manager&#x3D;SORT<br> Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销</li>\n<li>是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。</li>\n</ul>\n<p>参考：<br><a href=\"http://blog.jasonding.top/2015/07/14/Spark/%E3%80%90Spark%E3%80%91Spark%E7%9A%84Shuffle%E6%9C%BA%E5%88%B6/\">http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/</a><br><a href=\"http://www.jianshu.com/p/c83bb237caa8\">http://www.jianshu.com/p/c83bb237caa8</a><br><a href=\"https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\">https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md</a></p>\n","cover":false,"excerpt":"<h3 id=\"Shuffle-基本概念\"><a href=\"#Shuffle-基本概念\" class=\"headerlink\" title=\"Shuffle 基本概念\"></a>Shuffle 基本概念</h3><h4 id=\"概述：\"><a href=\"#概述：\" class=\"headerlink\" title=\"概述：\"></a>概述：</h4><ul>\n<li>Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。</li>\n<li>当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</li>\n<li>由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。","more":"</li>\n</ul>\n<h4 id=\"Spark-的Shuffle-分为-Write，Read-两阶段\"><a href=\"#Spark-的Shuffle-分为-Write，Read-两阶段\" class=\"headerlink\" title=\"Spark 的Shuffle 分为 Write，Read 两阶段\"></a>Spark 的Shuffle 分为 Write，Read 两阶段</h4><ul>\n<li>Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责</li>\n<li>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</li>\n<li>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</li>\n</ul>\n<p>Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>\n<h3 id=\"Spark的Shuffle机制\"><a href=\"#Spark的Shuffle机制\" class=\"headerlink\" title=\"Spark的Shuffle机制\"></a>Spark的Shuffle机制</h3><blockquote>\n<p>Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>\n</blockquote>\n<p>Shuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。<br>Shuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。</p>\n<h3 id=\"Shuffle的基本流程\"><a href=\"#Shuffle的基本流程\" class=\"headerlink\" title=\"Shuffle的基本流程\"></a>Shuffle的基本流程</h3><blockquote>\n<p>bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等</p>\n</blockquote>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"shuffle-write-no-consolidation.png\"></p>\n<ul>\n<li>首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</li>\n<li>其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。</li>\n<li>当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</li>\n</ul>\n<h3 id=\"Spark中Shuffle类型\"><a href=\"#Spark中Shuffle类型\" class=\"headerlink\" title=\"Spark中Shuffle类型\"></a>Spark中Shuffle类型</h3><ul>\n<li>Hash Shuffle：<br>第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。</li>\n<li>Sort Shuffle：<br>每个map产生一个文件，彻底解决了扩展性问题</li>\n</ul>\n<p>本文只是对Shuffle作了初步的描述，了解基本概念</p>\n<h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>今天遇到如下问题，特来了解一下。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)</span><br><span class=\"line\">java.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)</span><br><span class=\"line\">     at java.io.FileOutputStream.open(Native Method)</span><br><span class=\"line\">     at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)</span><br><span class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)</span><br><span class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)</span><br><span class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)</span><br><span class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)</span><br><span class=\"line\">     at org.apache.spark.scheduler.Task.run(Task.scala:85)</span><br><span class=\"line\">     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)</span><br><span class=\"line\">     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</span><br><span class=\"line\">     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</span><br><span class=\"line\">     at java.lang.Thread.run(Thread.java:722)</span><br></pre></td></tr></table></figure>\n<p>一般造成此问题的是系统资源不够用<br>参考网上的解决方案,修改启动参数：</p>\n<ul>\n<li>添加：–conf spark.shuffle.manager&#x3D;SORT<br> Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销</li>\n<li>是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。</li>\n</ul>\n<p>参考：<br><a href=\"http://blog.jasonding.top/2015/07/14/Spark/%E3%80%90Spark%E3%80%91Spark%E7%9A%84Shuffle%E6%9C%BA%E5%88%B6/\">http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/</a><br><a href=\"http://www.jianshu.com/p/c83bb237caa8\">http://www.jianshu.com/p/c83bb237caa8</a><br><a href=\"https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\">https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md</a></p>"},{"title":"Spring-Boot 入门学习","date":"2017-01-25T06:03:40.000Z","_content":"> Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \"just run\". We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.\n> 简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置\n\n### 特征\n* Create stand-alone Spring applications\n* Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)\n* Provide opinionated 'starter' POMs to simplify your Maven configuration\n* Automatically configure Spring whenever possible\n* Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）\n* Absolutely no code generation and no requirement for XML configuration\n\n\n### Spring-Boot工程简单搭建\n> Spring-boot文档：http://docs.spring.io/spring-boot/docs/current/reference/html/\n> 具体代码详见：https://github.com/yany8060/SpringDemo\n\n#### pom.xml详情（本工程以maven方式搭建）：\n官方建议引入父工程\n```\n<properties> \n\t<java.version>1.7</java.version>\n\t<spring.boot.version>1.4.3.RELEASE</spring.boot.version>\n</properties>\n\n\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-parent</artifactId>\n\t<version>${spring.boot.version}</version>\n\t<type>pom</type>\n\t<scope>import</scope>\n</dependency>\n```\n\n在子工程中引入自己所需要的依赖（版本号有父工程管理）：\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-test</artifactId>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n\n```\n并添加maven编译插件：\n```\n<plugin>\n\t<groupId>org.apache.maven.plugins</groupId>\n\t<artifactId>maven-compiler-plugin</artifactId>\n</plugin>\n```\n更多SpringBoot Maven插件详见：http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\n\n#### 创建一个启动入口（应用类）\n我们创建一个Application类：\n```java\n@EnableAutoConfiguration\n@ComponentScan(basePackages = \"com.yany\")\n@Configuration\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class);\n    }\n\n}\n```\n`@EnableAutoConfiguration`\n开启自动配置\n这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring\n`@ComponentScan`\nSpring原注解，扫描包加载bean\n`@Configuration`\n标注一个类为配置类\n\n我们再创建一个Controller层：\n```java\n@RestController\npublic class Example {\n\n    @RequestMapping(value = \"/hello\", method = {RequestMethod.GET})\n    public String test() {\n        return \"Hello world\";\n    }\n\n}\n```\n`@RestController`\n相当于同时添加@Controller和@ResponseBody注解。\n\n这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：http://localhost:8080/SpringBoot/hello 来得到返回值\n\n#### 启动\n以main方法方式启动Application类，\n\n在浏览器中输入：http://localhost:8080/SpringBoot/hello\n得到返回：Hello world\n![](/img/work/14853239880359.jpg)\n\n\n具体输入日志如下：\n```\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v1.4.3.RELEASE)\n\n2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)\n2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default\n2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in '[com.yany]' package. Please check your configuration.\n2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)\n2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)\n2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat\n2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms\n2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [/]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\n2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/hello],methods=[GET]}\" onto public java.lang.String com.yany.controller.Example.test()\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup\n2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)\n2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)\n\n```\n\n\n","source":"_posts/Spring-Boot-入门学习.md","raw":"---\ntitle: Spring-Boot 入门学习\ndate: 2017-01-25 14:03:40\ntags: [spring-boot,spring]\ncategories: [java]\n---\n> Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \"just run\". We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.\n> 简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置\n\n### 特征\n* Create stand-alone Spring applications\n* Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)\n* Provide opinionated 'starter' POMs to simplify your Maven configuration\n* Automatically configure Spring whenever possible\n* Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）\n* Absolutely no code generation and no requirement for XML configuration\n\n\n### Spring-Boot工程简单搭建\n> Spring-boot文档：http://docs.spring.io/spring-boot/docs/current/reference/html/\n> 具体代码详见：https://github.com/yany8060/SpringDemo\n\n#### pom.xml详情（本工程以maven方式搭建）：\n官方建议引入父工程\n```\n<properties> \n\t<java.version>1.7</java.version>\n\t<spring.boot.version>1.4.3.RELEASE</spring.boot.version>\n</properties>\n\n\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-parent</artifactId>\n\t<version>${spring.boot.version}</version>\n\t<type>pom</type>\n\t<scope>import</scope>\n</dependency>\n```\n\n在子工程中引入自己所需要的依赖（版本号有父工程管理）：\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-test</artifactId>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n\n```\n并添加maven编译插件：\n```\n<plugin>\n\t<groupId>org.apache.maven.plugins</groupId>\n\t<artifactId>maven-compiler-plugin</artifactId>\n</plugin>\n```\n更多SpringBoot Maven插件详见：http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\n\n#### 创建一个启动入口（应用类）\n我们创建一个Application类：\n```java\n@EnableAutoConfiguration\n@ComponentScan(basePackages = \"com.yany\")\n@Configuration\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class);\n    }\n\n}\n```\n`@EnableAutoConfiguration`\n开启自动配置\n这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring\n`@ComponentScan`\nSpring原注解，扫描包加载bean\n`@Configuration`\n标注一个类为配置类\n\n我们再创建一个Controller层：\n```java\n@RestController\npublic class Example {\n\n    @RequestMapping(value = \"/hello\", method = {RequestMethod.GET})\n    public String test() {\n        return \"Hello world\";\n    }\n\n}\n```\n`@RestController`\n相当于同时添加@Controller和@ResponseBody注解。\n\n这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：http://localhost:8080/SpringBoot/hello 来得到返回值\n\n#### 启动\n以main方法方式启动Application类，\n\n在浏览器中输入：http://localhost:8080/SpringBoot/hello\n得到返回：Hello world\n![](/img/work/14853239880359.jpg)\n\n\n具体输入日志如下：\n```\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v1.4.3.RELEASE)\n\n2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)\n2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default\n2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in '[com.yany]' package. Please check your configuration.\n2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)\n2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)\n2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat\n2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms\n2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [/]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\n2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/hello],methods=[GET]}\" onto public java.lang.String com.yany.controller.Example.test()\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup\n2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)\n2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)\n\n```\n\n\n","slug":"Spring-Boot-入门学习","published":1,"updated":"2024-04-07T07:42:55.402Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrx000oobns97wy65vk","content":"<blockquote>\n<p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.<br>简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置</p>\n</blockquote>\n<h3 id=\"特征\"><a href=\"#特征\" class=\"headerlink\" title=\"特征\"></a>特征</h3><ul>\n<li>Create stand-alone Spring applications</li>\n<li>Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>\n<li>Provide opinionated ‘starter’ POMs to simplify your Maven configuration</li>\n<li>Automatically configure Spring whenever possible</li>\n<li>Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）</li>\n<li>Absolutely no code generation and no requirement for XML configuration</li>\n</ul>\n<h3 id=\"Spring-Boot工程简单搭建\"><a href=\"#Spring-Boot工程简单搭建\" class=\"headerlink\" title=\"Spring-Boot工程简单搭建\"></a>Spring-Boot工程简单搭建</h3><blockquote>\n<p>Spring-boot文档：<a href=\"http://docs.spring.io/spring-boot/docs/current/reference/html/\">http://docs.spring.io/spring-boot/docs/current/reference/html/</a><br>具体代码详见：<a href=\"https://github.com/yany8060/SpringDemo\">https://github.com/yany8060/SpringDemo</a></p>\n</blockquote>\n<h4 id=\"pom-xml详情（本工程以maven方式搭建）：\"><a href=\"#pom-xml详情（本工程以maven方式搭建）：\" class=\"headerlink\" title=\"pom.xml详情（本工程以maven方式搭建）：\"></a>pom.xml详情（本工程以maven方式搭建）：</h4><p>官方建议引入父工程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt; </span><br><span class=\"line\">\t&lt;java.version&gt;1.7&lt;/java.version&gt;</span><br><span class=\"line\">\t&lt;spring.boot.version&gt;1.4.3.RELEASE&lt;/spring.boot.version&gt;</span><br><span class=\"line\">&lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;</span><br><span class=\"line\">\t&lt;type&gt;pom&lt;/type&gt;</span><br><span class=\"line\">\t&lt;scope&gt;import&lt;/scope&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p>在子工程中引入自己所需要的依赖（版本号有父工程管理）：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>并添加maven编译插件：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure>\n<p>更多SpringBoot Maven插件详见：<a href=\"http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\">http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html</a></p>\n<h4 id=\"创建一个启动入口（应用类）\"><a href=\"#创建一个启动入口（应用类）\" class=\"headerlink\" title=\"创建一个启动入口（应用类）\"></a>创建一个启动入口（应用类）</h4><p>我们创建一个Application类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span></span><br><span class=\"line\"><span class=\"meta\">@ComponentScan(basePackages = &quot;com.yany&quot;)</span></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Application</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        SpringApplication.run(Application.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>@EnableAutoConfiguration</code><br>开启自动配置<br>这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring<br><code>@ComponentScan</code><br>Spring原注解，扫描包加载bean<br><code>@Configuration</code><br>标注一个类为配置类</p>\n<p>我们再创建一个Controller层：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Example</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@RequestMapping(value = &quot;/hello&quot;, method = &#123;RequestMethod.GET&#125;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">test</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello world&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>@RestController</code><br>相当于同时添加@Controller和@ResponseBody注解。</p>\n<p>这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a> 来得到返回值</p>\n<h4 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h4><p>以main方法方式启动Application类，</p>\n<p>在浏览器中输入：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a><br>得到返回：Hello world<br><img src=\"/img/work/14853239880359.jpg\"></p>\n<p>具体输入日志如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  .   ____          _            __ _ _</span><br><span class=\"line\"> /\\\\ / ___&#x27;_ __ _ _(_)_ __  __ _ \\ \\ \\ \\</span><br><span class=\"line\">( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\</span><br><span class=\"line\"> \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class=\"line\">  &#x27;  |____| .__|_| |_|_| |_\\__, | / / / /</span><br><span class=\"line\"> =========|_|==============|___/=/_/_/_/</span><br><span class=\"line\"> :: Spring Boot ::        (v1.4.3.RELEASE)</span><br><span class=\"line\"></span><br><span class=\"line\">2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)</span><br><span class=\"line\">2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default</span><br><span class=\"line\">2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</span><br><span class=\"line\">2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in &#x27;[com.yany]&#x27; package. Please check your configuration.</span><br><span class=\"line\">2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#x27;org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration&#x27; of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)</span><br><span class=\"line\">2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)</span><br><span class=\"line\">2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat</span><br><span class=\"line\">2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6</span><br><span class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</span><br><span class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms</span><br><span class=\"line\">2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: &#x27;dispatcherServlet&#x27; to [/]</span><br><span class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;characterEncodingFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;hiddenHttpMethodFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;httpPutFormContentFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;requestContextFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</span><br><span class=\"line\">2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/hello],methods=[GET]&#125;&quot; onto public java.lang.String com.yany.controller.Example.test()</span><br><span class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error]&#125;&quot; onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)</span><br><span class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error],produces=[text/html]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)</span><br><span class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup</span><br><span class=\"line\">2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)</span><br><span class=\"line\">2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n","cover":false,"excerpt":"","more":"<blockquote>\n<p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.<br>简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置</p>\n</blockquote>\n<h3 id=\"特征\"><a href=\"#特征\" class=\"headerlink\" title=\"特征\"></a>特征</h3><ul>\n<li>Create stand-alone Spring applications</li>\n<li>Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>\n<li>Provide opinionated ‘starter’ POMs to simplify your Maven configuration</li>\n<li>Automatically configure Spring whenever possible</li>\n<li>Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）</li>\n<li>Absolutely no code generation and no requirement for XML configuration</li>\n</ul>\n<h3 id=\"Spring-Boot工程简单搭建\"><a href=\"#Spring-Boot工程简单搭建\" class=\"headerlink\" title=\"Spring-Boot工程简单搭建\"></a>Spring-Boot工程简单搭建</h3><blockquote>\n<p>Spring-boot文档：<a href=\"http://docs.spring.io/spring-boot/docs/current/reference/html/\">http://docs.spring.io/spring-boot/docs/current/reference/html/</a><br>具体代码详见：<a href=\"https://github.com/yany8060/SpringDemo\">https://github.com/yany8060/SpringDemo</a></p>\n</blockquote>\n<h4 id=\"pom-xml详情（本工程以maven方式搭建）：\"><a href=\"#pom-xml详情（本工程以maven方式搭建）：\" class=\"headerlink\" title=\"pom.xml详情（本工程以maven方式搭建）：\"></a>pom.xml详情（本工程以maven方式搭建）：</h4><p>官方建议引入父工程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;properties&gt; </span><br><span class=\"line\">\t&lt;java.version&gt;1.7&lt;/java.version&gt;</span><br><span class=\"line\">\t&lt;spring.boot.version&gt;1.4.3.RELEASE&lt;/spring.boot.version&gt;</span><br><span class=\"line\">&lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;</span><br><span class=\"line\">\t&lt;type&gt;pom&lt;/type&gt;</span><br><span class=\"line\">\t&lt;scope&gt;import&lt;/scope&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p>在子工程中引入自己所需要的依赖（版本号有父工程管理）：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>并添加maven编译插件：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure>\n<p>更多SpringBoot Maven插件详见：<a href=\"http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\">http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html</a></p>\n<h4 id=\"创建一个启动入口（应用类）\"><a href=\"#创建一个启动入口（应用类）\" class=\"headerlink\" title=\"创建一个启动入口（应用类）\"></a>创建一个启动入口（应用类）</h4><p>我们创建一个Application类：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span></span><br><span class=\"line\"><span class=\"meta\">@ComponentScan(basePackages = &quot;com.yany&quot;)</span></span><br><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Application</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\">        SpringApplication.run(Application.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>@EnableAutoConfiguration</code><br>开启自动配置<br>这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring<br><code>@ComponentScan</code><br>Spring原注解，扫描包加载bean<br><code>@Configuration</code><br>标注一个类为配置类</p>\n<p>我们再创建一个Controller层：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RestController</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Example</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@RequestMapping(value = &quot;/hello&quot;, method = &#123;RequestMethod.GET&#125;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> String <span class=\"title function_\">test</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello world&quot;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><code>@RestController</code><br>相当于同时添加@Controller和@ResponseBody注解。</p>\n<p>这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a> 来得到返回值</p>\n<h4 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h4><p>以main方法方式启动Application类，</p>\n<p>在浏览器中输入：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a><br>得到返回：Hello world<br><img src=\"/img/work/14853239880359.jpg\"></p>\n<p>具体输入日志如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  .   ____          _            __ _ _</span><br><span class=\"line\"> /\\\\ / ___&#x27;_ __ _ _(_)_ __  __ _ \\ \\ \\ \\</span><br><span class=\"line\">( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\</span><br><span class=\"line\"> \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class=\"line\">  &#x27;  |____| .__|_| |_|_| |_\\__, | / / / /</span><br><span class=\"line\"> =========|_|==============|___/=/_/_/_/</span><br><span class=\"line\"> :: Spring Boot ::        (v1.4.3.RELEASE)</span><br><span class=\"line\"></span><br><span class=\"line\">2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)</span><br><span class=\"line\">2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default</span><br><span class=\"line\">2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</span><br><span class=\"line\">2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in &#x27;[com.yany]&#x27; package. Please check your configuration.</span><br><span class=\"line\">2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &#x27;org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration&#x27; of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)</span><br><span class=\"line\">2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)</span><br><span class=\"line\">2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat</span><br><span class=\"line\">2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6</span><br><span class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</span><br><span class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms</span><br><span class=\"line\">2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: &#x27;dispatcherServlet&#x27; to [/]</span><br><span class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;characterEncodingFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;hiddenHttpMethodFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;httpPutFormContentFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &#x27;requestContextFilter&#x27; to: [/*]</span><br><span class=\"line\">2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</span><br><span class=\"line\">2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/hello],methods=[GET]&#125;&quot; onto public java.lang.String com.yany.controller.Example.test()</span><br><span class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error]&#125;&quot; onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)</span><br><span class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error],produces=[text/html]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)</span><br><span class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</span><br><span class=\"line\">2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup</span><br><span class=\"line\">2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)</span><br><span class=\"line\">2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n"},{"title":"SparkStream 函数详解-Transformations","date":"2017-02-05T12:36:59.000Z","_content":"一个DStream对象可以调用多种操作，主要分为如下几类：\n* Transformations\n* Window Operations\n* Join Operations\n* Output Operations\n\n### Transformations\n#### map(func)\n> Return a new DStream by passing each element of the source DStream through a function func.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//拼接一个”_NEW”字符串\nval linesNew = lines.map(lines => lines + \"_NEW\" )\n```\nmap中操作复杂可把函数抽出来在外部定义：\n```\n    val mapResult = stream.map(new Transformations().customeMap(_))\n\n    def customeMap(word: String): String = {\n        word + \"ss\"\n    }\n```\n\n#### filter(func)\n> Return a new DStream by selecting only the records of the source DStream on which func returns true.\n\n对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。\n```\nval filterWords = words.filter(_ != \"hello\" )\n```\n在定义函数式时，返回值必须是boolean类型，\n\n#### flatMap(func)\n> Similar to map, but each input item can be mapped to 0 or more output items.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//将lines根据空格进行分割，分割成若干个单词\nval words = lines.flatMap(_.split( \" \" ))\n```\n同map若函数复杂可提出去\n\n#### union(otherStream)\n> Return a new DStream that contains the union of the elements in the source DStream and otherDStream.\n\n这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。\n```\nval wordsOne = words.map(_ + \"_one\" )\nval wordsTwo = words.map(_ + \"_two\" )\nval unionWords = wordsOne.union(wordsTwo)\n```\n输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream\n\n```\nhello_one\nyany_one\ntian_one\nhello_two\nyany_two\ntian_two\n```\n\n#### repartition(numPartitions)\n> Changes the level of parallelism in this DStream by creating more or fewer partitions.\n\nReturn a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.\n\n#### count()\n> Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n\n统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。\n```\nval wordsCount = words.count()\n```\n\n#### reduce(func)\n> Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n\n返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。\n```\nval reduceWords = words.reduce(_ + \"-\" + _)\n// 或\n\ndef customerReduce(line1: String, line2: String): String = {\n    line1 + \"|||\" + line2\n  }\n // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));\n    val reduceResult = flatMapResult.reduce((x, y) => new Transformations().customerReduce(x, y));\n```\n\n#### countByValue()\n> When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n\n某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。\n```\nval countByValueWords = words.countByValue()\n```\n\n#### reduceByKey(func, [numTasks])\n> When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n\n调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。\n\n注：reduceByKey使用时DStream需要以(K, V)的形式出现\n```\nval pairs = words.map(word => (word , 1))\nval wordCounts = pairs.reduceByKey(_ + _)\n//===========\n/**\n    * \n    * reducebykey 通过对于两两的value进行操作,可自定义\n    * @param line1\n    * @param line2\n    * @return\n    */\n  def customerReduceByKey(line1: String, line2: String): String = {\n    \"ss\"\n  }\n    val reduceByKeyResult = flatMapResult.map((_, \"ss\")).reduceByKey(new Transformations().customerReduceByKey(_, _))\n```\n\n#### join(otherStream, [numTasks])\n> When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n\n由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.join(wordsTwo)\n\n```\n结果\n```\n// 输入 hello world hello join\n\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(join,(join_one,join_two))\n(world,(world_one,world_two))\n```\n如果key相同，出现笛卡尔积现象\n\n#### cogroup(otherStream, [numTasks])\n> When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n\n由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.cogroup(wordsTwo)\n```\n\n结果：\n```\n// 输入 hello world hello cogroup\n\n(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))\n(world,(CompactBuffer(world_one),CompactBuffer(world_two)))\n(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))\n```\n\n#### transform(func)\n#### updateStateByKey(func)\n\n\n参考：\nhttp://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\n\n博客：http://yany8060.xyz/\ngithup: https://github.com/yany8060/SparkDemo","source":"_posts/SparkStream-函数详解-Transformations.md","raw":"---\ntitle: SparkStream 函数详解-Transformations\ndate: 2017-02-05 20:36:59\ntags: [sparkstream,spark]\ncategories: [spark]\n---\n一个DStream对象可以调用多种操作，主要分为如下几类：\n* Transformations\n* Window Operations\n* Join Operations\n* Output Operations\n\n### Transformations\n#### map(func)\n> Return a new DStream by passing each element of the source DStream through a function func.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//拼接一个”_NEW”字符串\nval linesNew = lines.map(lines => lines + \"_NEW\" )\n```\nmap中操作复杂可把函数抽出来在外部定义：\n```\n    val mapResult = stream.map(new Transformations().customeMap(_))\n\n    def customeMap(word: String): String = {\n        word + \"ss\"\n    }\n```\n\n#### filter(func)\n> Return a new DStream by selecting only the records of the source DStream on which func returns true.\n\n对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。\n```\nval filterWords = words.filter(_ != \"hello\" )\n```\n在定义函数式时，返回值必须是boolean类型，\n\n#### flatMap(func)\n> Similar to map, but each input item can be mapped to 0 or more output items.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//将lines根据空格进行分割，分割成若干个单词\nval words = lines.flatMap(_.split( \" \" ))\n```\n同map若函数复杂可提出去\n\n#### union(otherStream)\n> Return a new DStream that contains the union of the elements in the source DStream and otherDStream.\n\n这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。\n```\nval wordsOne = words.map(_ + \"_one\" )\nval wordsTwo = words.map(_ + \"_two\" )\nval unionWords = wordsOne.union(wordsTwo)\n```\n输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream\n\n```\nhello_one\nyany_one\ntian_one\nhello_two\nyany_two\ntian_two\n```\n\n#### repartition(numPartitions)\n> Changes the level of parallelism in this DStream by creating more or fewer partitions.\n\nReturn a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.\n\n#### count()\n> Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n\n统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。\n```\nval wordsCount = words.count()\n```\n\n#### reduce(func)\n> Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n\n返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。\n```\nval reduceWords = words.reduce(_ + \"-\" + _)\n// 或\n\ndef customerReduce(line1: String, line2: String): String = {\n    line1 + \"|||\" + line2\n  }\n // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));\n    val reduceResult = flatMapResult.reduce((x, y) => new Transformations().customerReduce(x, y));\n```\n\n#### countByValue()\n> When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n\n某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。\n```\nval countByValueWords = words.countByValue()\n```\n\n#### reduceByKey(func, [numTasks])\n> When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n\n调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。\n\n注：reduceByKey使用时DStream需要以(K, V)的形式出现\n```\nval pairs = words.map(word => (word , 1))\nval wordCounts = pairs.reduceByKey(_ + _)\n//===========\n/**\n    * \n    * reducebykey 通过对于两两的value进行操作,可自定义\n    * @param line1\n    * @param line2\n    * @return\n    */\n  def customerReduceByKey(line1: String, line2: String): String = {\n    \"ss\"\n  }\n    val reduceByKeyResult = flatMapResult.map((_, \"ss\")).reduceByKey(new Transformations().customerReduceByKey(_, _))\n```\n\n#### join(otherStream, [numTasks])\n> When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n\n由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.join(wordsTwo)\n\n```\n结果\n```\n// 输入 hello world hello join\n\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(join,(join_one,join_two))\n(world,(world_one,world_two))\n```\n如果key相同，出现笛卡尔积现象\n\n#### cogroup(otherStream, [numTasks])\n> When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n\n由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.cogroup(wordsTwo)\n```\n\n结果：\n```\n// 输入 hello world hello cogroup\n\n(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))\n(world,(CompactBuffer(world_one),CompactBuffer(world_two)))\n(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))\n```\n\n#### transform(func)\n#### updateStateByKey(func)\n\n\n参考：\nhttp://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\n\n博客：http://yany8060.xyz/\ngithup: https://github.com/yany8060/SparkDemo","slug":"SparkStream-函数详解-Transformations","published":1,"updated":"2024-04-07T07:42:55.401Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lry000pobns0nl3ewpm","content":"<p>一个DStream对象可以调用多种操作，主要分为如下几类：</p>\n<ul>\n<li>Transformations</li>\n<li>Window Operations</li>\n<li>Join Operations</li>\n<li>Output Operations</li>\n</ul>\n<h3 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h3><h4 id=\"map-func\"><a href=\"#map-func\" class=\"headerlink\" title=\"map(func)\"></a>map(func)</h4><blockquote>\n<p>Return a new DStream by passing each element of the source DStream through a function func.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//拼接一个”_NEW”字符串</span><br><span class=\"line\">val linesNew = lines.map(lines =&gt; lines + &quot;_NEW&quot; )</span><br></pre></td></tr></table></figure>\n<p>map中操作复杂可把函数抽出来在外部定义：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val mapResult = stream.map(new Transformations().customeMap(_))</span><br><span class=\"line\"></span><br><span class=\"line\">def customeMap(word: String): String = &#123;</span><br><span class=\"line\">    word + &quot;ss&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"filter-func\"><a href=\"#filter-func\" class=\"headerlink\" title=\"filter(func)\"></a>filter(func)</h4><blockquote>\n<p>Return a new DStream by selecting only the records of the source DStream on which func returns true.</p>\n</blockquote>\n<p>对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val filterWords = words.filter(_ != &quot;hello&quot; )</span><br></pre></td></tr></table></figure>\n<p>在定义函数式时，返回值必须是boolean类型，</p>\n<h4 id=\"flatMap-func\"><a href=\"#flatMap-func\" class=\"headerlink\" title=\"flatMap(func)\"></a>flatMap(func)</h4><blockquote>\n<p>Similar to map, but each input item can be mapped to 0 or more output items.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将lines根据空格进行分割，分割成若干个单词</span><br><span class=\"line\">val words = lines.flatMap(_.split( &quot; &quot; ))</span><br></pre></td></tr></table></figure>\n<p>同map若函数复杂可提出去</p>\n<h4 id=\"union-otherStream\"><a href=\"#union-otherStream\" class=\"headerlink\" title=\"union(otherStream)\"></a>union(otherStream)</h4><blockquote>\n<p>Return a new DStream that contains the union of the elements in the source DStream and otherDStream.</p>\n</blockquote>\n<p>这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(_ + &quot;_one&quot; )</span><br><span class=\"line\">val wordsTwo = words.map(_ + &quot;_two&quot; )</span><br><span class=\"line\">val unionWords = wordsOne.union(wordsTwo)</span><br></pre></td></tr></table></figure>\n<p>输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hello_one</span><br><span class=\"line\">yany_one</span><br><span class=\"line\">tian_one</span><br><span class=\"line\">hello_two</span><br><span class=\"line\">yany_two</span><br><span class=\"line\">tian_two</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"repartition-numPartitions\"><a href=\"#repartition-numPartitions\" class=\"headerlink\" title=\"repartition(numPartitions)\"></a>repartition(numPartitions)</h4><blockquote>\n<p>Changes the level of parallelism in this DStream by creating more or fewer partitions.</p>\n</blockquote>\n<p>Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.</p>\n<h4 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count()\"></a>count()</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</p>\n</blockquote>\n<p>统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsCount = words.count()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduce-func\"><a href=\"#reduce-func\" class=\"headerlink\" title=\"reduce(func)\"></a>reduce(func)</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</p>\n</blockquote>\n<p>返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val reduceWords = words.reduce(_ + &quot;-&quot; + _)</span><br><span class=\"line\">// 或</span><br><span class=\"line\"></span><br><span class=\"line\">def customerReduce(line1: String, line2: String): String = &#123;</span><br><span class=\"line\">    line1 + &quot;|||&quot; + line2</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));</span><br><span class=\"line\">    val reduceResult = flatMapResult.reduce((x, y) =&gt; new Transformations().customerReduce(x, y));</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue()\"></a>countByValue()</h4><blockquote>\n<p>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</p>\n</blockquote>\n<p>某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val countByValueWords = words.countByValue()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduceByKey-func-numTasks\"><a href=\"#reduceByKey-func-numTasks\" class=\"headerlink\" title=\"reduceByKey(func, [numTasks])\"></a>reduceByKey(func, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</p>\n</blockquote>\n<p>调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。</p>\n<p>注：reduceByKey使用时DStream需要以(K, V)的形式出现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val pairs = words.map(word =&gt; (word , 1))</span><br><span class=\"line\">val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class=\"line\">//===========</span><br><span class=\"line\">/**</span><br><span class=\"line\">    * </span><br><span class=\"line\">    * reducebykey 通过对于两两的value进行操作,可自定义</span><br><span class=\"line\">    * @param line1</span><br><span class=\"line\">    * @param line2</span><br><span class=\"line\">    * @return</span><br><span class=\"line\">    */</span><br><span class=\"line\">  def customerReduceByKey(line1: String, line2: String): String = &#123;</span><br><span class=\"line\">    &quot;ss&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    val reduceByKeyResult = flatMapResult.map((_, &quot;ss&quot;)).reduceByKey(new Transformations().customerReduceByKey(_, _))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"join-otherStream-numTasks\"><a href=\"#join-otherStream-numTasks\" class=\"headerlink\" title=\"join(otherStream, [numTasks])\"></a>join(otherStream, [numTasks])</h4><blockquote>\n<p>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</span><br><span class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</span><br><span class=\"line\">val joinWords = wordsOne.join(wordsTwo)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 输入 hello world hello join</span><br><span class=\"line\"></span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(join,(join_one,join_two))</span><br><span class=\"line\">(world,(world_one,world_two))</span><br></pre></td></tr></table></figure>\n<p>如果key相同，出现笛卡尔积现象</p>\n<h4 id=\"cogroup-otherStream-numTasks\"><a href=\"#cogroup-otherStream-numTasks\" class=\"headerlink\" title=\"cogroup(otherStream, [numTasks])\"></a>cogroup(otherStream, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</span><br><span class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</span><br><span class=\"line\">val joinWords = wordsOne.cogroup(wordsTwo)</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 输入 hello world hello cogroup</span><br><span class=\"line\"></span><br><span class=\"line\">(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))</span><br><span class=\"line\">(world,(CompactBuffer(world_one),CompactBuffer(world_two)))</span><br><span class=\"line\">(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"transform-func\"><a href=\"#transform-func\" class=\"headerlink\" title=\"transform(func)\"></a>transform(func)</h4><h4 id=\"updateStateByKey-func\"><a href=\"#updateStateByKey-func\" class=\"headerlink\" title=\"updateStateByKey(func)\"></a>updateStateByKey(func)</h4><p>参考：<br><a href=\"http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\">http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<p>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a><br>githup: <a href=\"https://github.com/yany8060/SparkDemo\">https://github.com/yany8060/SparkDemo</a></p>\n","cover":false,"excerpt":"","more":"<p>一个DStream对象可以调用多种操作，主要分为如下几类：</p>\n<ul>\n<li>Transformations</li>\n<li>Window Operations</li>\n<li>Join Operations</li>\n<li>Output Operations</li>\n</ul>\n<h3 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h3><h4 id=\"map-func\"><a href=\"#map-func\" class=\"headerlink\" title=\"map(func)\"></a>map(func)</h4><blockquote>\n<p>Return a new DStream by passing each element of the source DStream through a function func.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//拼接一个”_NEW”字符串</span><br><span class=\"line\">val linesNew = lines.map(lines =&gt; lines + &quot;_NEW&quot; )</span><br></pre></td></tr></table></figure>\n<p>map中操作复杂可把函数抽出来在外部定义：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val mapResult = stream.map(new Transformations().customeMap(_))</span><br><span class=\"line\"></span><br><span class=\"line\">def customeMap(word: String): String = &#123;</span><br><span class=\"line\">    word + &quot;ss&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"filter-func\"><a href=\"#filter-func\" class=\"headerlink\" title=\"filter(func)\"></a>filter(func)</h4><blockquote>\n<p>Return a new DStream by selecting only the records of the source DStream on which func returns true.</p>\n</blockquote>\n<p>对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val filterWords = words.filter(_ != &quot;hello&quot; )</span><br></pre></td></tr></table></figure>\n<p>在定义函数式时，返回值必须是boolean类型，</p>\n<h4 id=\"flatMap-func\"><a href=\"#flatMap-func\" class=\"headerlink\" title=\"flatMap(func)\"></a>flatMap(func)</h4><blockquote>\n<p>Similar to map, but each input item can be mapped to 0 or more output items.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将lines根据空格进行分割，分割成若干个单词</span><br><span class=\"line\">val words = lines.flatMap(_.split( &quot; &quot; ))</span><br></pre></td></tr></table></figure>\n<p>同map若函数复杂可提出去</p>\n<h4 id=\"union-otherStream\"><a href=\"#union-otherStream\" class=\"headerlink\" title=\"union(otherStream)\"></a>union(otherStream)</h4><blockquote>\n<p>Return a new DStream that contains the union of the elements in the source DStream and otherDStream.</p>\n</blockquote>\n<p>这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(_ + &quot;_one&quot; )</span><br><span class=\"line\">val wordsTwo = words.map(_ + &quot;_two&quot; )</span><br><span class=\"line\">val unionWords = wordsOne.union(wordsTwo)</span><br></pre></td></tr></table></figure>\n<p>输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hello_one</span><br><span class=\"line\">yany_one</span><br><span class=\"line\">tian_one</span><br><span class=\"line\">hello_two</span><br><span class=\"line\">yany_two</span><br><span class=\"line\">tian_two</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"repartition-numPartitions\"><a href=\"#repartition-numPartitions\" class=\"headerlink\" title=\"repartition(numPartitions)\"></a>repartition(numPartitions)</h4><blockquote>\n<p>Changes the level of parallelism in this DStream by creating more or fewer partitions.</p>\n</blockquote>\n<p>Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.</p>\n<h4 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count()\"></a>count()</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</p>\n</blockquote>\n<p>统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsCount = words.count()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduce-func\"><a href=\"#reduce-func\" class=\"headerlink\" title=\"reduce(func)\"></a>reduce(func)</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</p>\n</blockquote>\n<p>返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val reduceWords = words.reduce(_ + &quot;-&quot; + _)</span><br><span class=\"line\">// 或</span><br><span class=\"line\"></span><br><span class=\"line\">def customerReduce(line1: String, line2: String): String = &#123;</span><br><span class=\"line\">    line1 + &quot;|||&quot; + line2</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));</span><br><span class=\"line\">    val reduceResult = flatMapResult.reduce((x, y) =&gt; new Transformations().customerReduce(x, y));</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue()\"></a>countByValue()</h4><blockquote>\n<p>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</p>\n</blockquote>\n<p>某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val countByValueWords = words.countByValue()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduceByKey-func-numTasks\"><a href=\"#reduceByKey-func-numTasks\" class=\"headerlink\" title=\"reduceByKey(func, [numTasks])\"></a>reduceByKey(func, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</p>\n</blockquote>\n<p>调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。</p>\n<p>注：reduceByKey使用时DStream需要以(K, V)的形式出现</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val pairs = words.map(word =&gt; (word , 1))</span><br><span class=\"line\">val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class=\"line\">//===========</span><br><span class=\"line\">/**</span><br><span class=\"line\">    * </span><br><span class=\"line\">    * reducebykey 通过对于两两的value进行操作,可自定义</span><br><span class=\"line\">    * @param line1</span><br><span class=\"line\">    * @param line2</span><br><span class=\"line\">    * @return</span><br><span class=\"line\">    */</span><br><span class=\"line\">  def customerReduceByKey(line1: String, line2: String): String = &#123;</span><br><span class=\"line\">    &quot;ss&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    val reduceByKeyResult = flatMapResult.map((_, &quot;ss&quot;)).reduceByKey(new Transformations().customerReduceByKey(_, _))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"join-otherStream-numTasks\"><a href=\"#join-otherStream-numTasks\" class=\"headerlink\" title=\"join(otherStream, [numTasks])\"></a>join(otherStream, [numTasks])</h4><blockquote>\n<p>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</span><br><span class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</span><br><span class=\"line\">val joinWords = wordsOne.join(wordsTwo)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 输入 hello world hello join</span><br><span class=\"line\"></span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(hello,(hello_one,hello_two))</span><br><span class=\"line\">(join,(join_one,join_two))</span><br><span class=\"line\">(world,(world_one,world_two))</span><br></pre></td></tr></table></figure>\n<p>如果key相同，出现笛卡尔积现象</p>\n<h4 id=\"cogroup-otherStream-numTasks\"><a href=\"#cogroup-otherStream-numTasks\" class=\"headerlink\" title=\"cogroup(otherStream, [numTasks])\"></a>cogroup(otherStream, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</span><br><span class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</span><br><span class=\"line\">val joinWords = wordsOne.cogroup(wordsTwo)</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 输入 hello world hello cogroup</span><br><span class=\"line\"></span><br><span class=\"line\">(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))</span><br><span class=\"line\">(world,(CompactBuffer(world_one),CompactBuffer(world_two)))</span><br><span class=\"line\">(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"transform-func\"><a href=\"#transform-func\" class=\"headerlink\" title=\"transform(func)\"></a>transform(func)</h4><h4 id=\"updateStateByKey-func\"><a href=\"#updateStateByKey-func\" class=\"headerlink\" title=\"updateStateByKey(func)\"></a>updateStateByKey(func)</h4><p>参考：<br><a href=\"http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\">http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<p>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a><br>githup: <a href=\"https://github.com/yany8060/SparkDemo\">https://github.com/yany8060/SparkDemo</a></p>\n"},{"title":"Spring-boot MyBatis配置-1","date":"2017-02-04T03:08:22.000Z","_content":"> 本例使用mysql作为数据库，使用druid作为数据库连接池\n> 主要有单数据源和多数据源实例\n> 多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式\n\n### 项目目录结构\n![catalog.png](http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324)\n\n[comment]: <> ( ![](/img/work/catalog.png) )\n\n### MyBatis配置实现\n> springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。\n\n1. DataSource的bean，主要配置数据来源\n2. SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加\n3. MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean\n4. DataSourceTransactionManager的bean，主要用设置事务\n\n### 添加maven依赖\n```\n        <!-- aop -->        \n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.4</version>\n        </dependency>\n        <!-- dataSource start -->\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n            <version>1.2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.0.20</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.pagehelper</groupId>\n            <artifactId>pagehelper</artifactId>\n            <version>5.0.0</version>\n        </dependency>\n        <!-- dataSource end -->\n```\n\n### 单数据源\n#### 基本配置\n在application.yml中添加datasource配置：\n```\nspring:\n  application:\n    name: SpringBoot\n  datasource:\n    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&characterEncoding=UTF-8&&useSSL=false\n    username: root\n    password: *****\n    driver-class-name: com.mysql.jdbc.Driver\n    # 使用druid数据源\n    type: com.alibaba.druid.pool.DruidDataSource\n    # 初始化大小，最小，最大\n    initialSize: 5\n    minIdle: 5\n    maxActive: 20\n    # 配置获取连接等待超时的时间\n    maxWait: 60000\n    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒\n    timeBetweenEvictionRunsMillis: 60000\n    # 配置一个连接在池中最小生存的时间，单位是毫秒\n    minEvictableIdleTimeMillis: 300000\n    validationQuery: SELECT 1 FROM DUAL\n    testWhileIdle: true\n    testOnBorrow: false\n    testOnReturn: false\n    # 打开PSCache，并且指定每个连接上PSCache的大小\n    poolPreparedStatements: true\n    maxPoolPreparedStatementPerConnectionSize: 20\n    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙\n    filters: stat,wall,log4j\n    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录\n    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000\n    # 合并多个DruidDataSource的监控数据\n    useGlobalDataSourceStat: true\n```\n配置mybatis-config:\n以下只是实例，可自定义添加一些别的配置\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <!-- 全局参数 -->\n    <settings>\n        <!-- 使全局的映射器启用或禁用缓存。 -->\n        <setting name=\"cacheEnabled\" value=\"true\"/>\n        <!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 -->\n        <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n        <!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 -->\n        <setting name=\"aggressiveLazyLoading\" value=\"true\"/>\n        <!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true -->\n        <setting name=\"multipleResultSetsEnabled\" value=\"true\"/>\n        <!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true -->\n        <setting name=\"useColumnLabel\" value=\"true\"/>\n        <!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  -->\n        <setting name=\"useGeneratedKeys\" value=\"true\"/>\n        <!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  -->\n        <setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/>\n        <!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  -->\n        <setting name=\"defaultExecutorType\" value=\"SIMPLE\"/>\n        <!-- 使用驼峰命名法转换字段。 -->\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n        <!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session -->\n        <setting name=\"localCacheScope\" value=\"SESSION\"/>\n        <!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 -->\n        <setting name=\"jdbcTypeForNull\" value=\"NULL\"/>\n    </settings>\n</configuration>\n```\n#### 代码实现\n##### SingleMyBatisConfig的类实现\n继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。\n```java\n@Configuration\npublic class SingleMyBatisConfig implements EnvironmentAware{\n  private final static Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);\n    private static String MYBATIS_CONFIG = \"mybatis-config.xml\";\n    //mybatis mapper resource 路径\n    private static String MAPPER_PATH = \"classpath:/com/yany/mapper/single/**.xml\";\n    \n    private RelaxedPropertyResolver propertyResolver;\n    @Override\n    public void setEnvironment(Environment environment) {\n        this.propertyResolver = new RelaxedPropertyResolver(environment, \"spring.datasource.\");\n    }\n\n  .......\n}\n```\n添加@Bean(name = \"singleDataSource\")，设置实现DataSource的bean\n```java\n    /**\n     * @return\n     * @Primary 优先方案，被注解的实现，优先被注入\n     */\n    @Primary\n    @Bean(name = \"singleDataSource\")\n    public DataSource singleDataSource() {\n        logger.info(\"datasource url:{}\", propertyResolver.getProperty(\"url\"));\n\n        DruidDataSource datasource = new DruidDataSource();\n        datasource.setUrl(propertyResolver.getProperty(\"url\"));\n        datasource.setDriverClassName(propertyResolver.getProperty(\"driver-class-name\"));\n        datasource.setUsername(propertyResolver.getProperty(\"username\"));\n        datasource.setPassword(propertyResolver.getProperty(\"password\"));\n\n\n        datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(\"initialSize\")));\n        datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(\"minIdle\")));\n        datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(\"maxWait\")));\n        datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(\"maxActive\")));\n        datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(\"timeBetweenEvictionRunsMillis\")));\n        datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(\"minEvictableIdleTimeMillis\")));\n        datasource.setValidationQuery(propertyResolver.getProperty(\"validationQuery\"));\n        datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(\"testWhileIdle\")));\n        datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnBorrow\")));\n        datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnReturn\")));\n        datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(\"poolPreparedStatements\")));\n        datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(\"maxPoolPreparedStatementPerConnectionSize\")));\n\n        try {\n            datasource.setFilters(propertyResolver.getProperty(\"filters\"));\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return datasource;\n\n    }\n```\n添加@Bean(name = \"singleSqlSessionFactory\")，设置实现SqlSessionFactoryBean\n```java\n/**\n     * 创建sqlSessionFactory实例\n     *\n     * @return\n     */\n    @Bean(name = \"singleSqlSessionFactory\")\n    @Primary\n    public SqlSessionFactoryBean createSqlSessionFactoryBean(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws IOException {\n        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();\n        //设置mybatis configuration 扫描路径\n        sqlSessionFactoryBean.setConfigLocation(new ClassPathResource(MYBATIS_CONFIG));\n        sqlSessionFactoryBean.setDataSource(singleDataSource);\n\n        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = new PathMatchingResourcePatternResolver();\n        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n        return sqlSessionFactoryBean;\n    }\n```\n添加@Bean(name = \"singleTransactionManager\")，设置实现事务DataSourceTransactionManager\n```java\n    /**\n     * 配置事务管理器\n     */\n    @Bean(name = \"singleTransactionManager\")\n    @Primary\n    public DataSourceTransactionManager transactionManager(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws Exception {\n        return new DataSourceTransactionManager(singleDataSource);\n    }\n```\n以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。\n\n#### MapperScannerConfig\n本来主要集中实现各个数据源的MapperScannerConfigurer\n```java\n@Configuration\npublic class MapperScannerConfig {\n\n    /**\n     * 单数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createSingleMapperScannerConfigurer() {\n        System.out.println(\"singleDataSource\");\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.single\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"singleSqlSessionFactory\");\n        return mapperScannerConfigurer;\n    }\n}\n```\n\n以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：https://github.com/yany8060/SpringDemo.git\ncom.yany.dao.single中编写单属于的到接口\ncom.yany.mapper.single中编写对应的sql的xml\n\n****\n由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码\n详情请见：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz\n","source":"_posts/Spring-boot-MyBatis配置-1.md","raw":"---\ntitle: Spring-boot MyBatis配置-1\ndate: 2017-02-04 11:08:22\ntags: [spring-boot,spring]\ncategories: [java]\n---\n> 本例使用mysql作为数据库，使用druid作为数据库连接池\n> 主要有单数据源和多数据源实例\n> 多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式\n\n### 项目目录结构\n![catalog.png](http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324)\n\n[comment]: <> ( ![](/img/work/catalog.png) )\n\n### MyBatis配置实现\n> springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。\n\n1. DataSource的bean，主要配置数据来源\n2. SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加\n3. MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean\n4. DataSourceTransactionManager的bean，主要用设置事务\n\n### 添加maven依赖\n```\n        <!-- aop -->        \n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.4</version>\n        </dependency>\n        <!-- dataSource start -->\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n            <version>1.2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.0.20</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.pagehelper</groupId>\n            <artifactId>pagehelper</artifactId>\n            <version>5.0.0</version>\n        </dependency>\n        <!-- dataSource end -->\n```\n\n### 单数据源\n#### 基本配置\n在application.yml中添加datasource配置：\n```\nspring:\n  application:\n    name: SpringBoot\n  datasource:\n    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&characterEncoding=UTF-8&&useSSL=false\n    username: root\n    password: *****\n    driver-class-name: com.mysql.jdbc.Driver\n    # 使用druid数据源\n    type: com.alibaba.druid.pool.DruidDataSource\n    # 初始化大小，最小，最大\n    initialSize: 5\n    minIdle: 5\n    maxActive: 20\n    # 配置获取连接等待超时的时间\n    maxWait: 60000\n    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒\n    timeBetweenEvictionRunsMillis: 60000\n    # 配置一个连接在池中最小生存的时间，单位是毫秒\n    minEvictableIdleTimeMillis: 300000\n    validationQuery: SELECT 1 FROM DUAL\n    testWhileIdle: true\n    testOnBorrow: false\n    testOnReturn: false\n    # 打开PSCache，并且指定每个连接上PSCache的大小\n    poolPreparedStatements: true\n    maxPoolPreparedStatementPerConnectionSize: 20\n    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙\n    filters: stat,wall,log4j\n    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录\n    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000\n    # 合并多个DruidDataSource的监控数据\n    useGlobalDataSourceStat: true\n```\n配置mybatis-config:\n以下只是实例，可自定义添加一些别的配置\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <!-- 全局参数 -->\n    <settings>\n        <!-- 使全局的映射器启用或禁用缓存。 -->\n        <setting name=\"cacheEnabled\" value=\"true\"/>\n        <!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 -->\n        <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n        <!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 -->\n        <setting name=\"aggressiveLazyLoading\" value=\"true\"/>\n        <!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true -->\n        <setting name=\"multipleResultSetsEnabled\" value=\"true\"/>\n        <!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true -->\n        <setting name=\"useColumnLabel\" value=\"true\"/>\n        <!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  -->\n        <setting name=\"useGeneratedKeys\" value=\"true\"/>\n        <!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  -->\n        <setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/>\n        <!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  -->\n        <setting name=\"defaultExecutorType\" value=\"SIMPLE\"/>\n        <!-- 使用驼峰命名法转换字段。 -->\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n        <!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session -->\n        <setting name=\"localCacheScope\" value=\"SESSION\"/>\n        <!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 -->\n        <setting name=\"jdbcTypeForNull\" value=\"NULL\"/>\n    </settings>\n</configuration>\n```\n#### 代码实现\n##### SingleMyBatisConfig的类实现\n继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。\n```java\n@Configuration\npublic class SingleMyBatisConfig implements EnvironmentAware{\n  private final static Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);\n    private static String MYBATIS_CONFIG = \"mybatis-config.xml\";\n    //mybatis mapper resource 路径\n    private static String MAPPER_PATH = \"classpath:/com/yany/mapper/single/**.xml\";\n    \n    private RelaxedPropertyResolver propertyResolver;\n    @Override\n    public void setEnvironment(Environment environment) {\n        this.propertyResolver = new RelaxedPropertyResolver(environment, \"spring.datasource.\");\n    }\n\n  .......\n}\n```\n添加@Bean(name = \"singleDataSource\")，设置实现DataSource的bean\n```java\n    /**\n     * @return\n     * @Primary 优先方案，被注解的实现，优先被注入\n     */\n    @Primary\n    @Bean(name = \"singleDataSource\")\n    public DataSource singleDataSource() {\n        logger.info(\"datasource url:{}\", propertyResolver.getProperty(\"url\"));\n\n        DruidDataSource datasource = new DruidDataSource();\n        datasource.setUrl(propertyResolver.getProperty(\"url\"));\n        datasource.setDriverClassName(propertyResolver.getProperty(\"driver-class-name\"));\n        datasource.setUsername(propertyResolver.getProperty(\"username\"));\n        datasource.setPassword(propertyResolver.getProperty(\"password\"));\n\n\n        datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(\"initialSize\")));\n        datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(\"minIdle\")));\n        datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(\"maxWait\")));\n        datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(\"maxActive\")));\n        datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(\"timeBetweenEvictionRunsMillis\")));\n        datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(\"minEvictableIdleTimeMillis\")));\n        datasource.setValidationQuery(propertyResolver.getProperty(\"validationQuery\"));\n        datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(\"testWhileIdle\")));\n        datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnBorrow\")));\n        datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnReturn\")));\n        datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(\"poolPreparedStatements\")));\n        datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(\"maxPoolPreparedStatementPerConnectionSize\")));\n\n        try {\n            datasource.setFilters(propertyResolver.getProperty(\"filters\"));\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return datasource;\n\n    }\n```\n添加@Bean(name = \"singleSqlSessionFactory\")，设置实现SqlSessionFactoryBean\n```java\n/**\n     * 创建sqlSessionFactory实例\n     *\n     * @return\n     */\n    @Bean(name = \"singleSqlSessionFactory\")\n    @Primary\n    public SqlSessionFactoryBean createSqlSessionFactoryBean(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws IOException {\n        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();\n        //设置mybatis configuration 扫描路径\n        sqlSessionFactoryBean.setConfigLocation(new ClassPathResource(MYBATIS_CONFIG));\n        sqlSessionFactoryBean.setDataSource(singleDataSource);\n\n        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = new PathMatchingResourcePatternResolver();\n        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n        return sqlSessionFactoryBean;\n    }\n```\n添加@Bean(name = \"singleTransactionManager\")，设置实现事务DataSourceTransactionManager\n```java\n    /**\n     * 配置事务管理器\n     */\n    @Bean(name = \"singleTransactionManager\")\n    @Primary\n    public DataSourceTransactionManager transactionManager(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws Exception {\n        return new DataSourceTransactionManager(singleDataSource);\n    }\n```\n以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。\n\n#### MapperScannerConfig\n本来主要集中实现各个数据源的MapperScannerConfigurer\n```java\n@Configuration\npublic class MapperScannerConfig {\n\n    /**\n     * 单数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createSingleMapperScannerConfigurer() {\n        System.out.println(\"singleDataSource\");\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.single\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"singleSqlSessionFactory\");\n        return mapperScannerConfigurer;\n    }\n}\n```\n\n以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：https://github.com/yany8060/SpringDemo.git\ncom.yany.dao.single中编写单属于的到接口\ncom.yany.mapper.single中编写对应的sql的xml\n\n****\n由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码\n详情请见：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz\n","slug":"Spring-boot-MyBatis配置-1","published":1,"updated":"2024-04-07T07:42:55.402Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lrz000tobns1g99e2jm","content":"<blockquote>\n<p>本例使用mysql作为数据库，使用druid作为数据库连接池<br>主要有单数据源和多数据源实例<br>多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式</p>\n</blockquote>\n<h3 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324\" alt=\"catalog.png\"></p>\n<p>[comment]: &lt;&gt; ( <img src=\"/img/work/catalog.png\"> )</p>\n<h3 id=\"MyBatis配置实现\"><a href=\"#MyBatis配置实现\" class=\"headerlink\" title=\"MyBatis配置实现\"></a>MyBatis配置实现</h3><blockquote>\n<p>springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。</p>\n</blockquote>\n<ol>\n<li>DataSource的bean，主要配置数据来源</li>\n<li>SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加</li>\n<li>MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean</li>\n<li>DataSourceTransactionManager的bean，主要用设置事务</li>\n</ol>\n<h3 id=\"添加maven依赖\"><a href=\"#添加maven依赖\" class=\"headerlink\" title=\"添加maven依赖\"></a>添加maven依赖</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- aop --&gt;        </span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.8.4&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- dataSource start --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.2.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;5.1.38&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;druid&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.0.20&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;5.0.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- dataSource end --&gt;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"单数据源\"><a href=\"#单数据源\" class=\"headerlink\" title=\"单数据源\"></a>单数据源</h3><h4 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h4><p>在application.yml中添加datasource配置：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spring:</span><br><span class=\"line\">  application:</span><br><span class=\"line\">    name: SpringBoot</span><br><span class=\"line\">  datasource:</span><br><span class=\"line\">    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=false</span><br><span class=\"line\">    username: root</span><br><span class=\"line\">    password: *****</span><br><span class=\"line\">    driver-class-name: com.mysql.jdbc.Driver</span><br><span class=\"line\">    # 使用druid数据源</span><br><span class=\"line\">    type: com.alibaba.druid.pool.DruidDataSource</span><br><span class=\"line\">    # 初始化大小，最小，最大</span><br><span class=\"line\">    initialSize: 5</span><br><span class=\"line\">    minIdle: 5</span><br><span class=\"line\">    maxActive: 20</span><br><span class=\"line\">    # 配置获取连接等待超时的时间</span><br><span class=\"line\">    maxWait: 60000</span><br><span class=\"line\">    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒</span><br><span class=\"line\">    timeBetweenEvictionRunsMillis: 60000</span><br><span class=\"line\">    # 配置一个连接在池中最小生存的时间，单位是毫秒</span><br><span class=\"line\">    minEvictableIdleTimeMillis: 300000</span><br><span class=\"line\">    validationQuery: SELECT 1 FROM DUAL</span><br><span class=\"line\">    testWhileIdle: true</span><br><span class=\"line\">    testOnBorrow: false</span><br><span class=\"line\">    testOnReturn: false</span><br><span class=\"line\">    # 打开PSCache，并且指定每个连接上PSCache的大小</span><br><span class=\"line\">    poolPreparedStatements: true</span><br><span class=\"line\">    maxPoolPreparedStatementPerConnectionSize: 20</span><br><span class=\"line\">    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&#x27;wall&#x27;用于防火墙</span><br><span class=\"line\">    filters: stat,wall,log4j</span><br><span class=\"line\">    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录</span><br><span class=\"line\">    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000</span><br><span class=\"line\">    # 合并多个DruidDataSource的监控数据</span><br><span class=\"line\">    useGlobalDataSourceStat: true</span><br></pre></td></tr></table></figure>\n<p>配置mybatis-config:<br>以下只是实例，可自定义添加一些别的配置</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;</span><br><span class=\"line\">        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 全局参数 --&gt;</span><br><span class=\"line\">    &lt;settings&gt;</span><br><span class=\"line\">        &lt;!-- 使全局的映射器启用或禁用缓存。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 使用驼峰命名法转换字段。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;NULL&quot;/&gt;</span><br><span class=\"line\">    &lt;/settings&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><h5 id=\"SingleMyBatisConfig的类实现\"><a href=\"#SingleMyBatisConfig的类实现\" class=\"headerlink\" title=\"SingleMyBatisConfig的类实现\"></a>SingleMyBatisConfig的类实现</h5><p>继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SingleMyBatisConfig</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">EnvironmentAware</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> <span class=\"type\">Logger</span> <span class=\"variable\">logger</span> <span class=\"operator\">=</span> LoggerFactory.getLogger(SingleMyBatisConfig.class);</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"type\">String</span> <span class=\"variable\">MYBATIS_CONFIG</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;mybatis-config.xml&quot;</span>;</span><br><span class=\"line\">    <span class=\"comment\">//mybatis mapper resource 路径</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"type\">String</span> <span class=\"variable\">MAPPER_PATH</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;classpath:/com/yany/mapper/single/**.xml&quot;</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">private</span> RelaxedPropertyResolver propertyResolver;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setEnvironment</span><span class=\"params\">(Environment environment)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.propertyResolver = <span class=\"keyword\">new</span> <span class=\"title class_\">RelaxedPropertyResolver</span>(environment, <span class=\"string\">&quot;spring.datasource.&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  .......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleDataSource”)，设置实现DataSource的bean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@Primary</span> 优先方案，被注解的实现，优先被注入</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Primary</span></span><br><span class=\"line\"><span class=\"meta\">@Bean(name = &quot;singleDataSource&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataSource <span class=\"title function_\">singleDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;datasource url:&#123;&#125;&quot;</span>, propertyResolver.getProperty(<span class=\"string\">&quot;url&quot;</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">DruidDataSource</span> <span class=\"variable\">datasource</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DruidDataSource</span>();</span><br><span class=\"line\">    datasource.setUrl(propertyResolver.getProperty(<span class=\"string\">&quot;url&quot;</span>));</span><br><span class=\"line\">    datasource.setDriverClassName(propertyResolver.getProperty(<span class=\"string\">&quot;driver-class-name&quot;</span>));</span><br><span class=\"line\">    datasource.setUsername(propertyResolver.getProperty(<span class=\"string\">&quot;username&quot;</span>));</span><br><span class=\"line\">    datasource.setPassword(propertyResolver.getProperty(<span class=\"string\">&quot;password&quot;</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;initialSize&quot;</span>)));</span><br><span class=\"line\">    datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;minIdle&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxWait&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxActive&quot;</span>)));</span><br><span class=\"line\">    datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;timeBetweenEvictionRunsMillis&quot;</span>)));</span><br><span class=\"line\">    datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;minEvictableIdleTimeMillis&quot;</span>)));</span><br><span class=\"line\">    datasource.setValidationQuery(propertyResolver.getProperty(<span class=\"string\">&quot;validationQuery&quot;</span>));</span><br><span class=\"line\">    datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testWhileIdle&quot;</span>)));</span><br><span class=\"line\">    datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testOnBorrow&quot;</span>)));</span><br><span class=\"line\">    datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testOnReturn&quot;</span>)));</span><br><span class=\"line\">    datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;poolPreparedStatements&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxPoolPreparedStatementPerConnectionSize&quot;</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        datasource.setFilters(propertyResolver.getProperty(<span class=\"string\">&quot;filters&quot;</span>));</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> datasource;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleSqlSessionFactory”)，设置实现SqlSessionFactoryBean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 创建sqlSessionFactory实例</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;singleSqlSessionFactory&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SqlSessionFactoryBean <span class=\"title function_\">createSqlSessionFactoryBean</span><span class=\"params\">(<span class=\"meta\">@Qualifier(&quot;singleDataSource&quot;)</span> DataSource singleDataSource)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">        <span class=\"type\">SqlSessionFactoryBean</span> <span class=\"variable\">sqlSessionFactoryBean</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SqlSessionFactoryBean</span>();</span><br><span class=\"line\">        <span class=\"comment\">//设置mybatis configuration 扫描路径</span></span><br><span class=\"line\">        sqlSessionFactoryBean.setConfigLocation(<span class=\"keyword\">new</span> <span class=\"title class_\">ClassPathResource</span>(MYBATIS_CONFIG));</span><br><span class=\"line\">        sqlSessionFactoryBean.setDataSource(singleDataSource);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">PathMatchingResourcePatternResolver</span> <span class=\"variable\">pathMatchingResourcePatternResolver</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">PathMatchingResourcePatternResolver</span>();</span><br><span class=\"line\">        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sqlSessionFactoryBean;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleTransactionManager”)，设置实现事务DataSourceTransactionManager</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 配置事务管理器</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean(name = &quot;singleTransactionManager&quot;)</span></span><br><span class=\"line\"><span class=\"meta\">@Primary</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataSourceTransactionManager <span class=\"title function_\">transactionManager</span><span class=\"params\">(<span class=\"meta\">@Qualifier(&quot;singleDataSource&quot;)</span> DataSource singleDataSource)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DataSourceTransactionManager</span>(singleDataSource);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。</p>\n<h4 id=\"MapperScannerConfig\"><a href=\"#MapperScannerConfig\" class=\"headerlink\" title=\"MapperScannerConfig\"></a>MapperScannerConfig</h4><p>本来主要集中实现各个数据源的MapperScannerConfigurer</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MapperScannerConfig</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 单数据源配置</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createSingleMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;singleDataSource&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">        mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.single&quot;</span>);</span><br><span class=\"line\">        mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;singleSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>com.yany.dao.single中编写单属于的到接口<br>com.yany.mapper.single中编写对应的sql的xml</p>\n<hr>\n<p>由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码<br>详情请见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz</a></p>\n","cover":false,"excerpt":"","more":"<blockquote>\n<p>本例使用mysql作为数据库，使用druid作为数据库连接池<br>主要有单数据源和多数据源实例<br>多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式</p>\n</blockquote>\n<h3 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324\" alt=\"catalog.png\"></p>\n<p>[comment]: &lt;&gt; ( <img src=\"/img/work/catalog.png\"> )</p>\n<h3 id=\"MyBatis配置实现\"><a href=\"#MyBatis配置实现\" class=\"headerlink\" title=\"MyBatis配置实现\"></a>MyBatis配置实现</h3><blockquote>\n<p>springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。</p>\n</blockquote>\n<ol>\n<li>DataSource的bean，主要配置数据来源</li>\n<li>SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加</li>\n<li>MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean</li>\n<li>DataSourceTransactionManager的bean，主要用设置事务</li>\n</ol>\n<h3 id=\"添加maven依赖\"><a href=\"#添加maven依赖\" class=\"headerlink\" title=\"添加maven依赖\"></a>添加maven依赖</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- aop --&gt;        </span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.8.4&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- dataSource start --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.2.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;5.1.38&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;druid&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.0.20&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;5.0.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- dataSource end --&gt;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"单数据源\"><a href=\"#单数据源\" class=\"headerlink\" title=\"单数据源\"></a>单数据源</h3><h4 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h4><p>在application.yml中添加datasource配置：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spring:</span><br><span class=\"line\">  application:</span><br><span class=\"line\">    name: SpringBoot</span><br><span class=\"line\">  datasource:</span><br><span class=\"line\">    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=false</span><br><span class=\"line\">    username: root</span><br><span class=\"line\">    password: *****</span><br><span class=\"line\">    driver-class-name: com.mysql.jdbc.Driver</span><br><span class=\"line\">    # 使用druid数据源</span><br><span class=\"line\">    type: com.alibaba.druid.pool.DruidDataSource</span><br><span class=\"line\">    # 初始化大小，最小，最大</span><br><span class=\"line\">    initialSize: 5</span><br><span class=\"line\">    minIdle: 5</span><br><span class=\"line\">    maxActive: 20</span><br><span class=\"line\">    # 配置获取连接等待超时的时间</span><br><span class=\"line\">    maxWait: 60000</span><br><span class=\"line\">    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒</span><br><span class=\"line\">    timeBetweenEvictionRunsMillis: 60000</span><br><span class=\"line\">    # 配置一个连接在池中最小生存的时间，单位是毫秒</span><br><span class=\"line\">    minEvictableIdleTimeMillis: 300000</span><br><span class=\"line\">    validationQuery: SELECT 1 FROM DUAL</span><br><span class=\"line\">    testWhileIdle: true</span><br><span class=\"line\">    testOnBorrow: false</span><br><span class=\"line\">    testOnReturn: false</span><br><span class=\"line\">    # 打开PSCache，并且指定每个连接上PSCache的大小</span><br><span class=\"line\">    poolPreparedStatements: true</span><br><span class=\"line\">    maxPoolPreparedStatementPerConnectionSize: 20</span><br><span class=\"line\">    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&#x27;wall&#x27;用于防火墙</span><br><span class=\"line\">    filters: stat,wall,log4j</span><br><span class=\"line\">    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录</span><br><span class=\"line\">    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000</span><br><span class=\"line\">    # 合并多个DruidDataSource的监控数据</span><br><span class=\"line\">    useGlobalDataSourceStat: true</span><br></pre></td></tr></table></figure>\n<p>配置mybatis-config:<br>以下只是实例，可自定义添加一些别的配置</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;</span><br><span class=\"line\">        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 全局参数 --&gt;</span><br><span class=\"line\">    &lt;settings&gt;</span><br><span class=\"line\">        &lt;!-- 使全局的映射器启用或禁用缓存。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 使用驼峰命名法转换字段。 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</span><br><span class=\"line\">        &lt;!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 --&gt;</span><br><span class=\"line\">        &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;NULL&quot;/&gt;</span><br><span class=\"line\">    &lt;/settings&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><h5 id=\"SingleMyBatisConfig的类实现\"><a href=\"#SingleMyBatisConfig的类实现\" class=\"headerlink\" title=\"SingleMyBatisConfig的类实现\"></a>SingleMyBatisConfig的类实现</h5><p>继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SingleMyBatisConfig</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">EnvironmentAware</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> <span class=\"type\">Logger</span> <span class=\"variable\">logger</span> <span class=\"operator\">=</span> LoggerFactory.getLogger(SingleMyBatisConfig.class);</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"type\">String</span> <span class=\"variable\">MYBATIS_CONFIG</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;mybatis-config.xml&quot;</span>;</span><br><span class=\"line\">    <span class=\"comment\">//mybatis mapper resource 路径</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"type\">String</span> <span class=\"variable\">MAPPER_PATH</span> <span class=\"operator\">=</span> <span class=\"string\">&quot;classpath:/com/yany/mapper/single/**.xml&quot;</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">private</span> RelaxedPropertyResolver propertyResolver;</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setEnvironment</span><span class=\"params\">(Environment environment)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.propertyResolver = <span class=\"keyword\">new</span> <span class=\"title class_\">RelaxedPropertyResolver</span>(environment, <span class=\"string\">&quot;spring.datasource.&quot;</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  .......</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleDataSource”)，设置实现DataSource的bean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@Primary</span> 优先方案，被注解的实现，优先被注入</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Primary</span></span><br><span class=\"line\"><span class=\"meta\">@Bean(name = &quot;singleDataSource&quot;)</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataSource <span class=\"title function_\">singleDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    logger.info(<span class=\"string\">&quot;datasource url:&#123;&#125;&quot;</span>, propertyResolver.getProperty(<span class=\"string\">&quot;url&quot;</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">DruidDataSource</span> <span class=\"variable\">datasource</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DruidDataSource</span>();</span><br><span class=\"line\">    datasource.setUrl(propertyResolver.getProperty(<span class=\"string\">&quot;url&quot;</span>));</span><br><span class=\"line\">    datasource.setDriverClassName(propertyResolver.getProperty(<span class=\"string\">&quot;driver-class-name&quot;</span>));</span><br><span class=\"line\">    datasource.setUsername(propertyResolver.getProperty(<span class=\"string\">&quot;username&quot;</span>));</span><br><span class=\"line\">    datasource.setPassword(propertyResolver.getProperty(<span class=\"string\">&quot;password&quot;</span>));</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;initialSize&quot;</span>)));</span><br><span class=\"line\">    datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;minIdle&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxWait&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxActive&quot;</span>)));</span><br><span class=\"line\">    datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;timeBetweenEvictionRunsMillis&quot;</span>)));</span><br><span class=\"line\">    datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;minEvictableIdleTimeMillis&quot;</span>)));</span><br><span class=\"line\">    datasource.setValidationQuery(propertyResolver.getProperty(<span class=\"string\">&quot;validationQuery&quot;</span>));</span><br><span class=\"line\">    datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testWhileIdle&quot;</span>)));</span><br><span class=\"line\">    datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testOnBorrow&quot;</span>)));</span><br><span class=\"line\">    datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;testOnReturn&quot;</span>)));</span><br><span class=\"line\">    datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">&quot;poolPreparedStatements&quot;</span>)));</span><br><span class=\"line\">    datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">&quot;maxPoolPreparedStatementPerConnectionSize&quot;</span>)));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        datasource.setFilters(propertyResolver.getProperty(<span class=\"string\">&quot;filters&quot;</span>));</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> datasource;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleSqlSessionFactory”)，设置实现SqlSessionFactoryBean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 创建sqlSessionFactory实例</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean(name = &quot;singleSqlSessionFactory&quot;)</span></span><br><span class=\"line\">    <span class=\"meta\">@Primary</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> SqlSessionFactoryBean <span class=\"title function_\">createSqlSessionFactoryBean</span><span class=\"params\">(<span class=\"meta\">@Qualifier(&quot;singleDataSource&quot;)</span> DataSource singleDataSource)</span> <span class=\"keyword\">throws</span> IOException &#123;</span><br><span class=\"line\">        <span class=\"type\">SqlSessionFactoryBean</span> <span class=\"variable\">sqlSessionFactoryBean</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SqlSessionFactoryBean</span>();</span><br><span class=\"line\">        <span class=\"comment\">//设置mybatis configuration 扫描路径</span></span><br><span class=\"line\">        sqlSessionFactoryBean.setConfigLocation(<span class=\"keyword\">new</span> <span class=\"title class_\">ClassPathResource</span>(MYBATIS_CONFIG));</span><br><span class=\"line\">        sqlSessionFactoryBean.setDataSource(singleDataSource);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">PathMatchingResourcePatternResolver</span> <span class=\"variable\">pathMatchingResourcePatternResolver</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">PathMatchingResourcePatternResolver</span>();</span><br><span class=\"line\">        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sqlSessionFactoryBean;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>添加@Bean(name &#x3D; “singleTransactionManager”)，设置实现事务DataSourceTransactionManager</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 配置事务管理器</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean(name = &quot;singleTransactionManager&quot;)</span></span><br><span class=\"line\"><span class=\"meta\">@Primary</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DataSourceTransactionManager <span class=\"title function_\">transactionManager</span><span class=\"params\">(<span class=\"meta\">@Qualifier(&quot;singleDataSource&quot;)</span> DataSource singleDataSource)</span> <span class=\"keyword\">throws</span> Exception &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DataSourceTransactionManager</span>(singleDataSource);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。</p>\n<h4 id=\"MapperScannerConfig\"><a href=\"#MapperScannerConfig\" class=\"headerlink\" title=\"MapperScannerConfig\"></a>MapperScannerConfig</h4><p>本来主要集中实现各个数据源的MapperScannerConfigurer</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Configuration</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">MapperScannerConfig</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 单数据源配置</span></span><br><span class=\"line\"><span class=\"comment\">     *</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Bean</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createSingleMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">&quot;singleDataSource&quot;</span>);</span><br><span class=\"line\">        <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">        mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.single&quot;</span>);</span><br><span class=\"line\">        mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;singleSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>com.yany.dao.single中编写单属于的到接口<br>com.yany.mapper.single中编写对应的sql的xml</p>\n<hr>\n<p>由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码<br>详情请见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz</a></p>\n"},{"title":"Spring-boot-MyBatis配置-2","date":"2017-02-04T08:35:04.000Z","_content":"### 多数据源配置\n1. 分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建\n2. 注解\n3. AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;\n\n\n### 分包形式\n> 不同的数据源的sql操作分布在不同的路径下\n\n两个数据源的basepackage分别为：\n* com.yany.dao.multi.ads\n* com.yany.dao.multi.rds\n\n在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径\n```java\nmapperScannerConfigurer.setBasePackage(\"xxxx\");\n```\n\n对应的xml，即MAPPER_PATH\n* classpath:/com/yany/mapper/multi/ads/**.xml\n* classpath:/com/yany/mapper/multi/rds/**.xml\n\n在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径\n```java\n sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n```\n在使用时，不同数据源的操作在分别在不同的路径创建即可。\n\n### 注解形式\n#### 准备好两个注解类，分别对应于两个数据源：\n```java\npublic @interface RdsRepository {\n}\npublic @interface AdsRepository {\n}\n```\n同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。\n#### 在创建两个MapperScannerConfigurer\n```java\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationAdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationAdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationRdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationRdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n```\n上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类\n\n#### 使用时，在不同数据源的dao接口上添加对应的注解\n\n```java\n@RdsRepository\npublic interface AnnotationRdsDao {\n    int selectCount();\n}\n@AdsRepository\npublic interface AnnotationAdsDao {\n    int selectCount();\n}\n```\n而sql对应的xml不变，对应好namespace即可\n\n### AOP形式\n#### 创建一个动态数据源\n创建数据源类型的枚举类\n```java\npublic enum DatabaseType {\n    Ads, Rds\n}\n```\n\n创建一个线程安全的DatabaseType容器\n\n```java\npublic class DatabaseContextHolder {\n    private final static ThreadLocal<DatabaseType> contextHolder = new ThreadLocal<>();\n\n    public static DatabaseType getDatabaseType() {\n        return contextHolder.get();\n    }\n\n    public static void setDatabaseType(DatabaseType type) {\n        contextHolder.set(type);\n    }\n\n}\n```\nThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。\n\n创建动态数据源DynamicDataSource继承AbstractRoutingDataSource\n```java\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n\n    @Override\n    protected Object determineCurrentLookupKey() {\n        return DatabaseContextHolder.getDatabaseType();\n    }\n}\n```\n\n#### 创建AOP对应的MyBatis配置\n  创建动态数据源的bean\n```java\n    @Bean\n    public DynamicDataSource setDataSource() {\n        Map<Object, Object> targetDataSources = new HashMap<>();\n        targetDataSources.put(DatabaseType.Ads, adsDataSource);\n        targetDataSources.put(DatabaseType.Rds, rdsDataSource);\n\n        DynamicDataSource dataSource = new DynamicDataSource();\n        dataSource.setTargetDataSources(targetDataSources);\n        dataSource.setDefaultTargetDataSource(rdsDataSource);// 默认的datasource设置为rdsDataSource\n        return dataSource;\n    }\n```\n创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码\n\n#### 创建切边\n扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。\n```java\n@Aspect\n@Component\npublic class DataSourceAspect {\n    /**\n     * 使用空方法定义切点表达式\n     */\n    @Pointcut(\"execution(* com.yany.service.**.*(..))\")\n    public void declareJointPointExpression() {\n    }\n\n    @Before(\"declareJointPointExpression()\")\n    public void setDataSourceKey(JoinPoint point) {\n        if (point.getTarget() instanceof IAdsAopService ||\n                point.getTarget() instanceof AdsAopServiceImpl) {\n            //根据连接点所属的类实例，动态切换数据源\n            System.out.println(\"IAdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);\n        } else {//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）\n            System.out.println(\"IRdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);\n        }\n\n    }\n}\n```\n上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。\n\n具体代码将github：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz","source":"_posts/Spring-boot-MyBatis配置-2.md","raw":"---\ntitle: Spring-boot-MyBatis配置-2\ndate: 2017-02-04 16:35:04\ntags: [spring-boot,spring]\ncategories: [java]\n---\n### 多数据源配置\n1. 分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建\n2. 注解\n3. AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;\n\n\n### 分包形式\n> 不同的数据源的sql操作分布在不同的路径下\n\n两个数据源的basepackage分别为：\n* com.yany.dao.multi.ads\n* com.yany.dao.multi.rds\n\n在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径\n```java\nmapperScannerConfigurer.setBasePackage(\"xxxx\");\n```\n\n对应的xml，即MAPPER_PATH\n* classpath:/com/yany/mapper/multi/ads/**.xml\n* classpath:/com/yany/mapper/multi/rds/**.xml\n\n在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径\n```java\n sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n```\n在使用时，不同数据源的操作在分别在不同的路径创建即可。\n\n### 注解形式\n#### 准备好两个注解类，分别对应于两个数据源：\n```java\npublic @interface RdsRepository {\n}\npublic @interface AdsRepository {\n}\n```\n同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。\n#### 在创建两个MapperScannerConfigurer\n```java\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationAdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationAdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationRdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationRdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n```\n上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类\n\n#### 使用时，在不同数据源的dao接口上添加对应的注解\n\n```java\n@RdsRepository\npublic interface AnnotationRdsDao {\n    int selectCount();\n}\n@AdsRepository\npublic interface AnnotationAdsDao {\n    int selectCount();\n}\n```\n而sql对应的xml不变，对应好namespace即可\n\n### AOP形式\n#### 创建一个动态数据源\n创建数据源类型的枚举类\n```java\npublic enum DatabaseType {\n    Ads, Rds\n}\n```\n\n创建一个线程安全的DatabaseType容器\n\n```java\npublic class DatabaseContextHolder {\n    private final static ThreadLocal<DatabaseType> contextHolder = new ThreadLocal<>();\n\n    public static DatabaseType getDatabaseType() {\n        return contextHolder.get();\n    }\n\n    public static void setDatabaseType(DatabaseType type) {\n        contextHolder.set(type);\n    }\n\n}\n```\nThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。\n\n创建动态数据源DynamicDataSource继承AbstractRoutingDataSource\n```java\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n\n    @Override\n    protected Object determineCurrentLookupKey() {\n        return DatabaseContextHolder.getDatabaseType();\n    }\n}\n```\n\n#### 创建AOP对应的MyBatis配置\n  创建动态数据源的bean\n```java\n    @Bean\n    public DynamicDataSource setDataSource() {\n        Map<Object, Object> targetDataSources = new HashMap<>();\n        targetDataSources.put(DatabaseType.Ads, adsDataSource);\n        targetDataSources.put(DatabaseType.Rds, rdsDataSource);\n\n        DynamicDataSource dataSource = new DynamicDataSource();\n        dataSource.setTargetDataSources(targetDataSources);\n        dataSource.setDefaultTargetDataSource(rdsDataSource);// 默认的datasource设置为rdsDataSource\n        return dataSource;\n    }\n```\n创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码\n\n#### 创建切边\n扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。\n```java\n@Aspect\n@Component\npublic class DataSourceAspect {\n    /**\n     * 使用空方法定义切点表达式\n     */\n    @Pointcut(\"execution(* com.yany.service.**.*(..))\")\n    public void declareJointPointExpression() {\n    }\n\n    @Before(\"declareJointPointExpression()\")\n    public void setDataSourceKey(JoinPoint point) {\n        if (point.getTarget() instanceof IAdsAopService ||\n                point.getTarget() instanceof AdsAopServiceImpl) {\n            //根据连接点所属的类实例，动态切换数据源\n            System.out.println(\"IAdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);\n        } else {//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）\n            System.out.println(\"IRdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);\n        }\n\n    }\n}\n```\n上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。\n\n具体代码将github：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz","slug":"Spring-boot-MyBatis配置-2","published":1,"updated":"2024-04-07T07:42:55.403Z","comments":1,"layout":"post","photos":[],"_id":"clupb7ls0000uobns67q65wzh","content":"<h3 id=\"多数据源配置\"><a href=\"#多数据源配置\" class=\"headerlink\" title=\"多数据源配置\"></a>多数据源配置</h3><ol>\n<li>分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建</li>\n<li>注解</li>\n<li>AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;</li>\n</ol>\n<h3 id=\"分包形式\"><a href=\"#分包形式\" class=\"headerlink\" title=\"分包形式\"></a>分包形式</h3><blockquote>\n<p>不同的数据源的sql操作分布在不同的路径下</p>\n</blockquote>\n<p>两个数据源的basepackage分别为：</p>\n<ul>\n<li>com.yany.dao.multi.ads</li>\n<li>com.yany.dao.multi.rds</li>\n</ul>\n<p>在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;xxxx&quot;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>对应的xml，即MAPPER_PATH</p>\n<ul>\n<li>classpath:&#x2F;com&#x2F;yany&#x2F;mapper&#x2F;multi&#x2F;ads&#x2F;**.xml</li>\n<li>classpath:&#x2F;com&#x2F;yany&#x2F;mapper&#x2F;multi&#x2F;rds&#x2F;**.xml</li>\n</ul>\n<p>在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</span><br></pre></td></tr></table></figure>\n<p>在使用时，不同数据源的操作在分别在不同的路径创建即可。</p>\n<h3 id=\"注解形式\"><a href=\"#注解形式\" class=\"headerlink\" title=\"注解形式\"></a>注解形式</h3><h4 id=\"准备好两个注解类，分别对应于两个数据源：\"><a href=\"#准备好两个注解类，分别对应于两个数据源：\" class=\"headerlink\" title=\"准备好两个注解类，分别对应于两个数据源：\"></a>准备好两个注解类，分别对应于两个数据源：</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> RdsRepository &#123;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> AdsRepository &#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。</p>\n<h4 id=\"在创建两个MapperScannerConfigurer\"><a href=\"#在创建两个MapperScannerConfigurer\" class=\"headerlink\" title=\"在创建两个MapperScannerConfigurer\"></a>在创建两个MapperScannerConfigurer</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 以注解的方式 进行多数据源配置</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createAnnotatationAdsMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.multi.annotation&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;annotationAdsSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 以注解的方式 进行多数据源配置</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createAnnotatationRdsMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.multi.annotation&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;annotationRdsSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类</p>\n<h4 id=\"使用时，在不同数据源的dao接口上添加对应的注解\"><a href=\"#使用时，在不同数据源的dao接口上添加对应的注解\" class=\"headerlink\" title=\"使用时，在不同数据源的dao接口上添加对应的注解\"></a>使用时，在不同数据源的dao接口上添加对应的注解</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RdsRepository</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">AnnotationRdsDao</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">selectCount</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"meta\">@AdsRepository</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">AnnotationAdsDao</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">selectCount</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>而sql对应的xml不变，对应好namespace即可</p>\n<h3 id=\"AOP形式\"><a href=\"#AOP形式\" class=\"headerlink\" title=\"AOP形式\"></a>AOP形式</h3><h4 id=\"创建一个动态数据源\"><a href=\"#创建一个动态数据源\" class=\"headerlink\" title=\"创建一个动态数据源\"></a>创建一个动态数据源</h4><p>创建数据源类型的枚举类</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">enum</span> <span class=\"title class_\">DatabaseType</span> &#123;</span><br><span class=\"line\">    Ads, Rds</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建一个线程安全的DatabaseType容器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DatabaseContextHolder</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> ThreadLocal&lt;DatabaseType&gt; contextHolder = <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadLocal</span>&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> DatabaseType <span class=\"title function_\">getDatabaseType</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> contextHolder.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDatabaseType</span><span class=\"params\">(DatabaseType type)</span> &#123;</span><br><span class=\"line\">        contextHolder.set(type);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>ThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。</p>\n<p>创建动态数据源DynamicDataSource继承AbstractRoutingDataSource</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DynamicDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">AbstractRoutingDataSource</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Object <span class=\"title function_\">determineCurrentLookupKey</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> DatabaseContextHolder.getDatabaseType();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建AOP对应的MyBatis配置\"><a href=\"#创建AOP对应的MyBatis配置\" class=\"headerlink\" title=\"创建AOP对应的MyBatis配置\"></a>创建AOP对应的MyBatis配置</h4><p>  创建动态数据源的bean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DynamicDataSource <span class=\"title function_\">setDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    Map&lt;Object, Object&gt; targetDataSources = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">    targetDataSources.put(DatabaseType.Ads, adsDataSource);</span><br><span class=\"line\">    targetDataSources.put(DatabaseType.Rds, rdsDataSource);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">DynamicDataSource</span> <span class=\"variable\">dataSource</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DynamicDataSource</span>();</span><br><span class=\"line\">    dataSource.setTargetDataSources(targetDataSources);</span><br><span class=\"line\">    dataSource.setDefaultTargetDataSource(rdsDataSource);<span class=\"comment\">// 默认的datasource设置为rdsDataSource</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dataSource;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码</p>\n<h4 id=\"创建切边\"><a href=\"#创建切边\" class=\"headerlink\" title=\"创建切边\"></a>创建切边</h4><p>扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Aspect</span></span><br><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DataSourceAspect</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 使用空方法定义切点表达式</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Pointcut(&quot;execution(* com.yany.service.**.*(..))&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">declareJointPointExpression</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Before(&quot;declareJointPointExpression()&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDataSourceKey</span><span class=\"params\">(JoinPoint point)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (point.getTarget() <span class=\"keyword\">instanceof</span> IAdsAopService ||</span><br><span class=\"line\">                point.getTarget() <span class=\"keyword\">instanceof</span> AdsAopServiceImpl) &#123;</span><br><span class=\"line\">            <span class=\"comment\">//根据连接点所属的类实例，动态切换数据源</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;IAdsAopService Aspect&quot;</span>);</span><br><span class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;IRdsAopService Aspect&quot;</span>);</span><br><span class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。</p>\n<p>具体代码将github：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz</a></p>\n","cover":false,"excerpt":"","more":"<h3 id=\"多数据源配置\"><a href=\"#多数据源配置\" class=\"headerlink\" title=\"多数据源配置\"></a>多数据源配置</h3><ol>\n<li>分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建</li>\n<li>注解</li>\n<li>AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;</li>\n</ol>\n<h3 id=\"分包形式\"><a href=\"#分包形式\" class=\"headerlink\" title=\"分包形式\"></a>分包形式</h3><blockquote>\n<p>不同的数据源的sql操作分布在不同的路径下</p>\n</blockquote>\n<p>两个数据源的basepackage分别为：</p>\n<ul>\n<li>com.yany.dao.multi.ads</li>\n<li>com.yany.dao.multi.rds</li>\n</ul>\n<p>在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;xxxx&quot;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>对应的xml，即MAPPER_PATH</p>\n<ul>\n<li>classpath:&#x2F;com&#x2F;yany&#x2F;mapper&#x2F;multi&#x2F;ads&#x2F;**.xml</li>\n<li>classpath:&#x2F;com&#x2F;yany&#x2F;mapper&#x2F;multi&#x2F;rds&#x2F;**.xml</li>\n</ul>\n<p>在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</span><br></pre></td></tr></table></figure>\n<p>在使用时，不同数据源的操作在分别在不同的路径创建即可。</p>\n<h3 id=\"注解形式\"><a href=\"#注解形式\" class=\"headerlink\" title=\"注解形式\"></a>注解形式</h3><h4 id=\"准备好两个注解类，分别对应于两个数据源：\"><a href=\"#准备好两个注解类，分别对应于两个数据源：\" class=\"headerlink\" title=\"准备好两个注解类，分别对应于两个数据源：\"></a>准备好两个注解类，分别对应于两个数据源：</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> RdsRepository &#123;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> AdsRepository &#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。</p>\n<h4 id=\"在创建两个MapperScannerConfigurer\"><a href=\"#在创建两个MapperScannerConfigurer\" class=\"headerlink\" title=\"在创建两个MapperScannerConfigurer\"></a>在创建两个MapperScannerConfigurer</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 以注解的方式 进行多数据源配置</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createAnnotatationAdsMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.multi.annotation&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;annotationAdsSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * 以注解的方式 进行多数据源配置</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title function_\">createAnnotatationRdsMapperScannerConfigurer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MapperScannerConfigurer</span> <span class=\"variable\">mapperScannerConfigurer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MapperScannerConfigurer</span>();</span><br><span class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">&quot;com.yany.dao.multi.annotation&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">&quot;annotationRdsSqlSessionFactory&quot;</span>);</span><br><span class=\"line\">    mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类</p>\n<h4 id=\"使用时，在不同数据源的dao接口上添加对应的注解\"><a href=\"#使用时，在不同数据源的dao接口上添加对应的注解\" class=\"headerlink\" title=\"使用时，在不同数据源的dao接口上添加对应的注解\"></a>使用时，在不同数据源的dao接口上添加对应的注解</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@RdsRepository</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">AnnotationRdsDao</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">selectCount</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"meta\">@AdsRepository</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">interface</span> <span class=\"title class_\">AnnotationAdsDao</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"title function_\">selectCount</span><span class=\"params\">()</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>而sql对应的xml不变，对应好namespace即可</p>\n<h3 id=\"AOP形式\"><a href=\"#AOP形式\" class=\"headerlink\" title=\"AOP形式\"></a>AOP形式</h3><h4 id=\"创建一个动态数据源\"><a href=\"#创建一个动态数据源\" class=\"headerlink\" title=\"创建一个动态数据源\"></a>创建一个动态数据源</h4><p>创建数据源类型的枚举类</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">enum</span> <span class=\"title class_\">DatabaseType</span> &#123;</span><br><span class=\"line\">    Ads, Rds</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建一个线程安全的DatabaseType容器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DatabaseContextHolder</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> ThreadLocal&lt;DatabaseType&gt; contextHolder = <span class=\"keyword\">new</span> <span class=\"title class_\">ThreadLocal</span>&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> DatabaseType <span class=\"title function_\">getDatabaseType</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> contextHolder.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDatabaseType</span><span class=\"params\">(DatabaseType type)</span> &#123;</span><br><span class=\"line\">        contextHolder.set(type);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>ThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。</p>\n<p>创建动态数据源DynamicDataSource继承AbstractRoutingDataSource</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DynamicDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title class_\">AbstractRoutingDataSource</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">protected</span> Object <span class=\"title function_\">determineCurrentLookupKey</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> DatabaseContextHolder.getDatabaseType();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建AOP对应的MyBatis配置\"><a href=\"#创建AOP对应的MyBatis配置\" class=\"headerlink\" title=\"创建AOP对应的MyBatis配置\"></a>创建AOP对应的MyBatis配置</h4><p>  创建动态数据源的bean</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Bean</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> DynamicDataSource <span class=\"title function_\">setDataSource</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    Map&lt;Object, Object&gt; targetDataSources = <span class=\"keyword\">new</span> <span class=\"title class_\">HashMap</span>&lt;&gt;();</span><br><span class=\"line\">    targetDataSources.put(DatabaseType.Ads, adsDataSource);</span><br><span class=\"line\">    targetDataSources.put(DatabaseType.Rds, rdsDataSource);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">DynamicDataSource</span> <span class=\"variable\">dataSource</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">DynamicDataSource</span>();</span><br><span class=\"line\">    dataSource.setTargetDataSources(targetDataSources);</span><br><span class=\"line\">    dataSource.setDefaultTargetDataSource(rdsDataSource);<span class=\"comment\">// 默认的datasource设置为rdsDataSource</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dataSource;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码</p>\n<h4 id=\"创建切边\"><a href=\"#创建切边\" class=\"headerlink\" title=\"创建切边\"></a>创建切边</h4><p>扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@Aspect</span></span><br><span class=\"line\"><span class=\"meta\">@Component</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">DataSourceAspect</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * 使用空方法定义切点表达式</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"meta\">@Pointcut(&quot;execution(* com.yany.service.**.*(..))&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">declareJointPointExpression</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Before(&quot;declareJointPointExpression()&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">setDataSourceKey</span><span class=\"params\">(JoinPoint point)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (point.getTarget() <span class=\"keyword\">instanceof</span> IAdsAopService ||</span><br><span class=\"line\">                point.getTarget() <span class=\"keyword\">instanceof</span> AdsAopServiceImpl) &#123;</span><br><span class=\"line\">            <span class=\"comment\">//根据连接点所属的类实例，动态切换数据源</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;IAdsAopService Aspect&quot;</span>);</span><br><span class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）</span></span><br><span class=\"line\">            System.out.println(<span class=\"string\">&quot;IRdsAopService Aspect&quot;</span>);</span><br><span class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。</p>\n<p>具体代码将github：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz</a></p>\n"},{"title":"TensorFlow-损失函数","date":"2018-10-16T12:11:11.000Z","_content":"\n<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [ ['$','$']],\n        displayMath: [ ['$$','$$']]\n    }\n});\n</script>\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"></script>\n\n\n\n### 损失函数定义\n损失函数（loss function）,量化了分类器输出地结果（预测值）和我们期望的结果（标签）之间的差距。  \n在机器学习中，**损失函数（loss function）**也称cost function（代价函数），是用来**计算预测值和真实值的差距**。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。\n\n损失函数分为**经验风险损失函数**和**结构风险损失函数**\n\n* 经验风险损失函数指预测结果和实际结果的差别\n* 结构风险损失函数是指经验风险损失函数加上正则项\n\n设总有N个样本的样本集为$(X,Y)=(x_i,y_i)$，那么总的损失函数为$$L = \\sum_{i=1}^n l(y_i,f(x_i)) $$\n其中 $y_i,i∈[1,N]$为样本$i$的真实值，$f(x_i),i∈[1,N]$为样本$i$的预测值， $f()$为分类或者回归函数。\n\n一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上似的损失函数值最小，即使得经验风险函数(Empirical risk)最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk），那么损失函数变为：$$L = \\sum_{i=1}^n l(y_i,f(x_i))+\\lambda R(w) $$\n\nR(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。\n\n\n\n#### 常见的损失函数\n1. 0-1损失函数（0-1 loss function）$$L(y,f(x)) = \\begin{cases}\n1, y = f(x) \\\\\n0, y \\neq  f(x) \\\\\n\\end{cases}$$ \n2. 平方损失函数（quadratic loss function） $$L(Y,f(x)) = (Y-f(x))^2$$\n3. L1正则损失函数（绝对损失函数 absolute loss function）$$L(Y,f(x)) = \\left| Y-f(x) \\right|$$\n4. L2正则损失函数（即欧拉损失函数）$$L(Y,f(x)) = \\sum_{i=1}^n(Y-f(x))^2$$\n当对L2取平均值，就变成均方误差（MSE, mean squared error） $$ MSE(y,{y}') = \\frac{1}{n}\\sum_{i=1}^n (y^i - {y}')^2 $$\n4. 对数损失函数(logarithmic loss function)或对数似然损失函数(log-likelihood loss function)$$L(Y,P(Y|X)) = -logP(Y|X)$$\n\n#### 交叉熵损失函数\n交叉熵（Cross Entropy）是Loss函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小（用来评估当前训练得到的概率分布与真实分布的差异情况），常见的Loss函数就是均方平方差（Mean Squared Error）\n\n$$  loss =\\frac{1}{n}\\sum_{i=0}^n{(y_{i} \\cdot log(y\\_predicted_{i})\n+(1-y_{i}) \\cdot log(1-y\\_predicted_{i})\n)}  $$\n\n熵：用来表示所有信息量的期望 \n$$H(X)=-\\sum_{i=1}^n p(x_i)log(p(x_i))$$\n信息量定义：$h(x) = -log(p(x))$，概率分布函数p(x)\n\n### 损失函数Api\n#### tf.nn.softmax\\_cross\\_entropy\\_with\\_logits\n\n对于每个**独立的分类任务**，这个函数是去度量概率误差。比如，在 CIFAR-10 数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车， 但绝对不可能两者都在一张图中。\n\n* 输入API的数据 logits 不能进行缩放，因为在这个API的执行中会进行 softmax 计算，如果 logits 进行了缩放，那么会影响计算正确率。\n* logits 和 labels 必须有相同的数据维度 [batch_size, num_classes]，和相同的数据类型 float32 或者 float64 。\n* 它适用于每个类别相互独立且排斥的情况，一幅图只能属于一类，而不能同时包含一条狗和一只大象.\n\n```\nsoftmax_cross_entropy_with_logits(\n    _sentinel=None,\n    labels=None,\n    logits=None,\n    dim=-1,\n    name=None\n)\n## softmax_cross_entropy_with_logits_v2 (现在)\n```\n流程：\n\n1. 首先是对网络最后一层的输出做一个softmax（公式详见激活函数）\n2. 对softmax输出的向量$[Y1,Y2,Y3,...]$和样本的时机标签做一个交叉熵\n\n```\nimport tensorflow as tf  \n\n# our NN's output  \nlogits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  \n\n# step1:do softmax  \ny=tf.nn.softmax(logits)  \n\n# true label  \ny_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])  \n\n# step2:do cross_entropy  \ncross_entropy = tf.reduce_sum(-tf.reduce_sum(y_*tf.log(y), axis=1))\n\n# do cross_entropy just one step  \ncross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n\nwith tf.Session() as sess:  \n\tprint(sess.run(cross_entropy))\n\tprint(sess.run(cross_entropy2))\n\n# 1.222818\n# 1.222818\n```\n\n\n\n\n#### tf.nn.sparse\\_softmax\\_cross\\_entropy\\_with\\_logits\n计算logits 和 labels 之间的稀疏softmax 交叉熵\n\n度量在离散分类任务中的错误率，这些类之间是相互排斥的（每个输入只能对应唯一确定的一个类）。举例来说，每个CIFAR-10 图片只能被标记为唯一的一个标签：一张图片可能是一只狗或一辆卡车，而不能两者都是。\n\n区别：**sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 直接用标签计算交叉熵，而 **softmax\\_cross\\_entropy\\_with\\_logits** 是标签的onehot向量参与计算。**softmax\\_cross\\_entropy\\_with\\_logits** 的 labels 是 **sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 的 labels 的一个独热版本。\n**tf.nn.sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 比 **tf.nn.softmax\\_cross\\_entropy\\_with\\_logits** 多了一步将labels稀疏化的操作\n\n> onehot标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0\n\n> 稀疏化：例如表示一个3分类的一个样本的标签，稀疏表示的形式为[0,0,1]（这个表示这个样本为第3个分类），而非稀疏表示就表示为2（因为从0开始算，0,1,2,就能表示三类）\n\n```\nimport tensorflow as tf\n\ninput_data = tf.Variable([[0.2, 0.1, 0.9], [0.3, 0.4, 0.6]], dtype=tf.float32)\n\noutput = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=input_data, \n\nlabels=[0, 2])\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    print(sess.run(output))\n\n# [ 1.36573195  0.93983102]\n```\n\n\n#### tf.nn.sigmoid\\_cross\\_entropy\\_with\\_logits\n函数意义：\n\n* 衡量的是分类任务中的概率误差，他也是试用于每一个类别都是相不排斥的\n* 这个函数的作用是计算经**sigmoid**函数激活之后的交叉熵。 \n* 与sigmoid搭配使用的交叉熵损失函数，输入不需要额外加一层sigmoid，**tf.nn.sigmoid\\_cross\\_entropy\\_with\\_logits**中会集成有sigmoid并进行了计算优化；它适用于分类的类别之间**不是相互排斥**的场景，即多个标签（如图片中包含狗和猫）。\n\n公式：$$targets \\* -log(sigmoid(logits)) + (1 - targets) \\* -log(1 - sigmoid(logits))$$\n\n现在我们使用 x = logits, z = targets\n\n$$\n\\begin {aligned}\nsigmod交叉熵 &=z \\* -log(sigmoid(x)) + (1 - z) \\* -log(1 - sigmoid(x)) \\\\\\\n&= z \\* -log(1 / (1 + exp(-x))) + (1 - z) \\* -log(exp(-x) / (1 + exp(-x))) \\\\\\\n&= z \\* log(1 + exp(-x)) + (1 - z) \\* (-log(exp(-x)) + log(1 + exp(-x)))\\\\\\\n&= z \\* log(1 + exp(-x)) + (1 - z) \\* (x + log(1 + exp(-x))\\\\\\\n&= (1 - z) \\* x + log(1 + exp(-x))\\\\\\\n&= x - x \\* z + log(1 + exp(-x))\n\\end {aligned}\n$$\n\n当$x<0$时，$ e^{-x} \\rightarrow \\infty$ 溢出，所以使用计算式：\n$$ -x*z + log(1+e^{x}) $$\n\n$$ \n\\begin {aligned}\n推到过程： &= x - x \\* z + log(1 + exp(-x))\\\\\\\n    &= log(exp(x)) - x \\* z + log(1 + exp(-x)) \\\\\\\n    &= - x \\* z + log(1 + exp(x)) \n\\end {aligned}\n$$\n\n\n为了确保计算稳定，避免溢出，综合x>0和x<0的情况,我们使用以下函数式 ： \n$$ max(x,0) - x\\*z + log(1+e^{−|x|})  $$\n\n\n#### tf.nn.weighted\\_cross\\_entropy\\_with\\_logits\n功能以及计算方式基本与**tf\\_nn\\_sigmoid\\_cross\\_entropy\\_with\\_logits**差不多,但是加上了权重的功能,是计算具有权重的sigmoid交叉熵函数\n$$pos_weight \\* targets \\* -log(sigmoid(logits)) + (1 - targets) \\* -log(1 - sigmoid(logits))$$\n\n\n现在我们使用 x = logits, z = targets, q = pos_weight的代数式\n\n$$\n\\begin {aligned}\n  weighted交叉熵 &= qz \\* -log(sigmoid(x)) + (1 - z) \\* -log(1 - sigmoid(x))\\\\\\\n  &= qz \\* -log(1 / (1 + exp(-x))) + (1 - z) \\* -log(exp(-x) / (1 + exp(-x)))\\\\\\\n  &= qz \\* log(1 + exp(-x)) + (1 - z) \\* (-log(exp(-x)) + log(1 + exp(-x)))\\\\\\\n  &= qz \\* log(1 + exp(-x)) + (1 - z) \\* (x + log(1 + exp(-x))\\\\\\\n  &= (1 - z) \\* x + (qz +  1 - z) \\* log(1 + exp(-x))\\\\\\\n  &= (1 - z) \\* x + (1 + (q - 1) \\* z) \\* log(1 + exp(-x))\\\\\\\n\\end {aligned}\n$$\n\n我们把$l = (1 + (q - 1) \\* z)$, 来确保稳定性并且避免溢出,公式为：\n$$(1 - z) \\* x + l \\* (log(1 + exp(-abs(x))) + max(-x, 0))$$\n\n\n### 了解\n\n**正则化**：防止过拟合，提高泛化能力；正则化的原理就是在损失函数中加入评价模型复杂度的指标。R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。\n\n**下溢出（underflow）和上溢出（overflow）**:实数在计算机内用二进制表示，所以不是一个精确值，当数值过小的时候，被四舍五入为0，这就是下溢出。此时如果对这个数再做某些运算（例如除以它）就会出问题。反之当数值过大的时候，情况就变成上溢出。","source":"_posts/TensorFlow-损失函数.md","raw":"---\ntitle: TensorFlow-损失函数\ndate: 2018-10-16 20:11:11\ntags: tensorflow\ncategories : tensorflow\n---\n\n<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [ ['$','$']],\n        displayMath: [ ['$$','$$']]\n    }\n});\n</script>\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"></script>\n\n\n\n### 损失函数定义\n损失函数（loss function）,量化了分类器输出地结果（预测值）和我们期望的结果（标签）之间的差距。  \n在机器学习中，**损失函数（loss function）**也称cost function（代价函数），是用来**计算预测值和真实值的差距**。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。\n\n损失函数分为**经验风险损失函数**和**结构风险损失函数**\n\n* 经验风险损失函数指预测结果和实际结果的差别\n* 结构风险损失函数是指经验风险损失函数加上正则项\n\n设总有N个样本的样本集为$(X,Y)=(x_i,y_i)$，那么总的损失函数为$$L = \\sum_{i=1}^n l(y_i,f(x_i)) $$\n其中 $y_i,i∈[1,N]$为样本$i$的真实值，$f(x_i),i∈[1,N]$为样本$i$的预测值， $f()$为分类或者回归函数。\n\n一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上似的损失函数值最小，即使得经验风险函数(Empirical risk)最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk），那么损失函数变为：$$L = \\sum_{i=1}^n l(y_i,f(x_i))+\\lambda R(w) $$\n\nR(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。\n\n\n\n#### 常见的损失函数\n1. 0-1损失函数（0-1 loss function）$$L(y,f(x)) = \\begin{cases}\n1, y = f(x) \\\\\n0, y \\neq  f(x) \\\\\n\\end{cases}$$ \n2. 平方损失函数（quadratic loss function） $$L(Y,f(x)) = (Y-f(x))^2$$\n3. L1正则损失函数（绝对损失函数 absolute loss function）$$L(Y,f(x)) = \\left| Y-f(x) \\right|$$\n4. L2正则损失函数（即欧拉损失函数）$$L(Y,f(x)) = \\sum_{i=1}^n(Y-f(x))^2$$\n当对L2取平均值，就变成均方误差（MSE, mean squared error） $$ MSE(y,{y}') = \\frac{1}{n}\\sum_{i=1}^n (y^i - {y}')^2 $$\n4. 对数损失函数(logarithmic loss function)或对数似然损失函数(log-likelihood loss function)$$L(Y,P(Y|X)) = -logP(Y|X)$$\n\n#### 交叉熵损失函数\n交叉熵（Cross Entropy）是Loss函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小（用来评估当前训练得到的概率分布与真实分布的差异情况），常见的Loss函数就是均方平方差（Mean Squared Error）\n\n$$  loss =\\frac{1}{n}\\sum_{i=0}^n{(y_{i} \\cdot log(y\\_predicted_{i})\n+(1-y_{i}) \\cdot log(1-y\\_predicted_{i})\n)}  $$\n\n熵：用来表示所有信息量的期望 \n$$H(X)=-\\sum_{i=1}^n p(x_i)log(p(x_i))$$\n信息量定义：$h(x) = -log(p(x))$，概率分布函数p(x)\n\n### 损失函数Api\n#### tf.nn.softmax\\_cross\\_entropy\\_with\\_logits\n\n对于每个**独立的分类任务**，这个函数是去度量概率误差。比如，在 CIFAR-10 数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车， 但绝对不可能两者都在一张图中。\n\n* 输入API的数据 logits 不能进行缩放，因为在这个API的执行中会进行 softmax 计算，如果 logits 进行了缩放，那么会影响计算正确率。\n* logits 和 labels 必须有相同的数据维度 [batch_size, num_classes]，和相同的数据类型 float32 或者 float64 。\n* 它适用于每个类别相互独立且排斥的情况，一幅图只能属于一类，而不能同时包含一条狗和一只大象.\n\n```\nsoftmax_cross_entropy_with_logits(\n    _sentinel=None,\n    labels=None,\n    logits=None,\n    dim=-1,\n    name=None\n)\n## softmax_cross_entropy_with_logits_v2 (现在)\n```\n流程：\n\n1. 首先是对网络最后一层的输出做一个softmax（公式详见激活函数）\n2. 对softmax输出的向量$[Y1,Y2,Y3,...]$和样本的时机标签做一个交叉熵\n\n```\nimport tensorflow as tf  \n\n# our NN's output  \nlogits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  \n\n# step1:do softmax  \ny=tf.nn.softmax(logits)  \n\n# true label  \ny_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])  \n\n# step2:do cross_entropy  \ncross_entropy = tf.reduce_sum(-tf.reduce_sum(y_*tf.log(y), axis=1))\n\n# do cross_entropy just one step  \ncross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n\nwith tf.Session() as sess:  \n\tprint(sess.run(cross_entropy))\n\tprint(sess.run(cross_entropy2))\n\n# 1.222818\n# 1.222818\n```\n\n\n\n\n#### tf.nn.sparse\\_softmax\\_cross\\_entropy\\_with\\_logits\n计算logits 和 labels 之间的稀疏softmax 交叉熵\n\n度量在离散分类任务中的错误率，这些类之间是相互排斥的（每个输入只能对应唯一确定的一个类）。举例来说，每个CIFAR-10 图片只能被标记为唯一的一个标签：一张图片可能是一只狗或一辆卡车，而不能两者都是。\n\n区别：**sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 直接用标签计算交叉熵，而 **softmax\\_cross\\_entropy\\_with\\_logits** 是标签的onehot向量参与计算。**softmax\\_cross\\_entropy\\_with\\_logits** 的 labels 是 **sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 的 labels 的一个独热版本。\n**tf.nn.sparse\\_softmax\\_cross\\_entropy\\_with\\_logits** 比 **tf.nn.softmax\\_cross\\_entropy\\_with\\_logits** 多了一步将labels稀疏化的操作\n\n> onehot标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0\n\n> 稀疏化：例如表示一个3分类的一个样本的标签，稀疏表示的形式为[0,0,1]（这个表示这个样本为第3个分类），而非稀疏表示就表示为2（因为从0开始算，0,1,2,就能表示三类）\n\n```\nimport tensorflow as tf\n\ninput_data = tf.Variable([[0.2, 0.1, 0.9], [0.3, 0.4, 0.6]], dtype=tf.float32)\n\noutput = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=input_data, \n\nlabels=[0, 2])\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    print(sess.run(output))\n\n# [ 1.36573195  0.93983102]\n```\n\n\n#### tf.nn.sigmoid\\_cross\\_entropy\\_with\\_logits\n函数意义：\n\n* 衡量的是分类任务中的概率误差，他也是试用于每一个类别都是相不排斥的\n* 这个函数的作用是计算经**sigmoid**函数激活之后的交叉熵。 \n* 与sigmoid搭配使用的交叉熵损失函数，输入不需要额外加一层sigmoid，**tf.nn.sigmoid\\_cross\\_entropy\\_with\\_logits**中会集成有sigmoid并进行了计算优化；它适用于分类的类别之间**不是相互排斥**的场景，即多个标签（如图片中包含狗和猫）。\n\n公式：$$targets \\* -log(sigmoid(logits)) + (1 - targets) \\* -log(1 - sigmoid(logits))$$\n\n现在我们使用 x = logits, z = targets\n\n$$\n\\begin {aligned}\nsigmod交叉熵 &=z \\* -log(sigmoid(x)) + (1 - z) \\* -log(1 - sigmoid(x)) \\\\\\\n&= z \\* -log(1 / (1 + exp(-x))) + (1 - z) \\* -log(exp(-x) / (1 + exp(-x))) \\\\\\\n&= z \\* log(1 + exp(-x)) + (1 - z) \\* (-log(exp(-x)) + log(1 + exp(-x)))\\\\\\\n&= z \\* log(1 + exp(-x)) + (1 - z) \\* (x + log(1 + exp(-x))\\\\\\\n&= (1 - z) \\* x + log(1 + exp(-x))\\\\\\\n&= x - x \\* z + log(1 + exp(-x))\n\\end {aligned}\n$$\n\n当$x<0$时，$ e^{-x} \\rightarrow \\infty$ 溢出，所以使用计算式：\n$$ -x*z + log(1+e^{x}) $$\n\n$$ \n\\begin {aligned}\n推到过程： &= x - x \\* z + log(1 + exp(-x))\\\\\\\n    &= log(exp(x)) - x \\* z + log(1 + exp(-x)) \\\\\\\n    &= - x \\* z + log(1 + exp(x)) \n\\end {aligned}\n$$\n\n\n为了确保计算稳定，避免溢出，综合x>0和x<0的情况,我们使用以下函数式 ： \n$$ max(x,0) - x\\*z + log(1+e^{−|x|})  $$\n\n\n#### tf.nn.weighted\\_cross\\_entropy\\_with\\_logits\n功能以及计算方式基本与**tf\\_nn\\_sigmoid\\_cross\\_entropy\\_with\\_logits**差不多,但是加上了权重的功能,是计算具有权重的sigmoid交叉熵函数\n$$pos_weight \\* targets \\* -log(sigmoid(logits)) + (1 - targets) \\* -log(1 - sigmoid(logits))$$\n\n\n现在我们使用 x = logits, z = targets, q = pos_weight的代数式\n\n$$\n\\begin {aligned}\n  weighted交叉熵 &= qz \\* -log(sigmoid(x)) + (1 - z) \\* -log(1 - sigmoid(x))\\\\\\\n  &= qz \\* -log(1 / (1 + exp(-x))) + (1 - z) \\* -log(exp(-x) / (1 + exp(-x)))\\\\\\\n  &= qz \\* log(1 + exp(-x)) + (1 - z) \\* (-log(exp(-x)) + log(1 + exp(-x)))\\\\\\\n  &= qz \\* log(1 + exp(-x)) + (1 - z) \\* (x + log(1 + exp(-x))\\\\\\\n  &= (1 - z) \\* x + (qz +  1 - z) \\* log(1 + exp(-x))\\\\\\\n  &= (1 - z) \\* x + (1 + (q - 1) \\* z) \\* log(1 + exp(-x))\\\\\\\n\\end {aligned}\n$$\n\n我们把$l = (1 + (q - 1) \\* z)$, 来确保稳定性并且避免溢出,公式为：\n$$(1 - z) \\* x + l \\* (log(1 + exp(-abs(x))) + max(-x, 0))$$\n\n\n### 了解\n\n**正则化**：防止过拟合，提高泛化能力；正则化的原理就是在损失函数中加入评价模型复杂度的指标。R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。\n\n**下溢出（underflow）和上溢出（overflow）**:实数在计算机内用二进制表示，所以不是一个精确值，当数值过小的时候，被四舍五入为0，这就是下溢出。此时如果对这个数再做某些运算（例如除以它）就会出问题。反之当数值过大的时候，情况就变成上溢出。","slug":"TensorFlow-损失函数","published":1,"updated":"2024-04-07T07:52:49.496Z","comments":1,"layout":"post","photos":[],"_id":"clupb7ls1000yobns6s3l1h0c","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [ ['$','$']],\n        displayMath: [ ['$$','$$']]\n    }\n});\n</script>\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"></script>\n\n\n\n<h3 id=\"损失函数定义\"><a href=\"#损失函数定义\" class=\"headerlink\" title=\"损失函数定义\"></a>损失函数定义</h3><p>损失函数（loss function）,量化了分类器输出地结果（预测值）和我们期望的结果（标签）之间的差距。<br>在机器学习中，<strong>损失函数（loss function）</strong>也称cost function（代价函数），是用来<strong>计算预测值和真实值的差距</strong>。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。</p>\n<p>损失函数分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong></p>\n<ul>\n<li>经验风险损失函数指预测结果和实际结果的差别</li>\n<li>结构风险损失函数是指经验风险损失函数加上正则项</li>\n</ul>\n<p>设总有N个样本的样本集为$(X,Y)&#x3D;(x_i,y_i)$，那么总的损失函数为$$L &#x3D; \\sum_{i&#x3D;1}^n l(y_i,f(x_i)) $$<br>其中 $y_i,i∈[1,N]$为样本$i$的真实值，$f(x_i),i∈[1,N]$为样本$i$的预测值， $f()$为分类或者回归函数。</p>\n<p>一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上似的损失函数值最小，即使得经验风险函数(Empirical risk)最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk），那么损失函数变为：$$L &#x3D; \\sum_{i&#x3D;1}^n l(y_i,f(x_i))+\\lambda R(w) $$</p>\n<p>R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。</p>\n<h4 id=\"常见的损失函数\"><a href=\"#常见的损失函数\" class=\"headerlink\" title=\"常见的损失函数\"></a>常见的损失函数</h4><ol>\n<li>0-1损失函数（0-1 loss function）$$L(y,f(x)) &#x3D; \\begin{cases}<br>1, y &#x3D; f(x) \\<br>0, y \\neq  f(x) \\<br>\\end{cases}$$ </li>\n<li>平方损失函数（quadratic loss function） $$L(Y,f(x)) &#x3D; (Y-f(x))^2$$</li>\n<li>L1正则损失函数（绝对损失函数 absolute loss function）$$L(Y,f(x)) &#x3D; \\left| Y-f(x) \\right|$$</li>\n<li>L2正则损失函数（即欧拉损失函数）$$L(Y,f(x)) &#x3D; \\sum_{i&#x3D;1}^n(Y-f(x))^2$$<br>当对L2取平均值，就变成均方误差（MSE, mean squared error） $$ MSE(y,{y}’) &#x3D; \\frac{1}{n}\\sum_{i&#x3D;1}^n (y^i - {y}’)^2 $$</li>\n<li>对数损失函数(logarithmic loss function)或对数似然损失函数(log-likelihood loss function)$$L(Y,P(Y|X)) &#x3D; -logP(Y|X)$$</li>\n</ol>\n<h4 id=\"交叉熵损失函数\"><a href=\"#交叉熵损失函数\" class=\"headerlink\" title=\"交叉熵损失函数\"></a>交叉熵损失函数</h4><p>交叉熵（Cross Entropy）是Loss函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小（用来评估当前训练得到的概率分布与真实分布的差异情况），常见的Loss函数就是均方平方差（Mean Squared Error）</p>\n<p>$$  loss &#x3D;\\frac{1}{n}\\sum_{i&#x3D;0}^n{(y_{i} \\cdot log(y_predicted_{i})<br>+(1-y_{i}) \\cdot log(1-y_predicted_{i})<br>)}  $$</p>\n<p>熵：用来表示所有信息量的期望<br>$$H(X)&#x3D;-\\sum_{i&#x3D;1}^n p(x_i)log(p(x_i))$$<br>信息量定义：$h(x) &#x3D; -log(p(x))$，概率分布函数p(x)</p>\n<h3 id=\"损失函数Api\"><a href=\"#损失函数Api\" class=\"headerlink\" title=\"损失函数Api\"></a>损失函数Api</h3><h4 id=\"tf-nn-softmax-cross-entropy-with-logits\"><a href=\"#tf-nn-softmax-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.softmax_cross_entropy_with_logits\"></a>tf.nn.softmax_cross_entropy_with_logits</h4><p>对于每个<strong>独立的分类任务</strong>，这个函数是去度量概率误差。比如，在 CIFAR-10 数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车， 但绝对不可能两者都在一张图中。</p>\n<ul>\n<li>输入API的数据 logits 不能进行缩放，因为在这个API的执行中会进行 softmax 计算，如果 logits 进行了缩放，那么会影响计算正确率。</li>\n<li>logits 和 labels 必须有相同的数据维度 [batch_size, num_classes]，和相同的数据类型 float32 或者 float64 。</li>\n<li>它适用于每个类别相互独立且排斥的情况，一幅图只能属于一类，而不能同时包含一条狗和一只大象.</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">softmax_cross_entropy_with_logits(</span><br><span class=\"line\">    _sentinel=None,</span><br><span class=\"line\">    labels=None,</span><br><span class=\"line\">    logits=None,</span><br><span class=\"line\">    dim=-1,</span><br><span class=\"line\">    name=None</span><br><span class=\"line\">)</span><br><span class=\"line\">## softmax_cross_entropy_with_logits_v2 (现在)</span><br></pre></td></tr></table></figure>\n<p>流程：</p>\n<ol>\n<li>首先是对网络最后一层的输出做一个softmax（公式详见激活函数）</li>\n<li>对softmax输出的向量$[Y1,Y2,Y3,…]$和样本的时机标签做一个交叉熵</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf  </span><br><span class=\"line\"></span><br><span class=\"line\"># our NN&#x27;s output  </span><br><span class=\"line\">logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  </span><br><span class=\"line\"></span><br><span class=\"line\"># step1:do softmax  </span><br><span class=\"line\">y=tf.nn.softmax(logits)  </span><br><span class=\"line\"></span><br><span class=\"line\"># true label  </span><br><span class=\"line\">y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])  </span><br><span class=\"line\"></span><br><span class=\"line\"># step2:do cross_entropy  </span><br><span class=\"line\">cross_entropy = tf.reduce_sum(-tf.reduce_sum(y_*tf.log(y), axis=1))</span><br><span class=\"line\"></span><br><span class=\"line\"># do cross_entropy just one step  </span><br><span class=\"line\">cross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"></span><br><span class=\"line\">with tf.Session() as sess:  </span><br><span class=\"line\">\tprint(sess.run(cross_entropy))</span><br><span class=\"line\">\tprint(sess.run(cross_entropy2))</span><br><span class=\"line\"></span><br><span class=\"line\"># 1.222818</span><br><span class=\"line\"># 1.222818</span><br></pre></td></tr></table></figure>\n\n\n\n\n<h4 id=\"tf-nn-sparse-softmax-cross-entropy-with-logits\"><a href=\"#tf-nn-sparse-softmax-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.sparse_softmax_cross_entropy_with_logits\"></a>tf.nn.sparse_softmax_cross_entropy_with_logits</h4><p>计算logits 和 labels 之间的稀疏softmax 交叉熵</p>\n<p>度量在离散分类任务中的错误率，这些类之间是相互排斥的（每个输入只能对应唯一确定的一个类）。举例来说，每个CIFAR-10 图片只能被标记为唯一的一个标签：一张图片可能是一只狗或一辆卡车，而不能两者都是。</p>\n<p>区别：<strong>sparse_softmax_cross_entropy_with_logits</strong> 直接用标签计算交叉熵，而 <strong>softmax_cross_entropy_with_logits</strong> 是标签的onehot向量参与计算。<strong>softmax_cross_entropy_with_logits</strong> 的 labels 是 <strong>sparse_softmax_cross_entropy_with_logits</strong> 的 labels 的一个独热版本。<br><strong>tf.nn.sparse_softmax_cross_entropy_with_logits</strong> 比 <strong>tf.nn.softmax_cross_entropy_with_logits</strong> 多了一步将labels稀疏化的操作</p>\n<blockquote>\n<p>onehot标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0</p>\n</blockquote>\n<blockquote>\n<p>稀疏化：例如表示一个3分类的一个样本的标签，稀疏表示的形式为[0,0,1]（这个表示这个样本为第3个分类），而非稀疏表示就表示为2（因为从0开始算，0,1,2,就能表示三类）</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">input_data = tf.Variable([[0.2, 0.1, 0.9], [0.3, 0.4, 0.6]], dtype=tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">output = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=input_data, </span><br><span class=\"line\"></span><br><span class=\"line\">labels=[0, 2])</span><br><span class=\"line\"></span><br><span class=\"line\">with tf.Session() as sess:</span><br><span class=\"line\">    init = tf.global_variables_initializer()</span><br><span class=\"line\">    sess.run(init)</span><br><span class=\"line\">    print(sess.run(output))</span><br><span class=\"line\"></span><br><span class=\"line\"># [ 1.36573195  0.93983102]</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"tf-nn-sigmoid-cross-entropy-with-logits\"><a href=\"#tf-nn-sigmoid-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.sigmoid_cross_entropy_with_logits\"></a>tf.nn.sigmoid_cross_entropy_with_logits</h4><p>函数意义：</p>\n<ul>\n<li>衡量的是分类任务中的概率误差，他也是试用于每一个类别都是相不排斥的</li>\n<li>这个函数的作用是计算经<strong>sigmoid</strong>函数激活之后的交叉熵。 </li>\n<li>与sigmoid搭配使用的交叉熵损失函数，输入不需要额外加一层sigmoid，<strong>tf.nn.sigmoid_cross_entropy_with_logits</strong>中会集成有sigmoid并进行了计算优化；它适用于分类的类别之间<strong>不是相互排斥</strong>的场景，即多个标签（如图片中包含狗和猫）。</li>\n</ul>\n<p>公式：$$targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))$$</p>\n<p>现在我们使用 x &#x3D; logits, z &#x3D; targets</p>\n<p>$$<br>\\begin {aligned}<br>sigmod交叉熵 &amp;&#x3D;z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) \\<br>&amp;&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x))) \\<br>&amp;&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\\<br>&amp;&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\\<br>&amp;&#x3D; (1 - z) * x + log(1 + exp(-x))\\<br>&amp;&#x3D; x - x * z + log(1 + exp(-x))<br>\\end {aligned}<br>$$</p>\n<p>当$x&lt;0$时，$ e^{-x} \\rightarrow \\infty$ 溢出，所以使用计算式：<br>$$ -x*z + log(1+e^{x}) $$</p>\n<p>$$<br>\\begin {aligned}<br>推到过程： &amp;&#x3D; x - x * z + log(1 + exp(-x))\\<br>    &amp;&#x3D; log(exp(x)) - x * z + log(1 + exp(-x)) \\<br>    &amp;&#x3D; - x * z + log(1 + exp(x))<br>\\end {aligned}<br>$$</p>\n<p>为了确保计算稳定，避免溢出，综合x&gt;0和x&lt;0的情况,我们使用以下函数式 ：<br>$$ max(x,0) - x*z + log(1+e^{−|x|})  $$</p>\n<h4 id=\"tf-nn-weighted-cross-entropy-with-logits\"><a href=\"#tf-nn-weighted-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.weighted_cross_entropy_with_logits\"></a>tf.nn.weighted_cross_entropy_with_logits</h4><p>功能以及计算方式基本与<strong>tf_nn_sigmoid_cross_entropy_with_logits</strong>差不多,但是加上了权重的功能,是计算具有权重的sigmoid交叉熵函数<br>$$pos_weight * targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))$$</p>\n<p>现在我们使用 x &#x3D; logits, z &#x3D; targets, q &#x3D; pos_weight的代数式</p>\n<p>$$<br>\\begin {aligned}<br>  weighted交叉熵 &amp;&#x3D; qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\\<br>  &amp;&#x3D; qz * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))\\<br>  &amp;&#x3D; qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\\<br>  &amp;&#x3D; qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\\<br>  &amp;&#x3D; (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))\\<br>  &amp;&#x3D; (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))\\<br>\\end {aligned}<br>$$</p>\n<p>我们把$l &#x3D; (1 + (q - 1) * z)$, 来确保稳定性并且避免溢出,公式为：<br>$$(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))$$</p>\n<h3 id=\"了解\"><a href=\"#了解\" class=\"headerlink\" title=\"了解\"></a>了解</h3><p><strong>正则化</strong>：防止过拟合，提高泛化能力；正则化的原理就是在损失函数中加入评价模型复杂度的指标。R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。</p>\n<p><strong>下溢出（underflow）和上溢出（overflow）</strong>:实数在计算机内用二进制表示，所以不是一个精确值，当数值过小的时候，被四舍五入为0，这就是下溢出。此时如果对这个数再做某些运算（例如除以它）就会出问题。反之当数值过大的时候，情况就变成上溢出。</p>\n","cover":false,"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [ ['$','$']],\n        displayMath: [ ['$$','$$']]\n    }\n});\n</script>\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"></script>\n\n\n\n<h3 id=\"损失函数定义\"><a href=\"#损失函数定义\" class=\"headerlink\" title=\"损失函数定义\"></a>损失函数定义</h3><p>损失函数（loss function）,量化了分类器输出地结果（预测值）和我们期望的结果（标签）之间的差距。<br>在机器学习中，<strong>损失函数（loss function）</strong>也称cost function（代价函数），是用来<strong>计算预测值和真实值的差距</strong>。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。</p>\n<p>损失函数分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong></p>\n<ul>\n<li>经验风险损失函数指预测结果和实际结果的差别</li>\n<li>结构风险损失函数是指经验风险损失函数加上正则项</li>\n</ul>\n<p>设总有N个样本的样本集为$(X,Y)&#x3D;(x_i,y_i)$，那么总的损失函数为$$L &#x3D; \\sum_{i&#x3D;1}^n l(y_i,f(x_i)) $$<br>其中 $y_i,i∈[1,N]$为样本$i$的真实值，$f(x_i),i∈[1,N]$为样本$i$的预测值， $f()$为分类或者回归函数。</p>\n<p>一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上似的损失函数值最小，即使得经验风险函数(Empirical risk)最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk），那么损失函数变为：$$L &#x3D; \\sum_{i&#x3D;1}^n l(y_i,f(x_i))+\\lambda R(w) $$</p>\n<p>R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。</p>\n<h4 id=\"常见的损失函数\"><a href=\"#常见的损失函数\" class=\"headerlink\" title=\"常见的损失函数\"></a>常见的损失函数</h4><ol>\n<li>0-1损失函数（0-1 loss function）$$L(y,f(x)) &#x3D; \\begin{cases}<br>1, y &#x3D; f(x) \\<br>0, y \\neq  f(x) \\<br>\\end{cases}$$ </li>\n<li>平方损失函数（quadratic loss function） $$L(Y,f(x)) &#x3D; (Y-f(x))^2$$</li>\n<li>L1正则损失函数（绝对损失函数 absolute loss function）$$L(Y,f(x)) &#x3D; \\left| Y-f(x) \\right|$$</li>\n<li>L2正则损失函数（即欧拉损失函数）$$L(Y,f(x)) &#x3D; \\sum_{i&#x3D;1}^n(Y-f(x))^2$$<br>当对L2取平均值，就变成均方误差（MSE, mean squared error） $$ MSE(y,{y}’) &#x3D; \\frac{1}{n}\\sum_{i&#x3D;1}^n (y^i - {y}’)^2 $$</li>\n<li>对数损失函数(logarithmic loss function)或对数似然损失函数(log-likelihood loss function)$$L(Y,P(Y|X)) &#x3D; -logP(Y|X)$$</li>\n</ol>\n<h4 id=\"交叉熵损失函数\"><a href=\"#交叉熵损失函数\" class=\"headerlink\" title=\"交叉熵损失函数\"></a>交叉熵损失函数</h4><p>交叉熵（Cross Entropy）是Loss函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小（用来评估当前训练得到的概率分布与真实分布的差异情况），常见的Loss函数就是均方平方差（Mean Squared Error）</p>\n<p>$$  loss &#x3D;\\frac{1}{n}\\sum_{i&#x3D;0}^n{(y_{i} \\cdot log(y_predicted_{i})<br>+(1-y_{i}) \\cdot log(1-y_predicted_{i})<br>)}  $$</p>\n<p>熵：用来表示所有信息量的期望<br>$$H(X)&#x3D;-\\sum_{i&#x3D;1}^n p(x_i)log(p(x_i))$$<br>信息量定义：$h(x) &#x3D; -log(p(x))$，概率分布函数p(x)</p>\n<h3 id=\"损失函数Api\"><a href=\"#损失函数Api\" class=\"headerlink\" title=\"损失函数Api\"></a>损失函数Api</h3><h4 id=\"tf-nn-softmax-cross-entropy-with-logits\"><a href=\"#tf-nn-softmax-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.softmax_cross_entropy_with_logits\"></a>tf.nn.softmax_cross_entropy_with_logits</h4><p>对于每个<strong>独立的分类任务</strong>，这个函数是去度量概率误差。比如，在 CIFAR-10 数据集上面，每张图片只有唯一一个分类标签：一张图可能是一只狗或者一辆卡车， 但绝对不可能两者都在一张图中。</p>\n<ul>\n<li>输入API的数据 logits 不能进行缩放，因为在这个API的执行中会进行 softmax 计算，如果 logits 进行了缩放，那么会影响计算正确率。</li>\n<li>logits 和 labels 必须有相同的数据维度 [batch_size, num_classes]，和相同的数据类型 float32 或者 float64 。</li>\n<li>它适用于每个类别相互独立且排斥的情况，一幅图只能属于一类，而不能同时包含一条狗和一只大象.</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">softmax_cross_entropy_with_logits(</span><br><span class=\"line\">    _sentinel=None,</span><br><span class=\"line\">    labels=None,</span><br><span class=\"line\">    logits=None,</span><br><span class=\"line\">    dim=-1,</span><br><span class=\"line\">    name=None</span><br><span class=\"line\">)</span><br><span class=\"line\">## softmax_cross_entropy_with_logits_v2 (现在)</span><br></pre></td></tr></table></figure>\n<p>流程：</p>\n<ol>\n<li>首先是对网络最后一层的输出做一个softmax（公式详见激活函数）</li>\n<li>对softmax输出的向量$[Y1,Y2,Y3,…]$和样本的时机标签做一个交叉熵</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf  </span><br><span class=\"line\"></span><br><span class=\"line\"># our NN&#x27;s output  </span><br><span class=\"line\">logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  </span><br><span class=\"line\"></span><br><span class=\"line\"># step1:do softmax  </span><br><span class=\"line\">y=tf.nn.softmax(logits)  </span><br><span class=\"line\"></span><br><span class=\"line\"># true label  </span><br><span class=\"line\">y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])  </span><br><span class=\"line\"></span><br><span class=\"line\"># step2:do cross_entropy  </span><br><span class=\"line\">cross_entropy = tf.reduce_sum(-tf.reduce_sum(y_*tf.log(y), axis=1))</span><br><span class=\"line\"></span><br><span class=\"line\"># do cross_entropy just one step  </span><br><span class=\"line\">cross_entropy2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"></span><br><span class=\"line\">with tf.Session() as sess:  </span><br><span class=\"line\">\tprint(sess.run(cross_entropy))</span><br><span class=\"line\">\tprint(sess.run(cross_entropy2))</span><br><span class=\"line\"></span><br><span class=\"line\"># 1.222818</span><br><span class=\"line\"># 1.222818</span><br></pre></td></tr></table></figure>\n\n\n\n\n<h4 id=\"tf-nn-sparse-softmax-cross-entropy-with-logits\"><a href=\"#tf-nn-sparse-softmax-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.sparse_softmax_cross_entropy_with_logits\"></a>tf.nn.sparse_softmax_cross_entropy_with_logits</h4><p>计算logits 和 labels 之间的稀疏softmax 交叉熵</p>\n<p>度量在离散分类任务中的错误率，这些类之间是相互排斥的（每个输入只能对应唯一确定的一个类）。举例来说，每个CIFAR-10 图片只能被标记为唯一的一个标签：一张图片可能是一只狗或一辆卡车，而不能两者都是。</p>\n<p>区别：<strong>sparse_softmax_cross_entropy_with_logits</strong> 直接用标签计算交叉熵，而 <strong>softmax_cross_entropy_with_logits</strong> 是标签的onehot向量参与计算。<strong>softmax_cross_entropy_with_logits</strong> 的 labels 是 <strong>sparse_softmax_cross_entropy_with_logits</strong> 的 labels 的一个独热版本。<br><strong>tf.nn.sparse_softmax_cross_entropy_with_logits</strong> 比 <strong>tf.nn.softmax_cross_entropy_with_logits</strong> 多了一步将labels稀疏化的操作</p>\n<blockquote>\n<p>onehot标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0</p>\n</blockquote>\n<blockquote>\n<p>稀疏化：例如表示一个3分类的一个样本的标签，稀疏表示的形式为[0,0,1]（这个表示这个样本为第3个分类），而非稀疏表示就表示为2（因为从0开始算，0,1,2,就能表示三类）</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">input_data = tf.Variable([[0.2, 0.1, 0.9], [0.3, 0.4, 0.6]], dtype=tf.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">output = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=input_data, </span><br><span class=\"line\"></span><br><span class=\"line\">labels=[0, 2])</span><br><span class=\"line\"></span><br><span class=\"line\">with tf.Session() as sess:</span><br><span class=\"line\">    init = tf.global_variables_initializer()</span><br><span class=\"line\">    sess.run(init)</span><br><span class=\"line\">    print(sess.run(output))</span><br><span class=\"line\"></span><br><span class=\"line\"># [ 1.36573195  0.93983102]</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"tf-nn-sigmoid-cross-entropy-with-logits\"><a href=\"#tf-nn-sigmoid-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.sigmoid_cross_entropy_with_logits\"></a>tf.nn.sigmoid_cross_entropy_with_logits</h4><p>函数意义：</p>\n<ul>\n<li>衡量的是分类任务中的概率误差，他也是试用于每一个类别都是相不排斥的</li>\n<li>这个函数的作用是计算经<strong>sigmoid</strong>函数激活之后的交叉熵。 </li>\n<li>与sigmoid搭配使用的交叉熵损失函数，输入不需要额外加一层sigmoid，<strong>tf.nn.sigmoid_cross_entropy_with_logits</strong>中会集成有sigmoid并进行了计算优化；它适用于分类的类别之间<strong>不是相互排斥</strong>的场景，即多个标签（如图片中包含狗和猫）。</li>\n</ul>\n<p>公式：$$targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))$$</p>\n<p>现在我们使用 x &#x3D; logits, z &#x3D; targets</p>\n<p>$$<br>\\begin {aligned}<br>sigmod交叉熵 &amp;&#x3D;z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) \\<br>&amp;&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x))) \\<br>&amp;&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\\<br>&amp;&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\\<br>&amp;&#x3D; (1 - z) * x + log(1 + exp(-x))\\<br>&amp;&#x3D; x - x * z + log(1 + exp(-x))<br>\\end {aligned}<br>$$</p>\n<p>当$x&lt;0$时，$ e^{-x} \\rightarrow \\infty$ 溢出，所以使用计算式：<br>$$ -x*z + log(1+e^{x}) $$</p>\n<p>$$<br>\\begin {aligned}<br>推到过程： &amp;&#x3D; x - x * z + log(1 + exp(-x))\\<br>    &amp;&#x3D; log(exp(x)) - x * z + log(1 + exp(-x)) \\<br>    &amp;&#x3D; - x * z + log(1 + exp(x))<br>\\end {aligned}<br>$$</p>\n<p>为了确保计算稳定，避免溢出，综合x&gt;0和x&lt;0的情况,我们使用以下函数式 ：<br>$$ max(x,0) - x*z + log(1+e^{−|x|})  $$</p>\n<h4 id=\"tf-nn-weighted-cross-entropy-with-logits\"><a href=\"#tf-nn-weighted-cross-entropy-with-logits\" class=\"headerlink\" title=\"tf.nn.weighted_cross_entropy_with_logits\"></a>tf.nn.weighted_cross_entropy_with_logits</h4><p>功能以及计算方式基本与<strong>tf_nn_sigmoid_cross_entropy_with_logits</strong>差不多,但是加上了权重的功能,是计算具有权重的sigmoid交叉熵函数<br>$$pos_weight * targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))$$</p>\n<p>现在我们使用 x &#x3D; logits, z &#x3D; targets, q &#x3D; pos_weight的代数式</p>\n<p>$$<br>\\begin {aligned}<br>  weighted交叉熵 &amp;&#x3D; qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\\<br>  &amp;&#x3D; qz * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))\\<br>  &amp;&#x3D; qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\\<br>  &amp;&#x3D; qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\\<br>  &amp;&#x3D; (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))\\<br>  &amp;&#x3D; (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))\\<br>\\end {aligned}<br>$$</p>\n<p>我们把$l &#x3D; (1 + (q - 1) * z)$, 来确保稳定性并且避免溢出,公式为：<br>$$(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))$$</p>\n<h3 id=\"了解\"><a href=\"#了解\" class=\"headerlink\" title=\"了解\"></a>了解</h3><p><strong>正则化</strong>：防止过拟合，提高泛化能力；正则化的原理就是在损失函数中加入评价模型复杂度的指标。R(w)就是评价模型复杂度的指标，一般只是权重w的函数，$\\lambda$表示复杂损失在总损失中的比例。</p>\n<p><strong>下溢出（underflow）和上溢出（overflow）</strong>:实数在计算机内用二进制表示，所以不是一个精确值，当数值过小的时候，被四舍五入为0，这就是下溢出。此时如果对这个数再做某些运算（例如除以它）就会出问题。反之当数值过大的时候，情况就变成上溢出。</p>\n"},{"title":"TensorFlow-激活函数","date":"2018-09-22T09:16:17.000Z","_content":"\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n### 激活函数\n\n激活函数（Activation Function）运行时激活神经网络中**某一部分神经元**，将**激活信息**向后传入下一层的**神经网络**。\n激活函数不会改变数据的维度，也就是**输入和输出的维度是相同的**。\n\n#### why (为什么要用激活函数)\n因为线性模型的表达能力不够，引入激活函数是为了添加非线性因素，解决线性模型所不能解决的问题\n> 参见：https://www.zhihu.com/question/22334626/answer/21036590\n\n#### what (激活函数是什么)\n激活函数就是一个普通函数。通过函数把特征保留并映射出来，这是神经网络能解决非线性问题关键。\n特性：\n\n* **非线性**：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。如果使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。\n* **可微性**：当优化方法是基于梯度的时候，这个性质是必须的。\n* **单调性**：当激活函数是单调的时候，单层网络能够保证是凸函数。\n* **f(x)≈x**：当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。\n* **输出值范围**：当激活函数输出值有限时，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限时，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate。\n\n#### 分类：\n\n* Traditional：`sigmoid(logistic)`、`tanh`\n* RELU Family：`RELU`、`Leaky RELU`、`PRELU`、`RRELU`\n* Exponential Family：`ELU`、`SELU`\n\n饱和性:\n\n* 软饱和：函数的导数趋近于0\n* 硬饱和：函数的导数等于0\n\n##### tf.nn.sigmoid\n\n$$f(x) = \\frac{1}{1+e^{-x}}$$\n\n![avatar](http://wx1.sinaimg.cn/mw690/007h1WTYly1fvrdsjbbehj30hs0d5t8x.jpg)\n\n\n优点：\n\n* sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层\n* 求导容易\n\n缺点：\n\n* sigmoid神经元有一个不好的特性，就是**当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0**。因此在反向传播时，这个局部梯度会与整个损失函数关于该单元输出的梯度相乘，结果也会接近为 0 。**因此这时梯度就对模型的更新没有任何贡献**。\n* sigmoid函数的输出不是零中心的，即sigmoid函数关于原点中心不对称\n\n> 非饱和神经元：反向传播经过这个神经元时，它的梯度不接近0，还能继续往前传\n\n```python\nimport tensorflow as tf\n\na = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])\nsess = tf.Session()\nprint(sess.run(tf.sigmoid(a)))\n\n# [[ 0.26894143  0.11920292]\n#  [ 0.7310586   0.88079703]\n#  [ 0.5         0.5       ]]\n```\n\n\n##### tf.nn.tanh\n\n$$f(x) = \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{1 - e^{-2x}}{1 + e^{-2x}} $$\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdsu5jjlj30hs0d5mxe.jpg)\n\ntf.nn.relu函数是将大于0的数保持不变，小于0的数置为0\n\ntanh函数也具有软饱和性。**因为它的输出是以0为中心，收敛速度比sigmoid函数要快**。但是仍然无法解决梯度消失问题。\n\n```\nimport tensorflow as tf\n\na = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])\nsess = tf.Session()\nprint(sess.run(tf.tanh(a)))\n# [[-0.76159418 -0.96402758]\n#  [ 0.76159418  0.96402758]\n#  [ 0.          0.        ]]\n```\n\n##### tf.nn.relu\n$$ f(x) = max(0, x) $$\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdrpcyyxj30hs0d50sx.jpg)\n\n这个函数的作用是计算激活函数relu，即max(features, 0)。 \n所有负数都会归一化为0，所以的正值保留为原值不变\n\n\n由上图的函数图像可以知道，relu在x<0时是硬饱和。由于当x>0时一阶导数为1。所以，relu函数在x>0时可以保持梯度不衰减，从而缓解梯度消失问题，还可以更快的去收敛。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新。我们称之为“神经元死亡”。\n\n* 优点在于不收”梯度消失”的影响,且取值范围在[0,+oo)\n* 缺点在于使用了较大的学习速率时,易受达到饱和的神经元的影响\n\n\n```pyton\nimport tensorflow as tf\n \na = tf.constant([-2,-1,0,2,3])\nwith tf.Session() as sess:\n \tprint(sess.run(tf.nn.relu(a)))\n\n# 结果 [0 0 0 2 3]\n```\n\n优化：leakrelu函数、ELU函数、SELU函数\n\n##### tf.nn.softmax\n\n$$softmax(x)_i = \\frac {e^{x_i}}{\\sum _j e^{x_j}}$$\n\nsoftmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n\nsoftmax它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！\n\n\n\n```\nimport tensorflow as tf\n\na = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0])\nwith tf.Session() as sess: \nprint sess.run(tf.nn.softmax(a))\n# [0.00426978  0.01160646  0.03154963  0.08576079  0.23312201  0.63369131]            \n```\n\n","source":"_posts/TensorFlow-激活函数.md","raw":"---\ntitle: TensorFlow-激活函数\ndate: 2018-09-22 17:16:17\ntags: tensorflow\ncategories : tensorflow\n---\n\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n### 激活函数\n\n激活函数（Activation Function）运行时激活神经网络中**某一部分神经元**，将**激活信息**向后传入下一层的**神经网络**。\n激活函数不会改变数据的维度，也就是**输入和输出的维度是相同的**。\n\n#### why (为什么要用激活函数)\n因为线性模型的表达能力不够，引入激活函数是为了添加非线性因素，解决线性模型所不能解决的问题\n> 参见：https://www.zhihu.com/question/22334626/answer/21036590\n\n#### what (激活函数是什么)\n激活函数就是一个普通函数。通过函数把特征保留并映射出来，这是神经网络能解决非线性问题关键。\n特性：\n\n* **非线性**：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。如果使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。\n* **可微性**：当优化方法是基于梯度的时候，这个性质是必须的。\n* **单调性**：当激活函数是单调的时候，单层网络能够保证是凸函数。\n* **f(x)≈x**：当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。\n* **输出值范围**：当激活函数输出值有限时，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限时，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate。\n\n#### 分类：\n\n* Traditional：`sigmoid(logistic)`、`tanh`\n* RELU Family：`RELU`、`Leaky RELU`、`PRELU`、`RRELU`\n* Exponential Family：`ELU`、`SELU`\n\n饱和性:\n\n* 软饱和：函数的导数趋近于0\n* 硬饱和：函数的导数等于0\n\n##### tf.nn.sigmoid\n\n$$f(x) = \\frac{1}{1+e^{-x}}$$\n\n![avatar](http://wx1.sinaimg.cn/mw690/007h1WTYly1fvrdsjbbehj30hs0d5t8x.jpg)\n\n\n优点：\n\n* sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层\n* 求导容易\n\n缺点：\n\n* sigmoid神经元有一个不好的特性，就是**当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0**。因此在反向传播时，这个局部梯度会与整个损失函数关于该单元输出的梯度相乘，结果也会接近为 0 。**因此这时梯度就对模型的更新没有任何贡献**。\n* sigmoid函数的输出不是零中心的，即sigmoid函数关于原点中心不对称\n\n> 非饱和神经元：反向传播经过这个神经元时，它的梯度不接近0，还能继续往前传\n\n```python\nimport tensorflow as tf\n\na = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])\nsess = tf.Session()\nprint(sess.run(tf.sigmoid(a)))\n\n# [[ 0.26894143  0.11920292]\n#  [ 0.7310586   0.88079703]\n#  [ 0.5         0.5       ]]\n```\n\n\n##### tf.nn.tanh\n\n$$f(x) = \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{1 - e^{-2x}}{1 + e^{-2x}} $$\n\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdsu5jjlj30hs0d5mxe.jpg)\n\ntf.nn.relu函数是将大于0的数保持不变，小于0的数置为0\n\ntanh函数也具有软饱和性。**因为它的输出是以0为中心，收敛速度比sigmoid函数要快**。但是仍然无法解决梯度消失问题。\n\n```\nimport tensorflow as tf\n\na = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])\nsess = tf.Session()\nprint(sess.run(tf.tanh(a)))\n# [[-0.76159418 -0.96402758]\n#  [ 0.76159418  0.96402758]\n#  [ 0.          0.        ]]\n```\n\n##### tf.nn.relu\n$$ f(x) = max(0, x) $$\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdrpcyyxj30hs0d50sx.jpg)\n\n这个函数的作用是计算激活函数relu，即max(features, 0)。 \n所有负数都会归一化为0，所以的正值保留为原值不变\n\n\n由上图的函数图像可以知道，relu在x<0时是硬饱和。由于当x>0时一阶导数为1。所以，relu函数在x>0时可以保持梯度不衰减，从而缓解梯度消失问题，还可以更快的去收敛。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新。我们称之为“神经元死亡”。\n\n* 优点在于不收”梯度消失”的影响,且取值范围在[0,+oo)\n* 缺点在于使用了较大的学习速率时,易受达到饱和的神经元的影响\n\n\n```pyton\nimport tensorflow as tf\n \na = tf.constant([-2,-1,0,2,3])\nwith tf.Session() as sess:\n \tprint(sess.run(tf.nn.relu(a)))\n\n# 结果 [0 0 0 2 3]\n```\n\n优化：leakrelu函数、ELU函数、SELU函数\n\n##### tf.nn.softmax\n\n$$softmax(x)_i = \\frac {e^{x_i}}{\\sum _j e^{x_j}}$$\n\nsoftmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n\nsoftmax它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！\n\n\n\n```\nimport tensorflow as tf\n\na = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0])\nwith tf.Session() as sess: \nprint sess.run(tf.nn.softmax(a))\n# [0.00426978  0.01160646  0.03154963  0.08576079  0.23312201  0.63369131]            \n```\n\n","slug":"TensorFlow-激活函数","published":1,"updated":"2024-04-07T07:52:49.497Z","comments":1,"layout":"post","photos":[],"_id":"clupb7ls20011obnsbh3a3lc3","content":"<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><p>激活函数（Activation Function）运行时激活神经网络中<strong>某一部分神经元</strong>，将<strong>激活信息</strong>向后传入下一层的<strong>神经网络</strong>。<br>激活函数不会改变数据的维度，也就是<strong>输入和输出的维度是相同的</strong>。</p>\n<h4 id=\"why-为什么要用激活函数\"><a href=\"#why-为什么要用激活函数\" class=\"headerlink\" title=\"why (为什么要用激活函数)\"></a>why (为什么要用激活函数)</h4><p>因为线性模型的表达能力不够，引入激活函数是为了添加非线性因素，解决线性模型所不能解决的问题</p>\n<blockquote>\n<p>参见：<a href=\"https://www.zhihu.com/question/22334626/answer/21036590\">https://www.zhihu.com/question/22334626/answer/21036590</a></p>\n</blockquote>\n<h4 id=\"what-激活函数是什么\"><a href=\"#what-激活函数是什么\" class=\"headerlink\" title=\"what (激活函数是什么)\"></a>what (激活函数是什么)</h4><p>激活函数就是一个普通函数。通过函数把特征保留并映射出来，这是神经网络能解决非线性问题关键。<br>特性：</p>\n<ul>\n<li><strong>非线性</strong>：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。如果使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。</li>\n<li><strong>可微性</strong>：当优化方法是基于梯度的时候，这个性质是必须的。</li>\n<li><strong>单调性</strong>：当激活函数是单调的时候，单层网络能够保证是凸函数。</li>\n<li><strong>f(x)≈x</strong>：当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。</li>\n<li><strong>输出值范围</strong>：当激活函数输出值有限时，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限时，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate。</li>\n</ul>\n<h4 id=\"分类：\"><a href=\"#分类：\" class=\"headerlink\" title=\"分类：\"></a>分类：</h4><ul>\n<li>Traditional：<code>sigmoid(logistic)</code>、<code>tanh</code></li>\n<li>RELU Family：<code>RELU</code>、<code>Leaky RELU</code>、<code>PRELU</code>、<code>RRELU</code></li>\n<li>Exponential Family：<code>ELU</code>、<code>SELU</code></li>\n</ul>\n<p>饱和性:</p>\n<ul>\n<li>软饱和：函数的导数趋近于0</li>\n<li>硬饱和：函数的导数等于0</li>\n</ul>\n<h5 id=\"tf-nn-sigmoid\"><a href=\"#tf-nn-sigmoid\" class=\"headerlink\" title=\"tf.nn.sigmoid\"></a>tf.nn.sigmoid</h5><p>$$f(x) &#x3D; \\frac{1}{1+e^{-x}}$$</p>\n<p><img src=\"http://wx1.sinaimg.cn/mw690/007h1WTYly1fvrdsjbbehj30hs0d5t8x.jpg\" alt=\"avatar\"></p>\n<p>优点：</p>\n<ul>\n<li>sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层</li>\n<li>求导容易</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>sigmoid神经元有一个不好的特性，就是<strong>当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0</strong>。因此在反向传播时，这个局部梯度会与整个损失函数关于该单元输出的梯度相乘，结果也会接近为 0 。<strong>因此这时梯度就对模型的更新没有任何贡献</strong>。</li>\n<li>sigmoid函数的输出不是零中心的，即sigmoid函数关于原点中心不对称</li>\n</ul>\n<blockquote>\n<p>非饱和神经元：反向传播经过这个神经元时，它的梯度不接近0，还能继续往前传</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([[-<span class=\"number\">1.0</span>, -<span class=\"number\">2.0</span>], [<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>]])</span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sess.run(tf.sigmoid(a)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># [[ 0.26894143  0.11920292]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.7310586   0.88079703]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.5         0.5       ]]</span></span><br></pre></td></tr></table></figure>\n\n\n<h5 id=\"tf-nn-tanh\"><a href=\"#tf-nn-tanh\" class=\"headerlink\" title=\"tf.nn.tanh\"></a>tf.nn.tanh</h5><p>$$f(x) &#x3D; \\tanh(x) &#x3D; \\frac{\\sinh(x)}{\\cosh(x)} &#x3D; \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} &#x3D; \\frac{1 - e^{-2x}}{1 + e^{-2x}} $$</p>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdsu5jjlj30hs0d5mxe.jpg\" alt=\"avatar\"></p>\n<p>tf.nn.relu函数是将大于0的数保持不变，小于0的数置为0</p>\n<p>tanh函数也具有软饱和性。<strong>因为它的输出是以0为中心，收敛速度比sigmoid函数要快</strong>。但是仍然无法解决梯度消失问题。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])</span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">print(sess.run(tf.tanh(a)))</span><br><span class=\"line\"># [[-0.76159418 -0.96402758]</span><br><span class=\"line\">#  [ 0.76159418  0.96402758]</span><br><span class=\"line\">#  [ 0.          0.        ]]</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"tf-nn-relu\"><a href=\"#tf-nn-relu\" class=\"headerlink\" title=\"tf.nn.relu\"></a>tf.nn.relu</h5><p>$$ f(x) &#x3D; max(0, x) $$<br><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdrpcyyxj30hs0d50sx.jpg\" alt=\"avatar\"></p>\n<p>这个函数的作用是计算激活函数relu，即max(features, 0)。<br>所有负数都会归一化为0，所以的正值保留为原值不变</p>\n<p>由上图的函数图像可以知道，relu在x&lt;0时是硬饱和。由于当x&gt;0时一阶导数为1。所以，relu函数在x&gt;0时可以保持梯度不衰减，从而缓解梯度消失问题，还可以更快的去收敛。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新。我们称之为“神经元死亡”。</p>\n<ul>\n<li>优点在于不收”梯度消失”的影响,且取值范围在[0,+oo)</li>\n<li>缺点在于使用了较大的学习速率时,易受达到饱和的神经元的影响</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"> </span><br><span class=\"line\">a = tf.constant([-2,-1,0,2,3])</span><br><span class=\"line\">with tf.Session() as sess:</span><br><span class=\"line\"> \tprint(sess.run(tf.nn.relu(a)))</span><br><span class=\"line\"></span><br><span class=\"line\"># 结果 [0 0 0 2 3]</span><br></pre></td></tr></table></figure>\n\n<p>优化：leakrelu函数、ELU函数、SELU函数</p>\n<h5 id=\"tf-nn-softmax\"><a href=\"#tf-nn-softmax\" class=\"headerlink\" title=\"tf.nn.softmax\"></a>tf.nn.softmax</h5><p>$$softmax(x)_i &#x3D; \\frac {e^{x_i}}{\\sum _j e^{x_j}}$$</p>\n<p>softmax &#x3D; tf.exp(logits) &#x2F; tf.reduce_sum(tf.exp(logits), axis)</p>\n<p>softmax它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0])</span><br><span class=\"line\">with tf.Session() as sess: </span><br><span class=\"line\">print sess.run(tf.nn.softmax(a))</span><br><span class=\"line\"># [0.00426978  0.01160646  0.03154963  0.08576079  0.23312201  0.63369131]            </span><br></pre></td></tr></table></figure>\n\n","cover":false,"excerpt":"","more":"<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><p>激活函数（Activation Function）运行时激活神经网络中<strong>某一部分神经元</strong>，将<strong>激活信息</strong>向后传入下一层的<strong>神经网络</strong>。<br>激活函数不会改变数据的维度，也就是<strong>输入和输出的维度是相同的</strong>。</p>\n<h4 id=\"why-为什么要用激活函数\"><a href=\"#why-为什么要用激活函数\" class=\"headerlink\" title=\"why (为什么要用激活函数)\"></a>why (为什么要用激活函数)</h4><p>因为线性模型的表达能力不够，引入激活函数是为了添加非线性因素，解决线性模型所不能解决的问题</p>\n<blockquote>\n<p>参见：<a href=\"https://www.zhihu.com/question/22334626/answer/21036590\">https://www.zhihu.com/question/22334626/answer/21036590</a></p>\n</blockquote>\n<h4 id=\"what-激活函数是什么\"><a href=\"#what-激活函数是什么\" class=\"headerlink\" title=\"what (激活函数是什么)\"></a>what (激活函数是什么)</h4><p>激活函数就是一个普通函数。通过函数把特征保留并映射出来，这是神经网络能解决非线性问题关键。<br>特性：</p>\n<ul>\n<li><strong>非线性</strong>：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。如果使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。</li>\n<li><strong>可微性</strong>：当优化方法是基于梯度的时候，这个性质是必须的。</li>\n<li><strong>单调性</strong>：当激活函数是单调的时候，单层网络能够保证是凸函数。</li>\n<li><strong>f(x)≈x</strong>：当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。</li>\n<li><strong>输出值范围</strong>：当激活函数输出值有限时，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限时，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate。</li>\n</ul>\n<h4 id=\"分类：\"><a href=\"#分类：\" class=\"headerlink\" title=\"分类：\"></a>分类：</h4><ul>\n<li>Traditional：<code>sigmoid(logistic)</code>、<code>tanh</code></li>\n<li>RELU Family：<code>RELU</code>、<code>Leaky RELU</code>、<code>PRELU</code>、<code>RRELU</code></li>\n<li>Exponential Family：<code>ELU</code>、<code>SELU</code></li>\n</ul>\n<p>饱和性:</p>\n<ul>\n<li>软饱和：函数的导数趋近于0</li>\n<li>硬饱和：函数的导数等于0</li>\n</ul>\n<h5 id=\"tf-nn-sigmoid\"><a href=\"#tf-nn-sigmoid\" class=\"headerlink\" title=\"tf.nn.sigmoid\"></a>tf.nn.sigmoid</h5><p>$$f(x) &#x3D; \\frac{1}{1+e^{-x}}$$</p>\n<p><img src=\"http://wx1.sinaimg.cn/mw690/007h1WTYly1fvrdsjbbehj30hs0d5t8x.jpg\" alt=\"avatar\"></p>\n<p>优点：</p>\n<ul>\n<li>sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层</li>\n<li>求导容易</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>sigmoid神经元有一个不好的特性，就是<strong>当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0</strong>。因此在反向传播时，这个局部梯度会与整个损失函数关于该单元输出的梯度相乘，结果也会接近为 0 。<strong>因此这时梯度就对模型的更新没有任何贡献</strong>。</li>\n<li>sigmoid函数的输出不是零中心的，即sigmoid函数关于原点中心不对称</li>\n</ul>\n<blockquote>\n<p>非饱和神经元：反向传播经过这个神经元时，它的梯度不接近0，还能继续往前传</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([[-<span class=\"number\">1.0</span>, -<span class=\"number\">2.0</span>], [<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>]])</span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sess.run(tf.sigmoid(a)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># [[ 0.26894143  0.11920292]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.7310586   0.88079703]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.5         0.5       ]]</span></span><br></pre></td></tr></table></figure>\n\n\n<h5 id=\"tf-nn-tanh\"><a href=\"#tf-nn-tanh\" class=\"headerlink\" title=\"tf.nn.tanh\"></a>tf.nn.tanh</h5><p>$$f(x) &#x3D; \\tanh(x) &#x3D; \\frac{\\sinh(x)}{\\cosh(x)} &#x3D; \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} &#x3D; \\frac{1 - e^{-2x}}{1 + e^{-2x}} $$</p>\n<p><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdsu5jjlj30hs0d5mxe.jpg\" alt=\"avatar\"></p>\n<p>tf.nn.relu函数是将大于0的数保持不变，小于0的数置为0</p>\n<p>tanh函数也具有软饱和性。<strong>因为它的输出是以0为中心，收敛速度比sigmoid函数要快</strong>。但是仍然无法解决梯度消失问题。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([[-1.0, -2.0], [1.0, 2.0], [0.0, 0.0]])</span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">print(sess.run(tf.tanh(a)))</span><br><span class=\"line\"># [[-0.76159418 -0.96402758]</span><br><span class=\"line\">#  [ 0.76159418  0.96402758]</span><br><span class=\"line\">#  [ 0.          0.        ]]</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"tf-nn-relu\"><a href=\"#tf-nn-relu\" class=\"headerlink\" title=\"tf.nn.relu\"></a>tf.nn.relu</h5><p>$$ f(x) &#x3D; max(0, x) $$<br><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fvrdrpcyyxj30hs0d50sx.jpg\" alt=\"avatar\"></p>\n<p>这个函数的作用是计算激活函数relu，即max(features, 0)。<br>所有负数都会归一化为0，所以的正值保留为原值不变</p>\n<p>由上图的函数图像可以知道，relu在x&lt;0时是硬饱和。由于当x&gt;0时一阶导数为1。所以，relu函数在x&gt;0时可以保持梯度不衰减，从而缓解梯度消失问题，还可以更快的去收敛。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新。我们称之为“神经元死亡”。</p>\n<ul>\n<li>优点在于不收”梯度消失”的影响,且取值范围在[0,+oo)</li>\n<li>缺点在于使用了较大的学习速率时,易受达到饱和的神经元的影响</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"> </span><br><span class=\"line\">a = tf.constant([-2,-1,0,2,3])</span><br><span class=\"line\">with tf.Session() as sess:</span><br><span class=\"line\"> \tprint(sess.run(tf.nn.relu(a)))</span><br><span class=\"line\"></span><br><span class=\"line\"># 结果 [0 0 0 2 3]</span><br></pre></td></tr></table></figure>\n\n<p>优化：leakrelu函数、ELU函数、SELU函数</p>\n<h5 id=\"tf-nn-softmax\"><a href=\"#tf-nn-softmax\" class=\"headerlink\" title=\"tf.nn.softmax\"></a>tf.nn.softmax</h5><p>$$softmax(x)_i &#x3D; \\frac {e^{x_i}}{\\sum _j e^{x_j}}$$</p>\n<p>softmax &#x3D; tf.exp(logits) &#x2F; tf.reduce_sum(tf.exp(logits), axis)</p>\n<p>softmax它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\"></span><br><span class=\"line\">a = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0])</span><br><span class=\"line\">with tf.Session() as sess: </span><br><span class=\"line\">print sess.run(tf.nn.softmax(a))</span><br><span class=\"line\"># [0.00426978  0.01160646  0.03154963  0.08576079  0.23312201  0.63369131]            </span><br></pre></td></tr></table></figure>\n\n"},{"title":"Tensorflow-卷积神经网络","date":"2019-12-06T08:55:42.000Z","_content":"\n### 神经网络\n具体过程就是：神经信号x乘上权重向量w，经过输入函数（Net input function）求和后，由激活函数（Activation function）输出。监督学习过程中，输出结果将会对比数据集样本结果（label），使用损失函数（cost function）计算损失，并且经过优化器迭代后更新权重。\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvx74sscdj30lz0asn4a.jpg)\n\n\n#### 输入层\n该层只起到输入信号的扇出作用.所以在计算网络的层数时不被记入。该层负责接收来自网络外部的信息。\n#### 输出层\n它是网络的最后一层，具有该网络的最大层号，负责输出网络的计算结果。\n#### 隐藏层\n除输入层和输出层以外的其他各层叫做隐藏层。隐藏层不直接接受外界的信号,也不直接向外界发送信号。\n\n#### 计算过程\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvxj3nd8nj30ib0aggp7.jpg\n)\n\n\n\n### 卷积神经网络\n卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。\n\n卷积神经网络分为了三部分，第一部分为输入层，第二部分由若干个卷积层和池化层 组成，第三部分为一个全连接的多层感知分类器构成。\n![avatar](http://wx2.sinaimg.cn/mw690/007h1WTYly1fywy9j09g3j30va08mgnj.jpg)\n\n\n\n#### 卷积层\n卷积层通常包含若干个特征平面（featureMap），每一个特征平面由一些矩形排列的神经元组成，同一特征平面的神经元共享权值，这个共享权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。\n![avatar](http://wx1.sinaimg.cn/mw690/007h1WTYly1fywy9mg3h9g30dj04owh8.gif)\n\n\n#### 池化层\n子采样也叫做池化（pooling），目的是为了减少特征图。通常有**均值采样（mean poooling）和 最大值采样（max pooling）**两种形式。\n\n池化操作将保存深度大小不变。\n\n如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。\n\n池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fywy419ylnj30lv0a8n0i.jpg)\n\n\n\n\n\n#### 全连接层\n全连接层的每一个节点都与上一层的所有节点相连，用来把前边提取到的特征综合起来。\n\n### CNN卷积神经网络\n#### 卷积核\n带着一组固定权重的神经元，通常是n\\*m二维的矩阵，n和m也是神经元的感受视野（local receptive fields）。n\\*m矩阵中存的是对感受视野中数据处理的系数。一个卷积核的滤波可以用来提取特定的特征（例如提取物体轮廓，颜色深浅等）。通过卷积层从原始数据中提取出的新的特征过程又成为feature map（特征映射）。将感受视野对输入的扫描间隔称为步长（stride）；\n\n#### tf.nn.conv2d\ntf.nn.conv2d\\(input, filter, strides, padding, use\\_cudnn\\_on\\_gpu=None, name=None\\) \n\n* **参数input**：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一\n\n* **参数filter**：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维\n*  **参数strides**：卷积时在图像每一维的步长，这是一个一维的向量，长度4\n*  **参数padding**：string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式。\n\t*  当其为\"VALID\"时，输出的size总比原图的size小，有时不会覆盖原图所有元素(既，可能舍弃边上的某些元素)。\n\t*  当其为\"SAME\"时，表示卷积核可以停留在图像边缘，输出并不一定和原图size一致，但会保证覆盖原图所有像素，不会舍弃边上的莫些元素;\n* **参数use\\_cudnn\\_on\\_gpu**：bool类型，是否使用cudnn加速，默认为true\n* **结果返回一个Tensor**，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。\n\n#### 网络案例：\nLeNet、AlexNet、ZF Net、GoogLeNet、VGGNet、ResNet\n\n\n","source":"_posts/Tensorflow-卷积神经网络.md","raw":"---\ntitle: Tensorflow-卷积神经网络\ndate: 2019-12-06 16:55:42\ntags: tensorflow\ncategories : tensorflow\n---\n\n### 神经网络\n具体过程就是：神经信号x乘上权重向量w，经过输入函数（Net input function）求和后，由激活函数（Activation function）输出。监督学习过程中，输出结果将会对比数据集样本结果（label），使用损失函数（cost function）计算损失，并且经过优化器迭代后更新权重。\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvx74sscdj30lz0asn4a.jpg)\n\n\n#### 输入层\n该层只起到输入信号的扇出作用.所以在计算网络的层数时不被记入。该层负责接收来自网络外部的信息。\n#### 输出层\n它是网络的最后一层，具有该网络的最大层号，负责输出网络的计算结果。\n#### 隐藏层\n除输入层和输出层以外的其他各层叫做隐藏层。隐藏层不直接接受外界的信号,也不直接向外界发送信号。\n\n#### 计算过程\n\n![avatar](http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvxj3nd8nj30ib0aggp7.jpg\n)\n\n\n\n### 卷积神经网络\n卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。\n\n卷积神经网络分为了三部分，第一部分为输入层，第二部分由若干个卷积层和池化层 组成，第三部分为一个全连接的多层感知分类器构成。\n![avatar](http://wx2.sinaimg.cn/mw690/007h1WTYly1fywy9j09g3j30va08mgnj.jpg)\n\n\n\n#### 卷积层\n卷积层通常包含若干个特征平面（featureMap），每一个特征平面由一些矩形排列的神经元组成，同一特征平面的神经元共享权值，这个共享权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。\n![avatar](http://wx1.sinaimg.cn/mw690/007h1WTYly1fywy9mg3h9g30dj04owh8.gif)\n\n\n#### 池化层\n子采样也叫做池化（pooling），目的是为了减少特征图。通常有**均值采样（mean poooling）和 最大值采样（max pooling）**两种形式。\n\n池化操作将保存深度大小不变。\n\n如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。\n\n池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算\n![avatar](http://wx4.sinaimg.cn/mw690/007h1WTYly1fywy419ylnj30lv0a8n0i.jpg)\n\n\n\n\n\n#### 全连接层\n全连接层的每一个节点都与上一层的所有节点相连，用来把前边提取到的特征综合起来。\n\n### CNN卷积神经网络\n#### 卷积核\n带着一组固定权重的神经元，通常是n\\*m二维的矩阵，n和m也是神经元的感受视野（local receptive fields）。n\\*m矩阵中存的是对感受视野中数据处理的系数。一个卷积核的滤波可以用来提取特定的特征（例如提取物体轮廓，颜色深浅等）。通过卷积层从原始数据中提取出的新的特征过程又成为feature map（特征映射）。将感受视野对输入的扫描间隔称为步长（stride）；\n\n#### tf.nn.conv2d\ntf.nn.conv2d\\(input, filter, strides, padding, use\\_cudnn\\_on\\_gpu=None, name=None\\) \n\n* **参数input**：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一\n\n* **参数filter**：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维\n*  **参数strides**：卷积时在图像每一维的步长，这是一个一维的向量，长度4\n*  **参数padding**：string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式。\n\t*  当其为\"VALID\"时，输出的size总比原图的size小，有时不会覆盖原图所有元素(既，可能舍弃边上的某些元素)。\n\t*  当其为\"SAME\"时，表示卷积核可以停留在图像边缘，输出并不一定和原图size一致，但会保证覆盖原图所有像素，不会舍弃边上的莫些元素;\n* **参数use\\_cudnn\\_on\\_gpu**：bool类型，是否使用cudnn加速，默认为true\n* **结果返回一个Tensor**，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。\n\n#### 网络案例：\nLeNet、AlexNet、ZF Net、GoogLeNet、VGGNet、ResNet\n\n\n","slug":"Tensorflow-卷积神经网络","published":1,"updated":"2024-04-07T07:52:49.501Z","comments":1,"layout":"post","photos":[],"_id":"clupb7ls30016obns5vsw7rie","content":"<h3 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h3><p>具体过程就是：神经信号x乘上权重向量w，经过输入函数（Net input function）求和后，由激活函数（Activation function）输出。监督学习过程中，输出结果将会对比数据集样本结果（label），使用损失函数（cost function）计算损失，并且经过优化器迭代后更新权重。</p>\n<p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvx74sscdj30lz0asn4a.jpg\" alt=\"avatar\"></p>\n<h4 id=\"输入层\"><a href=\"#输入层\" class=\"headerlink\" title=\"输入层\"></a>输入层</h4><p>该层只起到输入信号的扇出作用.所以在计算网络的层数时不被记入。该层负责接收来自网络外部的信息。</p>\n<h4 id=\"输出层\"><a href=\"#输出层\" class=\"headerlink\" title=\"输出层\"></a>输出层</h4><p>它是网络的最后一层，具有该网络的最大层号，负责输出网络的计算结果。</p>\n<h4 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h4><p>除输入层和输出层以外的其他各层叫做隐藏层。隐藏层不直接接受外界的信号,也不直接向外界发送信号。</p>\n<h4 id=\"计算过程\"><a href=\"#计算过程\" class=\"headerlink\" title=\"计算过程\"></a>计算过程</h4><p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvxj3nd8nj30ib0aggp7.jpg\" alt=\"avatar\"></p>\n<h3 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。</p>\n<p>卷积神经网络分为了三部分，第一部分为输入层，第二部分由若干个卷积层和池化层 组成，第三部分为一个全连接的多层感知分类器构成。<br><img src=\"http://wx2.sinaimg.cn/mw690/007h1WTYly1fywy9j09g3j30va08mgnj.jpg\" alt=\"avatar\"></p>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p>卷积层通常包含若干个特征平面（featureMap），每一个特征平面由一些矩形排列的神经元组成，同一特征平面的神经元共享权值，这个共享权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。<br><img src=\"http://wx1.sinaimg.cn/mw690/007h1WTYly1fywy9mg3h9g30dj04owh8.gif\" alt=\"avatar\"></p>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>子采样也叫做池化（pooling），目的是为了减少特征图。通常有<strong>均值采样（mean poooling）和 最大值采样（max pooling）</strong>两种形式。</p>\n<p>池化操作将保存深度大小不变。</p>\n<p>如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。</p>\n<p>池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算<br><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fywy419ylnj30lv0a8n0i.jpg\" alt=\"avatar\"></p>\n<h4 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h4><p>全连接层的每一个节点都与上一层的所有节点相连，用来把前边提取到的特征综合起来。</p>\n<h3 id=\"CNN卷积神经网络\"><a href=\"#CNN卷积神经网络\" class=\"headerlink\" title=\"CNN卷积神经网络\"></a>CNN卷积神经网络</h3><h4 id=\"卷积核\"><a href=\"#卷积核\" class=\"headerlink\" title=\"卷积核\"></a>卷积核</h4><p>带着一组固定权重的神经元，通常是n*m二维的矩阵，n和m也是神经元的感受视野（local receptive fields）。n*m矩阵中存的是对感受视野中数据处理的系数。一个卷积核的滤波可以用来提取特定的特征（例如提取物体轮廓，颜色深浅等）。通过卷积层从原始数据中提取出的新的特征过程又成为feature map（特征映射）。将感受视野对输入的扫描间隔称为步长（stride）；</p>\n<h4 id=\"tf-nn-conv2d\"><a href=\"#tf-nn-conv2d\" class=\"headerlink\" title=\"tf.nn.conv2d\"></a>tf.nn.conv2d</h4><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu&#x3D;None, name&#x3D;None) </p>\n<ul>\n<li><p><strong>参数input</strong>：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一</p>\n</li>\n<li><p><strong>参数filter</strong>：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维</p>\n</li>\n<li><p><strong>参数strides</strong>：卷积时在图像每一维的步长，这是一个一维的向量，长度4</p>\n</li>\n<li><p><strong>参数padding</strong>：string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式。</p>\n<ul>\n<li>当其为”VALID”时，输出的size总比原图的size小，有时不会覆盖原图所有元素(既，可能舍弃边上的某些元素)。</li>\n<li>当其为”SAME”时，表示卷积核可以停留在图像边缘，输出并不一定和原图size一致，但会保证覆盖原图所有像素，不会舍弃边上的莫些元素;</li>\n</ul>\n</li>\n<li><p><strong>参数use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速，默认为true</p>\n</li>\n<li><p><strong>结果返回一个Tensor</strong>，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。</p>\n</li>\n</ul>\n<h4 id=\"网络案例：\"><a href=\"#网络案例：\" class=\"headerlink\" title=\"网络案例：\"></a>网络案例：</h4><p>LeNet、AlexNet、ZF Net、GoogLeNet、VGGNet、ResNet</p>\n","cover":false,"excerpt":"","more":"<h3 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h3><p>具体过程就是：神经信号x乘上权重向量w，经过输入函数（Net input function）求和后，由激活函数（Activation function）输出。监督学习过程中，输出结果将会对比数据集样本结果（label），使用损失函数（cost function）计算损失，并且经过优化器迭代后更新权重。</p>\n<p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvx74sscdj30lz0asn4a.jpg\" alt=\"avatar\"></p>\n<h4 id=\"输入层\"><a href=\"#输入层\" class=\"headerlink\" title=\"输入层\"></a>输入层</h4><p>该层只起到输入信号的扇出作用.所以在计算网络的层数时不被记入。该层负责接收来自网络外部的信息。</p>\n<h4 id=\"输出层\"><a href=\"#输出层\" class=\"headerlink\" title=\"输出层\"></a>输出层</h4><p>它是网络的最后一层，具有该网络的最大层号，负责输出网络的计算结果。</p>\n<h4 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h4><p>除输入层和输出层以外的其他各层叫做隐藏层。隐藏层不直接接受外界的信号,也不直接向外界发送信号。</p>\n<h4 id=\"计算过程\"><a href=\"#计算过程\" class=\"headerlink\" title=\"计算过程\"></a>计算过程</h4><p><img src=\"http://wx3.sinaimg.cn/mw690/007h1WTYly1fyvxj3nd8nj30ib0aggp7.jpg\" alt=\"avatar\"></p>\n<h3 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。</p>\n<p>卷积神经网络分为了三部分，第一部分为输入层，第二部分由若干个卷积层和池化层 组成，第三部分为一个全连接的多层感知分类器构成。<br><img src=\"http://wx2.sinaimg.cn/mw690/007h1WTYly1fywy9j09g3j30va08mgnj.jpg\" alt=\"avatar\"></p>\n<h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><p>卷积层通常包含若干个特征平面（featureMap），每一个特征平面由一些矩形排列的神经元组成，同一特征平面的神经元共享权值，这个共享权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。<br><img src=\"http://wx1.sinaimg.cn/mw690/007h1WTYly1fywy9mg3h9g30dj04owh8.gif\" alt=\"avatar\"></p>\n<h4 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h4><p>子采样也叫做池化（pooling），目的是为了减少特征图。通常有<strong>均值采样（mean poooling）和 最大值采样（max pooling）</strong>两种形式。</p>\n<p>池化操作将保存深度大小不变。</p>\n<p>如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。</p>\n<p>池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算<br><img src=\"http://wx4.sinaimg.cn/mw690/007h1WTYly1fywy419ylnj30lv0a8n0i.jpg\" alt=\"avatar\"></p>\n<h4 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h4><p>全连接层的每一个节点都与上一层的所有节点相连，用来把前边提取到的特征综合起来。</p>\n<h3 id=\"CNN卷积神经网络\"><a href=\"#CNN卷积神经网络\" class=\"headerlink\" title=\"CNN卷积神经网络\"></a>CNN卷积神经网络</h3><h4 id=\"卷积核\"><a href=\"#卷积核\" class=\"headerlink\" title=\"卷积核\"></a>卷积核</h4><p>带着一组固定权重的神经元，通常是n*m二维的矩阵，n和m也是神经元的感受视野（local receptive fields）。n*m矩阵中存的是对感受视野中数据处理的系数。一个卷积核的滤波可以用来提取特定的特征（例如提取物体轮廓，颜色深浅等）。通过卷积层从原始数据中提取出的新的特征过程又成为feature map（特征映射）。将感受视野对输入的扫描间隔称为步长（stride）；</p>\n<h4 id=\"tf-nn-conv2d\"><a href=\"#tf-nn-conv2d\" class=\"headerlink\" title=\"tf.nn.conv2d\"></a>tf.nn.conv2d</h4><p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu&#x3D;None, name&#x3D;None) </p>\n<ul>\n<li><p><strong>参数input</strong>：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一</p>\n</li>\n<li><p><strong>参数filter</strong>：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维</p>\n</li>\n<li><p><strong>参数strides</strong>：卷积时在图像每一维的步长，这是一个一维的向量，长度4</p>\n</li>\n<li><p><strong>参数padding</strong>：string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式。</p>\n<ul>\n<li>当其为”VALID”时，输出的size总比原图的size小，有时不会覆盖原图所有元素(既，可能舍弃边上的某些元素)。</li>\n<li>当其为”SAME”时，表示卷积核可以停留在图像边缘，输出并不一定和原图size一致，但会保证覆盖原图所有像素，不会舍弃边上的莫些元素;</li>\n</ul>\n</li>\n<li><p><strong>参数use_cudnn_on_gpu</strong>：bool类型，是否使用cudnn加速，默认为true</p>\n</li>\n<li><p><strong>结果返回一个Tensor</strong>，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。</p>\n</li>\n</ul>\n<h4 id=\"网络案例：\"><a href=\"#网络案例：\" class=\"headerlink\" title=\"网络案例：\"></a>网络案例：</h4><p>LeNet、AlexNet、ZF Net、GoogLeNet、VGGNet、ResNet</p>\n"},{"title":"TensorFlow 基本概念与函数-1","year":2018,"date":"2018-09-03T12:18:38.000Z","_content":"\n### 编程模型\n> TensorFlow的数据流图是由`节点（Node） `和`边（edge）`组成的`有向无环图（directed acycline graph，DAG）`。TensorFlow 由 Tensor 和 Flow 两部 分组成，Tensor(张量)代表了数据流图中的边，而 Flow(流动)这个动作就代表了数据流图中节点所做的操作。\n> \n\n计算过程：首先从输入层开始，经过塑形层后，一层一层进行前向传播运算。Relu层（隐藏层）里两个参数，即Wh1和bh1，在输出前使用Rule（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm 和 bsm。用Softmax来计算输出结果中各个类别的概率分布。用交叉熵来度量两个概率分布（源样本的概率分布和输出结果 的概率分布）直接的相似性。然后开始计算梯度，这是需要参数Wh1、bh1、Wsm 和 bsm，以及 交叉熵后的结果。随后进入 SGD 训练，也就是反向传播的过程，从上往下计算每一层的参数， 依次进行更新。也就是说，计算和更新的顺序为 bsm、Wsm、bh1 和 Wh1。\n\n![avatar](https://wx3.sinaimg.cn/large/007h1WTYly1fup5sc9m7ag30700cgwol.gif)\n\n### 张量\nTensorFlow用张量这种数据结构来表示所有的数据。一个张量有一个静态类型和动态类型的维数。张量可以在图中的节点之间流通。\n张量的维数来被描述为`阶`\n> 秩：Tensor维度的数量\n\n#### 属性\n* 数据类型（例如 float32、int32 或 string）\n* 形状：即张量的维数和每个维度的大小\n\n#### 方法\n* tf.rank：返回张量的秩\n\n```python\n# shape of tensor 't' is [2, 2, 3]\nt = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\ntf.rank(t)  # 3\n```\n\n* tf.shape：返回某个张量的形状\n\n```python\nt = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\ntf.shape(t)  # [2, 2, 3]\n```\n* tf.reshape：重构张量\n* tf.cast：将 tf.Tensor 从一种数据类型转型为另一种\n\n### 变量\n\n#### 变量集合\n默认情况下，每个 tf.Variable 都放置在以下两个集合中\n\n* `tf.GraphKeys.GLOBAL_VARIABLES` - 可以在多台设备间共享的变量，\n* `tf.GraphKeys.TRAINABLE_VARIABLES` - TensorFlow 将计算其梯度的变量\n\n```\n# 如果不希望变量可训练，可以将其添加到 tf.GraphKeys.LOCAL_VARIABLES 集合中\nmy_local = tf.get_variable(\"my_local\", shape=(),collections=[tf.GraphKeys.LOCAL_VARIABLES])\n# 或\nmy_non_trainable = tf.get_variable(\"my_non_trainable\",\n                                   shape=(),\n                                   trainable=False)\n```\n可以定义自己的集合\n\n```\n# 将名为 my_local 的现有变量添加到名为 my_collection_name 的集合中\ntf.add_to_collection(\"my_collection_name\", my_local)\n# 获取某个集合中的所有变量（或其他对象）的列表\ntf.get_collection(\"my_collection_name\")\n\n```\n\n#### 初始化变量\n> 初始化器: tf.initializers（constant、zeros）\n\n要在训练开始前一次性初始化所有可训练变量，请调用`tf.global_variables_initializer()`。默认情况下，`tf.global_variables_initializer`不会指定变量的初始化顺序\n\n```\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nw = tf.get_variable(\"w\", initializer=v.initialized_value() + 1)\n```\n\n#### 基本操作\n要为变量赋值，请使用 assign、assign_add 方法以及 tf.Variable 类中的友元。\n\n共享变量两种方式：\n\n* 显式传递 `tf.Variable` 对象\n* 在 `tf.variable_scope` 对象内隐式封装 tf.Variable 对象。\n\n变量作用域允许您在调用隐式创建和使用变量的函数时控制变量重用。作用域还允许您以分层和可理解的方式命名变量。\n\n### 图和会话\n\n#### tf.Graph \n* 图结构：图的节点和边缘，表示各个操作组合在一起的方式，但不规定它们的使用方式。\n* 图集合：TensorFlow 提供了一种在`tf.Graph`中存储元数据集合的通用机制。`tf.add_to_collection` 函数允许您将对象列表与一个键关联（其中 tf.GraphKeys 定义了部分标准键），`tf.get_collection` 允许您查询与某个键关联的所有对象。\n\n大多数 TensorFlow 程序都以数据流图构建阶段开始。在此阶段，您会调用 TensorFlow API 函数，这些函数可构建新的 `tf.Operation（节点）`和 `tf.Tensor（边缘` 对象并将它们添加到 tf.Graph 实例中。\n\n类张量类型:\n\n* tf.Tensor\n* tf.Variable\n* numpy.ndarray\n* list（以及类似于张量的对象的列表）\n* 标量 Python 类型：bool、float、int、str\n\n> 注意：在 TensorFlow API 中调用大多数函数时，只是将操作和张量添加到默认图中，并不会执行实际的计算。您应编写这些函数，直到拥有表示整个计算（例如执行梯度下降法的一步）的 tf.Tensor 或 tf.Operation，然后将该对象传递给 tf.Session 以执行计算。\n\n\n#### tf.Sessoin\n`tf.Session` 拥有物理资源（例如 GPU 和网络连接），因此通常（在 with 代码块中）用作上下文管理器，并在您退出代码块时自动关闭会话\n\n```python\n# Create a default in-process session.\nwith tf.Session() as sess:\n  # ...\n\n# Create a remote session.\nwith tf.Session(\"grpc://example.org:2222\"):\n  # ...\n```\n\n* tf.Session.init \n\t* target： 如果将此参数留空（默认设置），会话将仅使用本地机器中的设备。也可以指定 grpc:// 网址，以便指定 TensorFlow 服务器的地址，这使得会话可以访问该服务器控制的机器上的所有设备。\n\t* graph：默认情况下，新的 tf.Session 将绑定到当前的默认图，并且仅能够在当前的默认图中运行操作。如果您在程序中使用了多个图，则可以在构建会话时指定明确的 `tf.Graph`。\n\t* config：此参数允许您指定一个控制会话行为的`tf.ConfigProto`\n* `tf.Session.run`：运行 `tf.Operation` 或评估 `tf.Tensor` 的主要机制\n\t* `tf.Session.run` 要求您指定一组 fetch，这些 fetch 可确定返回值，并且可能是 `tf.Operation`、`tf.Tensor` 或`类张量类型`\n\t* `tf.Session.run` 也可以选择接受 Feed 字典，该字典是从 `tf.Tensor` 对象（通常是 tf.placeholder 张量）到在执行时会被替换为这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射\n\t* `tf.Session.run` 也接受可选的 options 参数（允许您指定与调用有关的选项）和可选的 run_metadata 参数（允许您收集与执行有关的元数据）","source":"_posts/Tensorflow 入门-1.md","raw":"---\ntitle: TensorFlow 基本概念与函数-1\nyear: 2018\ndate: 2018-09-03 20:18:38\ntags: tensorflow\ncategories: tensorflow\n---\n\n### 编程模型\n> TensorFlow的数据流图是由`节点（Node） `和`边（edge）`组成的`有向无环图（directed acycline graph，DAG）`。TensorFlow 由 Tensor 和 Flow 两部 分组成，Tensor(张量)代表了数据流图中的边，而 Flow(流动)这个动作就代表了数据流图中节点所做的操作。\n> \n\n计算过程：首先从输入层开始，经过塑形层后，一层一层进行前向传播运算。Relu层（隐藏层）里两个参数，即Wh1和bh1，在输出前使用Rule（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm 和 bsm。用Softmax来计算输出结果中各个类别的概率分布。用交叉熵来度量两个概率分布（源样本的概率分布和输出结果 的概率分布）直接的相似性。然后开始计算梯度，这是需要参数Wh1、bh1、Wsm 和 bsm，以及 交叉熵后的结果。随后进入 SGD 训练，也就是反向传播的过程，从上往下计算每一层的参数， 依次进行更新。也就是说，计算和更新的顺序为 bsm、Wsm、bh1 和 Wh1。\n\n![avatar](https://wx3.sinaimg.cn/large/007h1WTYly1fup5sc9m7ag30700cgwol.gif)\n\n### 张量\nTensorFlow用张量这种数据结构来表示所有的数据。一个张量有一个静态类型和动态类型的维数。张量可以在图中的节点之间流通。\n张量的维数来被描述为`阶`\n> 秩：Tensor维度的数量\n\n#### 属性\n* 数据类型（例如 float32、int32 或 string）\n* 形状：即张量的维数和每个维度的大小\n\n#### 方法\n* tf.rank：返回张量的秩\n\n```python\n# shape of tensor 't' is [2, 2, 3]\nt = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\ntf.rank(t)  # 3\n```\n\n* tf.shape：返回某个张量的形状\n\n```python\nt = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\ntf.shape(t)  # [2, 2, 3]\n```\n* tf.reshape：重构张量\n* tf.cast：将 tf.Tensor 从一种数据类型转型为另一种\n\n### 变量\n\n#### 变量集合\n默认情况下，每个 tf.Variable 都放置在以下两个集合中\n\n* `tf.GraphKeys.GLOBAL_VARIABLES` - 可以在多台设备间共享的变量，\n* `tf.GraphKeys.TRAINABLE_VARIABLES` - TensorFlow 将计算其梯度的变量\n\n```\n# 如果不希望变量可训练，可以将其添加到 tf.GraphKeys.LOCAL_VARIABLES 集合中\nmy_local = tf.get_variable(\"my_local\", shape=(),collections=[tf.GraphKeys.LOCAL_VARIABLES])\n# 或\nmy_non_trainable = tf.get_variable(\"my_non_trainable\",\n                                   shape=(),\n                                   trainable=False)\n```\n可以定义自己的集合\n\n```\n# 将名为 my_local 的现有变量添加到名为 my_collection_name 的集合中\ntf.add_to_collection(\"my_collection_name\", my_local)\n# 获取某个集合中的所有变量（或其他对象）的列表\ntf.get_collection(\"my_collection_name\")\n\n```\n\n#### 初始化变量\n> 初始化器: tf.initializers（constant、zeros）\n\n要在训练开始前一次性初始化所有可训练变量，请调用`tf.global_variables_initializer()`。默认情况下，`tf.global_variables_initializer`不会指定变量的初始化顺序\n\n```\nv = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\nw = tf.get_variable(\"w\", initializer=v.initialized_value() + 1)\n```\n\n#### 基本操作\n要为变量赋值，请使用 assign、assign_add 方法以及 tf.Variable 类中的友元。\n\n共享变量两种方式：\n\n* 显式传递 `tf.Variable` 对象\n* 在 `tf.variable_scope` 对象内隐式封装 tf.Variable 对象。\n\n变量作用域允许您在调用隐式创建和使用变量的函数时控制变量重用。作用域还允许您以分层和可理解的方式命名变量。\n\n### 图和会话\n\n#### tf.Graph \n* 图结构：图的节点和边缘，表示各个操作组合在一起的方式，但不规定它们的使用方式。\n* 图集合：TensorFlow 提供了一种在`tf.Graph`中存储元数据集合的通用机制。`tf.add_to_collection` 函数允许您将对象列表与一个键关联（其中 tf.GraphKeys 定义了部分标准键），`tf.get_collection` 允许您查询与某个键关联的所有对象。\n\n大多数 TensorFlow 程序都以数据流图构建阶段开始。在此阶段，您会调用 TensorFlow API 函数，这些函数可构建新的 `tf.Operation（节点）`和 `tf.Tensor（边缘` 对象并将它们添加到 tf.Graph 实例中。\n\n类张量类型:\n\n* tf.Tensor\n* tf.Variable\n* numpy.ndarray\n* list（以及类似于张量的对象的列表）\n* 标量 Python 类型：bool、float、int、str\n\n> 注意：在 TensorFlow API 中调用大多数函数时，只是将操作和张量添加到默认图中，并不会执行实际的计算。您应编写这些函数，直到拥有表示整个计算（例如执行梯度下降法的一步）的 tf.Tensor 或 tf.Operation，然后将该对象传递给 tf.Session 以执行计算。\n\n\n#### tf.Sessoin\n`tf.Session` 拥有物理资源（例如 GPU 和网络连接），因此通常（在 with 代码块中）用作上下文管理器，并在您退出代码块时自动关闭会话\n\n```python\n# Create a default in-process session.\nwith tf.Session() as sess:\n  # ...\n\n# Create a remote session.\nwith tf.Session(\"grpc://example.org:2222\"):\n  # ...\n```\n\n* tf.Session.init \n\t* target： 如果将此参数留空（默认设置），会话将仅使用本地机器中的设备。也可以指定 grpc:// 网址，以便指定 TensorFlow 服务器的地址，这使得会话可以访问该服务器控制的机器上的所有设备。\n\t* graph：默认情况下，新的 tf.Session 将绑定到当前的默认图，并且仅能够在当前的默认图中运行操作。如果您在程序中使用了多个图，则可以在构建会话时指定明确的 `tf.Graph`。\n\t* config：此参数允许您指定一个控制会话行为的`tf.ConfigProto`\n* `tf.Session.run`：运行 `tf.Operation` 或评估 `tf.Tensor` 的主要机制\n\t* `tf.Session.run` 要求您指定一组 fetch，这些 fetch 可确定返回值，并且可能是 `tf.Operation`、`tf.Tensor` 或`类张量类型`\n\t* `tf.Session.run` 也可以选择接受 Feed 字典，该字典是从 `tf.Tensor` 对象（通常是 tf.placeholder 张量）到在执行时会被替换为这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射\n\t* `tf.Session.run` 也接受可选的 options 参数（允许您指定与调用有关的选项）和可选的 run_metadata 参数（允许您收集与执行有关的元数据）","slug":"Tensorflow 入门-1","published":1,"updated":"2024-04-07T07:52:49.500Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lsf002qobns2evp1gyg","content":"<h3 id=\"编程模型\"><a href=\"#编程模型\" class=\"headerlink\" title=\"编程模型\"></a>编程模型</h3><blockquote>\n<p>TensorFlow的数据流图是由<code>节点（Node） </code>和<code>边（edge）</code>组成的<code>有向无环图（directed acycline graph，DAG）</code>。TensorFlow 由 Tensor 和 Flow 两部 分组成，Tensor(张量)代表了数据流图中的边，而 Flow(流动)这个动作就代表了数据流图中节点所做的操作。</p>\n</blockquote>\n<p>计算过程：首先从输入层开始，经过塑形层后，一层一层进行前向传播运算。Relu层（隐藏层）里两个参数，即Wh1和bh1，在输出前使用Rule（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm 和 bsm。用Softmax来计算输出结果中各个类别的概率分布。用交叉熵来度量两个概率分布（源样本的概率分布和输出结果 的概率分布）直接的相似性。然后开始计算梯度，这是需要参数Wh1、bh1、Wsm 和 bsm，以及 交叉熵后的结果。随后进入 SGD 训练，也就是反向传播的过程，从上往下计算每一层的参数， 依次进行更新。也就是说，计算和更新的顺序为 bsm、Wsm、bh1 和 Wh1。</p>\n<p><img src=\"https://wx3.sinaimg.cn/large/007h1WTYly1fup5sc9m7ag30700cgwol.gif\" alt=\"avatar\"></p>\n<h3 id=\"张量\"><a href=\"#张量\" class=\"headerlink\" title=\"张量\"></a>张量</h3><p>TensorFlow用张量这种数据结构来表示所有的数据。一个张量有一个静态类型和动态类型的维数。张量可以在图中的节点之间流通。<br>张量的维数来被描述为<code>阶</code></p>\n<blockquote>\n<p>秩：Tensor维度的数量</p>\n</blockquote>\n<h4 id=\"属性\"><a href=\"#属性\" class=\"headerlink\" title=\"属性\"></a>属性</h4><ul>\n<li>数据类型（例如 float32、int32 或 string）</li>\n<li>形状：即张量的维数和每个维度的大小</li>\n</ul>\n<h4 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h4><ul>\n<li>tf.rank：返回张量的秩</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># shape of tensor &#x27;t&#x27; is [2, 2, 3]</span></span><br><span class=\"line\">t = tf.constant([[[<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]], [[<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]]])</span><br><span class=\"line\">tf.rank(t)  <span class=\"comment\"># 3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tf.shape：返回某个张量的形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t = tf.constant([[[<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]], [[<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]]])</span><br><span class=\"line\">tf.shape(t)  <span class=\"comment\"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.reshape：重构张量</li>\n<li>tf.cast：将 tf.Tensor 从一种数据类型转型为另一种</li>\n</ul>\n<h3 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h3><h4 id=\"变量集合\"><a href=\"#变量集合\" class=\"headerlink\" title=\"变量集合\"></a>变量集合</h4><p>默认情况下，每个 tf.Variable 都放置在以下两个集合中</p>\n<ul>\n<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code> - 可以在多台设备间共享的变量，</li>\n<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code> - TensorFlow 将计算其梯度的变量</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 如果不希望变量可训练，可以将其添加到 tf.GraphKeys.LOCAL_VARIABLES 集合中</span><br><span class=\"line\">my_local = tf.get_variable(&quot;my_local&quot;, shape=(),collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br><span class=\"line\"># 或</span><br><span class=\"line\">my_non_trainable = tf.get_variable(&quot;my_non_trainable&quot;,</span><br><span class=\"line\">                                   shape=(),</span><br><span class=\"line\">                                   trainable=False)</span><br></pre></td></tr></table></figure>\n<p>可以定义自己的集合</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 将名为 my_local 的现有变量添加到名为 my_collection_name 的集合中</span><br><span class=\"line\">tf.add_to_collection(&quot;my_collection_name&quot;, my_local)</span><br><span class=\"line\"># 获取某个集合中的所有变量（或其他对象）的列表</span><br><span class=\"line\">tf.get_collection(&quot;my_collection_name&quot;)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化变量\"><a href=\"#初始化变量\" class=\"headerlink\" title=\"初始化变量\"></a>初始化变量</h4><blockquote>\n<p>初始化器: tf.initializers（constant、zeros）</p>\n</blockquote>\n<p>要在训练开始前一次性初始化所有可训练变量，请调用<code>tf.global_variables_initializer()</code>。默认情况下，<code>tf.global_variables_initializer</code>不会指定变量的初始化顺序</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">v = tf.get_variable(&quot;v&quot;, shape=(), initializer=tf.zeros_initializer())</span><br><span class=\"line\">w = tf.get_variable(&quot;w&quot;, initializer=v.initialized_value() + 1)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h4><p>要为变量赋值，请使用 assign、assign_add 方法以及 tf.Variable 类中的友元。</p>\n<p>共享变量两种方式：</p>\n<ul>\n<li>显式传递 <code>tf.Variable</code> 对象</li>\n<li>在 <code>tf.variable_scope</code> 对象内隐式封装 tf.Variable 对象。</li>\n</ul>\n<p>变量作用域允许您在调用隐式创建和使用变量的函数时控制变量重用。作用域还允许您以分层和可理解的方式命名变量。</p>\n<h3 id=\"图和会话\"><a href=\"#图和会话\" class=\"headerlink\" title=\"图和会话\"></a>图和会话</h3><h4 id=\"tf-Graph\"><a href=\"#tf-Graph\" class=\"headerlink\" title=\"tf.Graph\"></a>tf.Graph</h4><ul>\n<li>图结构：图的节点和边缘，表示各个操作组合在一起的方式，但不规定它们的使用方式。</li>\n<li>图集合：TensorFlow 提供了一种在<code>tf.Graph</code>中存储元数据集合的通用机制。<code>tf.add_to_collection</code> 函数允许您将对象列表与一个键关联（其中 tf.GraphKeys 定义了部分标准键），<code>tf.get_collection</code> 允许您查询与某个键关联的所有对象。</li>\n</ul>\n<p>大多数 TensorFlow 程序都以数据流图构建阶段开始。在此阶段，您会调用 TensorFlow API 函数，这些函数可构建新的 <code>tf.Operation（节点）</code>和 <code>tf.Tensor（边缘</code> 对象并将它们添加到 tf.Graph 实例中。</p>\n<p>类张量类型:</p>\n<ul>\n<li>tf.Tensor</li>\n<li>tf.Variable</li>\n<li>numpy.ndarray</li>\n<li>list（以及类似于张量的对象的列表）</li>\n<li>标量 Python 类型：bool、float、int、str</li>\n</ul>\n<blockquote>\n<p>注意：在 TensorFlow API 中调用大多数函数时，只是将操作和张量添加到默认图中，并不会执行实际的计算。您应编写这些函数，直到拥有表示整个计算（例如执行梯度下降法的一步）的 tf.Tensor 或 tf.Operation，然后将该对象传递给 tf.Session 以执行计算。</p>\n</blockquote>\n<h4 id=\"tf-Sessoin\"><a href=\"#tf-Sessoin\" class=\"headerlink\" title=\"tf.Sessoin\"></a>tf.Sessoin</h4><p><code>tf.Session</code> 拥有物理资源（例如 GPU 和网络连接），因此通常（在 with 代码块中）用作上下文管理器，并在您退出代码块时自动关闭会话</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Create a default in-process session.</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">  <span class=\"comment\"># ...</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Create a remote session.</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session(<span class=\"string\">&quot;grpc://example.org:2222&quot;</span>):</span><br><span class=\"line\">  <span class=\"comment\"># ...</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tf.Session.init <ul>\n<li>target： 如果将此参数留空（默认设置），会话将仅使用本地机器中的设备。也可以指定 grpc:&#x2F;&#x2F; 网址，以便指定 TensorFlow 服务器的地址，这使得会话可以访问该服务器控制的机器上的所有设备。</li>\n<li>graph：默认情况下，新的 tf.Session 将绑定到当前的默认图，并且仅能够在当前的默认图中运行操作。如果您在程序中使用了多个图，则可以在构建会话时指定明确的 <code>tf.Graph</code>。</li>\n<li>config：此参数允许您指定一个控制会话行为的<code>tf.ConfigProto</code></li>\n</ul>\n</li>\n<li><code>tf.Session.run</code>：运行 <code>tf.Operation</code> 或评估 <code>tf.Tensor</code> 的主要机制<ul>\n<li><code>tf.Session.run</code> 要求您指定一组 fetch，这些 fetch 可确定返回值，并且可能是 <code>tf.Operation</code>、<code>tf.Tensor</code> 或<code>类张量类型</code></li>\n<li><code>tf.Session.run</code> 也可以选择接受 Feed 字典，该字典是从 <code>tf.Tensor</code> 对象（通常是 tf.placeholder 张量）到在执行时会被替换为这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射</li>\n<li><code>tf.Session.run</code> 也接受可选的 options 参数（允许您指定与调用有关的选项）和可选的 run_metadata 参数（允许您收集与执行有关的元数据）</li>\n</ul>\n</li>\n</ul>\n","cover":false,"excerpt":"","more":"<h3 id=\"编程模型\"><a href=\"#编程模型\" class=\"headerlink\" title=\"编程模型\"></a>编程模型</h3><blockquote>\n<p>TensorFlow的数据流图是由<code>节点（Node） </code>和<code>边（edge）</code>组成的<code>有向无环图（directed acycline graph，DAG）</code>。TensorFlow 由 Tensor 和 Flow 两部 分组成，Tensor(张量)代表了数据流图中的边，而 Flow(流动)这个动作就代表了数据流图中节点所做的操作。</p>\n</blockquote>\n<p>计算过程：首先从输入层开始，经过塑形层后，一层一层进行前向传播运算。Relu层（隐藏层）里两个参数，即Wh1和bh1，在输出前使用Rule（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm 和 bsm。用Softmax来计算输出结果中各个类别的概率分布。用交叉熵来度量两个概率分布（源样本的概率分布和输出结果 的概率分布）直接的相似性。然后开始计算梯度，这是需要参数Wh1、bh1、Wsm 和 bsm，以及 交叉熵后的结果。随后进入 SGD 训练，也就是反向传播的过程，从上往下计算每一层的参数， 依次进行更新。也就是说，计算和更新的顺序为 bsm、Wsm、bh1 和 Wh1。</p>\n<p><img src=\"https://wx3.sinaimg.cn/large/007h1WTYly1fup5sc9m7ag30700cgwol.gif\" alt=\"avatar\"></p>\n<h3 id=\"张量\"><a href=\"#张量\" class=\"headerlink\" title=\"张量\"></a>张量</h3><p>TensorFlow用张量这种数据结构来表示所有的数据。一个张量有一个静态类型和动态类型的维数。张量可以在图中的节点之间流通。<br>张量的维数来被描述为<code>阶</code></p>\n<blockquote>\n<p>秩：Tensor维度的数量</p>\n</blockquote>\n<h4 id=\"属性\"><a href=\"#属性\" class=\"headerlink\" title=\"属性\"></a>属性</h4><ul>\n<li>数据类型（例如 float32、int32 或 string）</li>\n<li>形状：即张量的维数和每个维度的大小</li>\n</ul>\n<h4 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h4><ul>\n<li>tf.rank：返回张量的秩</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># shape of tensor &#x27;t&#x27; is [2, 2, 3]</span></span><br><span class=\"line\">t = tf.constant([[[<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]], [[<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]]])</span><br><span class=\"line\">tf.rank(t)  <span class=\"comment\"># 3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tf.shape：返回某个张量的形状</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">t = tf.constant([[[<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]], [[<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">4</span>, <span class=\"number\">4</span>]]])</span><br><span class=\"line\">tf.shape(t)  <span class=\"comment\"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>tf.reshape：重构张量</li>\n<li>tf.cast：将 tf.Tensor 从一种数据类型转型为另一种</li>\n</ul>\n<h3 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h3><h4 id=\"变量集合\"><a href=\"#变量集合\" class=\"headerlink\" title=\"变量集合\"></a>变量集合</h4><p>默认情况下，每个 tf.Variable 都放置在以下两个集合中</p>\n<ul>\n<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code> - 可以在多台设备间共享的变量，</li>\n<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code> - TensorFlow 将计算其梯度的变量</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 如果不希望变量可训练，可以将其添加到 tf.GraphKeys.LOCAL_VARIABLES 集合中</span><br><span class=\"line\">my_local = tf.get_variable(&quot;my_local&quot;, shape=(),collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br><span class=\"line\"># 或</span><br><span class=\"line\">my_non_trainable = tf.get_variable(&quot;my_non_trainable&quot;,</span><br><span class=\"line\">                                   shape=(),</span><br><span class=\"line\">                                   trainable=False)</span><br></pre></td></tr></table></figure>\n<p>可以定义自己的集合</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 将名为 my_local 的现有变量添加到名为 my_collection_name 的集合中</span><br><span class=\"line\">tf.add_to_collection(&quot;my_collection_name&quot;, my_local)</span><br><span class=\"line\"># 获取某个集合中的所有变量（或其他对象）的列表</span><br><span class=\"line\">tf.get_collection(&quot;my_collection_name&quot;)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化变量\"><a href=\"#初始化变量\" class=\"headerlink\" title=\"初始化变量\"></a>初始化变量</h4><blockquote>\n<p>初始化器: tf.initializers（constant、zeros）</p>\n</blockquote>\n<p>要在训练开始前一次性初始化所有可训练变量，请调用<code>tf.global_variables_initializer()</code>。默认情况下，<code>tf.global_variables_initializer</code>不会指定变量的初始化顺序</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">v = tf.get_variable(&quot;v&quot;, shape=(), initializer=tf.zeros_initializer())</span><br><span class=\"line\">w = tf.get_variable(&quot;w&quot;, initializer=v.initialized_value() + 1)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"基本操作\"><a href=\"#基本操作\" class=\"headerlink\" title=\"基本操作\"></a>基本操作</h4><p>要为变量赋值，请使用 assign、assign_add 方法以及 tf.Variable 类中的友元。</p>\n<p>共享变量两种方式：</p>\n<ul>\n<li>显式传递 <code>tf.Variable</code> 对象</li>\n<li>在 <code>tf.variable_scope</code> 对象内隐式封装 tf.Variable 对象。</li>\n</ul>\n<p>变量作用域允许您在调用隐式创建和使用变量的函数时控制变量重用。作用域还允许您以分层和可理解的方式命名变量。</p>\n<h3 id=\"图和会话\"><a href=\"#图和会话\" class=\"headerlink\" title=\"图和会话\"></a>图和会话</h3><h4 id=\"tf-Graph\"><a href=\"#tf-Graph\" class=\"headerlink\" title=\"tf.Graph\"></a>tf.Graph</h4><ul>\n<li>图结构：图的节点和边缘，表示各个操作组合在一起的方式，但不规定它们的使用方式。</li>\n<li>图集合：TensorFlow 提供了一种在<code>tf.Graph</code>中存储元数据集合的通用机制。<code>tf.add_to_collection</code> 函数允许您将对象列表与一个键关联（其中 tf.GraphKeys 定义了部分标准键），<code>tf.get_collection</code> 允许您查询与某个键关联的所有对象。</li>\n</ul>\n<p>大多数 TensorFlow 程序都以数据流图构建阶段开始。在此阶段，您会调用 TensorFlow API 函数，这些函数可构建新的 <code>tf.Operation（节点）</code>和 <code>tf.Tensor（边缘</code> 对象并将它们添加到 tf.Graph 实例中。</p>\n<p>类张量类型:</p>\n<ul>\n<li>tf.Tensor</li>\n<li>tf.Variable</li>\n<li>numpy.ndarray</li>\n<li>list（以及类似于张量的对象的列表）</li>\n<li>标量 Python 类型：bool、float、int、str</li>\n</ul>\n<blockquote>\n<p>注意：在 TensorFlow API 中调用大多数函数时，只是将操作和张量添加到默认图中，并不会执行实际的计算。您应编写这些函数，直到拥有表示整个计算（例如执行梯度下降法的一步）的 tf.Tensor 或 tf.Operation，然后将该对象传递给 tf.Session 以执行计算。</p>\n</blockquote>\n<h4 id=\"tf-Sessoin\"><a href=\"#tf-Sessoin\" class=\"headerlink\" title=\"tf.Sessoin\"></a>tf.Sessoin</h4><p><code>tf.Session</code> 拥有物理资源（例如 GPU 和网络连接），因此通常（在 with 代码块中）用作上下文管理器，并在您退出代码块时自动关闭会话</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Create a default in-process session.</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">  <span class=\"comment\"># ...</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Create a remote session.</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session(<span class=\"string\">&quot;grpc://example.org:2222&quot;</span>):</span><br><span class=\"line\">  <span class=\"comment\"># ...</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tf.Session.init <ul>\n<li>target： 如果将此参数留空（默认设置），会话将仅使用本地机器中的设备。也可以指定 grpc:&#x2F;&#x2F; 网址，以便指定 TensorFlow 服务器的地址，这使得会话可以访问该服务器控制的机器上的所有设备。</li>\n<li>graph：默认情况下，新的 tf.Session 将绑定到当前的默认图，并且仅能够在当前的默认图中运行操作。如果您在程序中使用了多个图，则可以在构建会话时指定明确的 <code>tf.Graph</code>。</li>\n<li>config：此参数允许您指定一个控制会话行为的<code>tf.ConfigProto</code></li>\n</ul>\n</li>\n<li><code>tf.Session.run</code>：运行 <code>tf.Operation</code> 或评估 <code>tf.Tensor</code> 的主要机制<ul>\n<li><code>tf.Session.run</code> 要求您指定一组 fetch，这些 fetch 可确定返回值，并且可能是 <code>tf.Operation</code>、<code>tf.Tensor</code> 或<code>类张量类型</code></li>\n<li><code>tf.Session.run</code> 也可以选择接受 Feed 字典，该字典是从 <code>tf.Tensor</code> 对象（通常是 tf.placeholder 张量）到在执行时会被替换为这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射</li>\n<li><code>tf.Session.run</code> 也接受可选的 options 参数（允许您指定与调用有关的选项）和可选的 run_metadata 参数（允许您收集与执行有关的元数据）</li>\n</ul>\n</li>\n</ul>\n"},{"title":"Yarn 概述","date":"2017-02-16T12:15:59.000Z","_content":"### 名词介绍\nResourceManager：简称RM，是YARN资源控制框架的中心模块，负责集群中所有的资源的统一管理和分配，它接收来自NM（NodeManager）的汇报，建立AM，并将资源派送给AM（ApplicationMaster）。\nNodeManager：简称NM，NodeManager是ResourceManager在每台机器上的代理，负责容器的管理，并监控他们的资源使用情况（CPU、内存、磁盘及网络等），以及向ResourceManager提供这些资源使用报告。\nApplicationMaster：简称AM，YARN中每个应用都会启动一个AM，负责向RM申请资源，请求NM启动container，并告诉container要做什么事情。\nContainer：资源容器。YARN中所有的应用都是在container之上运行的。AM也是在container上运行的，不过AM的container是RM申请的。\nApache Hadoop YARN（Yet Another Resource Negotiator，另一个资源协调者）：是一种新的Hadoop资源管理器，它是一个通用资源管理系统。\n\n![yarn.png](http://upload-images.jianshu.io/upload_images/1419542-0aad7c3fd732d90a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### Yarn主要架构\n#### ResourceManager（RM）\nResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成： 调度器 （Scheduler）和 应用程序管理器 （Applications Manager，ASM）。\n* 调度器（Scheduler）：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n* 应用程序管理器：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。\n\n####  ApplicationMaster（AM）\n用户提交的每个应用程序均包含一个AM，主要功能包括：\n* 与RM调度器协商以获取资源（以Container表示）\n* 将得到的任务进一步分配给内部的任务\n* 与NM通信以启动/停止任务\n* 监控所有任务运行状态，并在任务失败时重新为任务申请资源以重启任务\n\n#### NodeManager（NM）\nNM是每个节点上的资源和任务管理器。\n* 它定时地向RM汇报本节点的资源使用情况和Container运行状态；\n* 它接受并处理来自AM的Container启动/停止等各种请求。\n\n#### Container\nContainer是YARN中的资源抽象，它封装了某个节点上的多维资源，如CPU、内存、磁盘、网络等。当AM向RM申请资源时，RM向AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。Container是一个动态资源划分单位，是根据应用程序的需求自动生成的。目前，YARN仅支持CPU和内存两种资源。\n\n### YARN工作流程\n![yarn-progress-20170216.png](http://upload-images.jianshu.io/upload_images/1419542-4577d88af297c5f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n1、用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。\n2、ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。\n3、ApplicationMaster首先向ResourceManager注册，这样用户就可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。\n4、ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。\n5、一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。\n6、NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。\n7、各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。\n8、应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。","source":"_posts/Yarn-概述.md","raw":"---\ntitle: Yarn 概述\ndate: 2017-02-16 20:15:59\ntags: [yarn,hadoop]\ncategories: [Big data]\n---\n### 名词介绍\nResourceManager：简称RM，是YARN资源控制框架的中心模块，负责集群中所有的资源的统一管理和分配，它接收来自NM（NodeManager）的汇报，建立AM，并将资源派送给AM（ApplicationMaster）。\nNodeManager：简称NM，NodeManager是ResourceManager在每台机器上的代理，负责容器的管理，并监控他们的资源使用情况（CPU、内存、磁盘及网络等），以及向ResourceManager提供这些资源使用报告。\nApplicationMaster：简称AM，YARN中每个应用都会启动一个AM，负责向RM申请资源，请求NM启动container，并告诉container要做什么事情。\nContainer：资源容器。YARN中所有的应用都是在container之上运行的。AM也是在container上运行的，不过AM的container是RM申请的。\nApache Hadoop YARN（Yet Another Resource Negotiator，另一个资源协调者）：是一种新的Hadoop资源管理器，它是一个通用资源管理系统。\n\n![yarn.png](http://upload-images.jianshu.io/upload_images/1419542-0aad7c3fd732d90a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n### Yarn主要架构\n#### ResourceManager（RM）\nResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成： 调度器 （Scheduler）和 应用程序管理器 （Applications Manager，ASM）。\n* 调度器（Scheduler）：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n* 应用程序管理器：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。\n\n####  ApplicationMaster（AM）\n用户提交的每个应用程序均包含一个AM，主要功能包括：\n* 与RM调度器协商以获取资源（以Container表示）\n* 将得到的任务进一步分配给内部的任务\n* 与NM通信以启动/停止任务\n* 监控所有任务运行状态，并在任务失败时重新为任务申请资源以重启任务\n\n#### NodeManager（NM）\nNM是每个节点上的资源和任务管理器。\n* 它定时地向RM汇报本节点的资源使用情况和Container运行状态；\n* 它接受并处理来自AM的Container启动/停止等各种请求。\n\n#### Container\nContainer是YARN中的资源抽象，它封装了某个节点上的多维资源，如CPU、内存、磁盘、网络等。当AM向RM申请资源时，RM向AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。Container是一个动态资源划分单位，是根据应用程序的需求自动生成的。目前，YARN仅支持CPU和内存两种资源。\n\n### YARN工作流程\n![yarn-progress-20170216.png](http://upload-images.jianshu.io/upload_images/1419542-4577d88af297c5f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n1、用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。\n2、ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。\n3、ApplicationMaster首先向ResourceManager注册，这样用户就可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。\n4、ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。\n5、一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。\n6、NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。\n7、各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。\n8、应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。","slug":"Yarn-概述","published":1,"updated":"2024-04-07T07:42:55.403Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lsg002robns0rjq4r03","content":"<h3 id=\"名词介绍\"><a href=\"#名词介绍\" class=\"headerlink\" title=\"名词介绍\"></a>名词介绍</h3><p>ResourceManager：简称RM，是YARN资源控制框架的中心模块，负责集群中所有的资源的统一管理和分配，它接收来自NM（NodeManager）的汇报，建立AM，并将资源派送给AM（ApplicationMaster）。<br>NodeManager：简称NM，NodeManager是ResourceManager在每台机器上的代理，负责容器的管理，并监控他们的资源使用情况（CPU、内存、磁盘及网络等），以及向ResourceManager提供这些资源使用报告。<br>ApplicationMaster：简称AM，YARN中每个应用都会启动一个AM，负责向RM申请资源，请求NM启动container，并告诉container要做什么事情。<br>Container：资源容器。YARN中所有的应用都是在container之上运行的。AM也是在container上运行的，不过AM的container是RM申请的。<br>Apache Hadoop YARN（Yet Another Resource Negotiator，另一个资源协调者）：是一种新的Hadoop资源管理器，它是一个通用资源管理系统。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0aad7c3fd732d90a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"yarn.png\"></p>\n<h3 id=\"Yarn主要架构\"><a href=\"#Yarn主要架构\" class=\"headerlink\" title=\"Yarn主要架构\"></a>Yarn主要架构</h3><h4 id=\"ResourceManager（RM）\"><a href=\"#ResourceManager（RM）\" class=\"headerlink\" title=\"ResourceManager（RM）\"></a>ResourceManager（RM）</h4><p>ResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成： 调度器 （Scheduler）和 应用程序管理器 （Applications Manager，ASM）。</p>\n<ul>\n<li>调度器（Scheduler）：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。</li>\n<li>应用程序管理器：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。</li>\n</ul>\n<h4 id=\"ApplicationMaster（AM）\"><a href=\"#ApplicationMaster（AM）\" class=\"headerlink\" title=\"ApplicationMaster（AM）\"></a>ApplicationMaster（AM）</h4><p>用户提交的每个应用程序均包含一个AM，主要功能包括：</p>\n<ul>\n<li>与RM调度器协商以获取资源（以Container表示）</li>\n<li>将得到的任务进一步分配给内部的任务</li>\n<li>与NM通信以启动&#x2F;停止任务</li>\n<li>监控所有任务运行状态，并在任务失败时重新为任务申请资源以重启任务</li>\n</ul>\n<h4 id=\"NodeManager（NM）\"><a href=\"#NodeManager（NM）\" class=\"headerlink\" title=\"NodeManager（NM）\"></a>NodeManager（NM）</h4><p>NM是每个节点上的资源和任务管理器。</p>\n<ul>\n<li>它定时地向RM汇报本节点的资源使用情况和Container运行状态；</li>\n<li>它接受并处理来自AM的Container启动&#x2F;停止等各种请求。</li>\n</ul>\n<h4 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h4><p>Container是YARN中的资源抽象，它封装了某个节点上的多维资源，如CPU、内存、磁盘、网络等。当AM向RM申请资源时，RM向AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。Container是一个动态资源划分单位，是根据应用程序的需求自动生成的。目前，YARN仅支持CPU和内存两种资源。</p>\n<h3 id=\"YARN工作流程\"><a href=\"#YARN工作流程\" class=\"headerlink\" title=\"YARN工作流程\"></a>YARN工作流程</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-4577d88af297c5f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"yarn-progress-20170216.png\"></p>\n<p>1、用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。<br>2、ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。<br>3、ApplicationMaster首先向ResourceManager注册，这样用户就可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。<br>4、ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。<br>5、一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。<br>6、NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。<br>7、各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。<br>8、应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</p>\n","cover":false,"excerpt":"","more":"<h3 id=\"名词介绍\"><a href=\"#名词介绍\" class=\"headerlink\" title=\"名词介绍\"></a>名词介绍</h3><p>ResourceManager：简称RM，是YARN资源控制框架的中心模块，负责集群中所有的资源的统一管理和分配，它接收来自NM（NodeManager）的汇报，建立AM，并将资源派送给AM（ApplicationMaster）。<br>NodeManager：简称NM，NodeManager是ResourceManager在每台机器上的代理，负责容器的管理，并监控他们的资源使用情况（CPU、内存、磁盘及网络等），以及向ResourceManager提供这些资源使用报告。<br>ApplicationMaster：简称AM，YARN中每个应用都会启动一个AM，负责向RM申请资源，请求NM启动container，并告诉container要做什么事情。<br>Container：资源容器。YARN中所有的应用都是在container之上运行的。AM也是在container上运行的，不过AM的container是RM申请的。<br>Apache Hadoop YARN（Yet Another Resource Negotiator，另一个资源协调者）：是一种新的Hadoop资源管理器，它是一个通用资源管理系统。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0aad7c3fd732d90a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"yarn.png\"></p>\n<h3 id=\"Yarn主要架构\"><a href=\"#Yarn主要架构\" class=\"headerlink\" title=\"Yarn主要架构\"></a>Yarn主要架构</h3><h4 id=\"ResourceManager（RM）\"><a href=\"#ResourceManager（RM）\" class=\"headerlink\" title=\"ResourceManager（RM）\"></a>ResourceManager（RM）</h4><p>ResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成： 调度器 （Scheduler）和 应用程序管理器 （Applications Manager，ASM）。</p>\n<ul>\n<li>调度器（Scheduler）：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。</li>\n<li>应用程序管理器：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。</li>\n</ul>\n<h4 id=\"ApplicationMaster（AM）\"><a href=\"#ApplicationMaster（AM）\" class=\"headerlink\" title=\"ApplicationMaster（AM）\"></a>ApplicationMaster（AM）</h4><p>用户提交的每个应用程序均包含一个AM，主要功能包括：</p>\n<ul>\n<li>与RM调度器协商以获取资源（以Container表示）</li>\n<li>将得到的任务进一步分配给内部的任务</li>\n<li>与NM通信以启动&#x2F;停止任务</li>\n<li>监控所有任务运行状态，并在任务失败时重新为任务申请资源以重启任务</li>\n</ul>\n<h4 id=\"NodeManager（NM）\"><a href=\"#NodeManager（NM）\" class=\"headerlink\" title=\"NodeManager（NM）\"></a>NodeManager（NM）</h4><p>NM是每个节点上的资源和任务管理器。</p>\n<ul>\n<li>它定时地向RM汇报本节点的资源使用情况和Container运行状态；</li>\n<li>它接受并处理来自AM的Container启动&#x2F;停止等各种请求。</li>\n</ul>\n<h4 id=\"Container\"><a href=\"#Container\" class=\"headerlink\" title=\"Container\"></a>Container</h4><p>Container是YARN中的资源抽象，它封装了某个节点上的多维资源，如CPU、内存、磁盘、网络等。当AM向RM申请资源时，RM向AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。Container是一个动态资源划分单位，是根据应用程序的需求自动生成的。目前，YARN仅支持CPU和内存两种资源。</p>\n<h3 id=\"YARN工作流程\"><a href=\"#YARN工作流程\" class=\"headerlink\" title=\"YARN工作流程\"></a>YARN工作流程</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-4577d88af297c5f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"yarn-progress-20170216.png\"></p>\n<p>1、用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。<br>2、ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。<br>3、ApplicationMaster首先向ResourceManager注册，这样用户就可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。<br>4、ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。<br>5、一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。<br>6、NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。<br>7、各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。<br>8、应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</p>\n"},{"title":"go-ethereum-简单搭建私有链","date":"2017-12-05T12:34:24.000Z","_content":"\n### Geth\n> https://ethereum.github.io/go-ethereum/install/\n\n#### 安装 geth：\n访问[https://geth.ethereum.org/downloads/](https://geth.ethereum.org/downloads/)，下载Geth for macOS。\n![F674D81F-4D30-4C90-9D2C-5888F42F688C.png](http://upload-images.jianshu.io/upload_images/1419542-1cd247330ce95842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)\n给geth做一个软连接到/usr/local/bin目录下，然后在命令行输入：geth version  显示如下边上安装成功\n![image.png](http://upload-images.jianshu.io/upload_images/1419542-d6d5d014aeb2a3bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400)\n使用homebrew安装：\n```\nbrew tap ethereum/ethereum\nbrew install ethereum\n```\n\n#### Geth工具介绍：\n* Geth工具是Go Ethereum, 是以太坊的官方客户端（Go语言实现）\n* Geth的命令行中包含了大多数的以太坊的命令，包括账户新建，账户之间的以太币转移，挖矿，获取余额，部署以太坊合约等\n\n### 配置私链节点\n新建文件夹，命名随意，在此文件夹下创建genesis.json文件和data文件夹\ngenesis.json内容如下\n```\n{\n  \"config\": {\n        \"chainId\": 0,\n        \"homesteadBlock\": 0,\n        \"eip155Block\": 0,\n        \"eip158Block\": 0\n    },\n  \"alloc\"      : {},\n  \"coinbase\"   : \"0x0000000000000000000000000000000000000000\",\n  \"difficulty\" : \"0x20000\",\n  \"extraData\"  : \"\",\n  \"gasLimit\"   : \"0x2fefd8\",\n  \"nonce\"      : \"0x0000000000000042\",\n  \"mixhash\"    : \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"timestamp\"  : \"0x00\"\n}\n```\n![创世块.png](http://upload-images.jianshu.io/upload_images/1419542-6403c3c435533892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### 初始化：\n在命令行下进入刚才创建的文件夹，输入如下命令：\n`geth --datadir ./data/00 init genesis.json`\n各参数代表的含义如下：\n* init 表示初始化区块，后面跟着创世块的配置文件genesis.json\n* datadir 数据存放的位置\n```\nprivatechain\n├── data\n│   ├── geth\n│   │   └── chaindata\n│   │       ├── 000002.log\n│   │       ├── CURRENT\n│   │       ├── LOCK\n│   │       ├── LOG\n│   │       └── MANIFEST-000003\n│   └── keystore\n└── genesis.json\n```\n\n\n#### 启动节点：\n`geth --datadir ./data/00 --networkid 15 console`\n各参数代表的含义如下：\n* networkid 设置当前区块链的网络ID，用于区分不同的网络，1表示公链\n* console 表示启动命令行模式，可以在Geth中执行命令\n![启动节点.png](http://upload-images.jianshu.io/upload_images/1419542-c1ac395ce5bdd206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n#### 控制台\n* 创建账号：`personal.newAccount()`或者 `personal.newAccount(\"123456\")`\n* 查看节点信息:`admin.nodeInfo`\n* 挖矿\n  *  开始挖矿 `miner.start()`\n  * 停止挖矿 `miner.stop()`\n* 查看账号：`eth.accounts`\n* 查看账号余额：`eth.getBalance(\"账号名称（eth.accounts[0]）\")`\n\n","source":"_posts/go-ethereum-简单搭建私有链.md","raw":"---\ntitle: go-ethereum-简单搭建私有链\ndate: 2017-12-05 20:34:24\ntags: [以太坊,区块链]\ncategories: [区块链]\n---\n\n### Geth\n> https://ethereum.github.io/go-ethereum/install/\n\n#### 安装 geth：\n访问[https://geth.ethereum.org/downloads/](https://geth.ethereum.org/downloads/)，下载Geth for macOS。\n![F674D81F-4D30-4C90-9D2C-5888F42F688C.png](http://upload-images.jianshu.io/upload_images/1419542-1cd247330ce95842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)\n给geth做一个软连接到/usr/local/bin目录下，然后在命令行输入：geth version  显示如下边上安装成功\n![image.png](http://upload-images.jianshu.io/upload_images/1419542-d6d5d014aeb2a3bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400)\n使用homebrew安装：\n```\nbrew tap ethereum/ethereum\nbrew install ethereum\n```\n\n#### Geth工具介绍：\n* Geth工具是Go Ethereum, 是以太坊的官方客户端（Go语言实现）\n* Geth的命令行中包含了大多数的以太坊的命令，包括账户新建，账户之间的以太币转移，挖矿，获取余额，部署以太坊合约等\n\n### 配置私链节点\n新建文件夹，命名随意，在此文件夹下创建genesis.json文件和data文件夹\ngenesis.json内容如下\n```\n{\n  \"config\": {\n        \"chainId\": 0,\n        \"homesteadBlock\": 0,\n        \"eip155Block\": 0,\n        \"eip158Block\": 0\n    },\n  \"alloc\"      : {},\n  \"coinbase\"   : \"0x0000000000000000000000000000000000000000\",\n  \"difficulty\" : \"0x20000\",\n  \"extraData\"  : \"\",\n  \"gasLimit\"   : \"0x2fefd8\",\n  \"nonce\"      : \"0x0000000000000042\",\n  \"mixhash\"    : \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"timestamp\"  : \"0x00\"\n}\n```\n![创世块.png](http://upload-images.jianshu.io/upload_images/1419542-6403c3c435533892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### 初始化：\n在命令行下进入刚才创建的文件夹，输入如下命令：\n`geth --datadir ./data/00 init genesis.json`\n各参数代表的含义如下：\n* init 表示初始化区块，后面跟着创世块的配置文件genesis.json\n* datadir 数据存放的位置\n```\nprivatechain\n├── data\n│   ├── geth\n│   │   └── chaindata\n│   │       ├── 000002.log\n│   │       ├── CURRENT\n│   │       ├── LOCK\n│   │       ├── LOG\n│   │       └── MANIFEST-000003\n│   └── keystore\n└── genesis.json\n```\n\n\n#### 启动节点：\n`geth --datadir ./data/00 --networkid 15 console`\n各参数代表的含义如下：\n* networkid 设置当前区块链的网络ID，用于区分不同的网络，1表示公链\n* console 表示启动命令行模式，可以在Geth中执行命令\n![启动节点.png](http://upload-images.jianshu.io/upload_images/1419542-c1ac395ce5bdd206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n#### 控制台\n* 创建账号：`personal.newAccount()`或者 `personal.newAccount(\"123456\")`\n* 查看节点信息:`admin.nodeInfo`\n* 挖矿\n  *  开始挖矿 `miner.start()`\n  * 停止挖矿 `miner.stop()`\n* 查看账号：`eth.accounts`\n* 查看账号余额：`eth.getBalance(\"账号名称（eth.accounts[0]）\")`\n\n","slug":"go-ethereum-简单搭建私有链","published":1,"updated":"2024-04-07T07:42:55.404Z","comments":1,"layout":"post","photos":[],"_id":"clupb7lsh002tobns7jzt6h5b","content":"<h3 id=\"Geth\"><a href=\"#Geth\" class=\"headerlink\" title=\"Geth\"></a>Geth</h3><blockquote>\n<p><a href=\"https://ethereum.github.io/go-ethereum/install/\">https://ethereum.github.io/go-ethereum/install/</a></p>\n</blockquote>\n<h4 id=\"安装-geth：\"><a href=\"#安装-geth：\" class=\"headerlink\" title=\"安装 geth：\"></a>安装 geth：</h4><p>访问<a href=\"https://geth.ethereum.org/downloads/\">https://geth.ethereum.org/downloads/</a>，下载Geth for macOS。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1cd247330ce95842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600\" alt=\"F674D81F-4D30-4C90-9D2C-5888F42F688C.png\"><br>给geth做一个软连接到&#x2F;usr&#x2F;local&#x2F;bin目录下，然后在命令行输入：geth version  显示如下边上安装成功<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d6d5d014aeb2a3bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400\" alt=\"image.png\"><br>使用homebrew安装：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">brew tap ethereum/ethereum</span><br><span class=\"line\">brew install ethereum</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Geth工具介绍：\"><a href=\"#Geth工具介绍：\" class=\"headerlink\" title=\"Geth工具介绍：\"></a>Geth工具介绍：</h4><ul>\n<li>Geth工具是Go Ethereum, 是以太坊的官方客户端（Go语言实现）</li>\n<li>Geth的命令行中包含了大多数的以太坊的命令，包括账户新建，账户之间的以太币转移，挖矿，获取余额，部署以太坊合约等</li>\n</ul>\n<h3 id=\"配置私链节点\"><a href=\"#配置私链节点\" class=\"headerlink\" title=\"配置私链节点\"></a>配置私链节点</h3><p>新建文件夹，命名随意，在此文件夹下创建genesis.json文件和data文件夹<br>genesis.json内容如下</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;config&quot;: &#123;</span><br><span class=\"line\">        &quot;chainId&quot;: 0,</span><br><span class=\"line\">        &quot;homesteadBlock&quot;: 0,</span><br><span class=\"line\">        &quot;eip155Block&quot;: 0,</span><br><span class=\"line\">        &quot;eip158Block&quot;: 0</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">  &quot;alloc&quot;      : &#123;&#125;,</span><br><span class=\"line\">  &quot;coinbase&quot;   : &quot;0x0000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;difficulty&quot; : &quot;0x20000&quot;,</span><br><span class=\"line\">  &quot;extraData&quot;  : &quot;&quot;,</span><br><span class=\"line\">  &quot;gasLimit&quot;   : &quot;0x2fefd8&quot;,</span><br><span class=\"line\">  &quot;nonce&quot;      : &quot;0x0000000000000042&quot;,</span><br><span class=\"line\">  &quot;mixhash&quot;    : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;timestamp&quot;  : &quot;0x00&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-6403c3c435533892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"创世块.png\"></p>\n<h4 id=\"初始化：\"><a href=\"#初始化：\" class=\"headerlink\" title=\"初始化：\"></a>初始化：</h4><p>在命令行下进入刚才创建的文件夹，输入如下命令：<br><code>geth --datadir ./data/00 init genesis.json</code><br>各参数代表的含义如下：</p>\n<ul>\n<li>init 表示初始化区块，后面跟着创世块的配置文件genesis.json</li>\n<li>datadir 数据存放的位置<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">privatechain</span><br><span class=\"line\">├── data</span><br><span class=\"line\">│   ├── geth</span><br><span class=\"line\">│   │   └── chaindata</span><br><span class=\"line\">│   │       ├── 000002.log</span><br><span class=\"line\">│   │       ├── CURRENT</span><br><span class=\"line\">│   │       ├── LOCK</span><br><span class=\"line\">│   │       ├── LOG</span><br><span class=\"line\">│   │       └── MANIFEST-000003</span><br><span class=\"line\">│   └── keystore</span><br><span class=\"line\">└── genesis.json</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"启动节点：\"><a href=\"#启动节点：\" class=\"headerlink\" title=\"启动节点：\"></a>启动节点：</h4><p><code>geth --datadir ./data/00 --networkid 15 console</code><br>各参数代表的含义如下：</p>\n<ul>\n<li>networkid 设置当前区块链的网络ID，用于区分不同的网络，1表示公链</li>\n<li>console 表示启动命令行模式，可以在Geth中执行命令<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-c1ac395ce5bdd206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"启动节点.png\"></li>\n</ul>\n<h4 id=\"控制台\"><a href=\"#控制台\" class=\"headerlink\" title=\"控制台\"></a>控制台</h4><ul>\n<li>创建账号：<code>personal.newAccount()</code>或者 <code>personal.newAccount(&quot;123456&quot;)</code></li>\n<li>查看节点信息:<code>admin.nodeInfo</code></li>\n<li>挖矿<ul>\n<li>开始挖矿 <code>miner.start()</code></li>\n<li>停止挖矿 <code>miner.stop()</code></li>\n</ul>\n</li>\n<li>查看账号：<code>eth.accounts</code></li>\n<li>查看账号余额：<code>eth.getBalance(&quot;账号名称（eth.accounts[0]）&quot;)</code></li>\n</ul>\n","cover":false,"excerpt":"","more":"<h3 id=\"Geth\"><a href=\"#Geth\" class=\"headerlink\" title=\"Geth\"></a>Geth</h3><blockquote>\n<p><a href=\"https://ethereum.github.io/go-ethereum/install/\">https://ethereum.github.io/go-ethereum/install/</a></p>\n</blockquote>\n<h4 id=\"安装-geth：\"><a href=\"#安装-geth：\" class=\"headerlink\" title=\"安装 geth：\"></a>安装 geth：</h4><p>访问<a href=\"https://geth.ethereum.org/downloads/\">https://geth.ethereum.org/downloads/</a>，下载Geth for macOS。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1cd247330ce95842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600\" alt=\"F674D81F-4D30-4C90-9D2C-5888F42F688C.png\"><br>给geth做一个软连接到&#x2F;usr&#x2F;local&#x2F;bin目录下，然后在命令行输入：geth version  显示如下边上安装成功<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d6d5d014aeb2a3bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/400\" alt=\"image.png\"><br>使用homebrew安装：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">brew tap ethereum/ethereum</span><br><span class=\"line\">brew install ethereum</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Geth工具介绍：\"><a href=\"#Geth工具介绍：\" class=\"headerlink\" title=\"Geth工具介绍：\"></a>Geth工具介绍：</h4><ul>\n<li>Geth工具是Go Ethereum, 是以太坊的官方客户端（Go语言实现）</li>\n<li>Geth的命令行中包含了大多数的以太坊的命令，包括账户新建，账户之间的以太币转移，挖矿，获取余额，部署以太坊合约等</li>\n</ul>\n<h3 id=\"配置私链节点\"><a href=\"#配置私链节点\" class=\"headerlink\" title=\"配置私链节点\"></a>配置私链节点</h3><p>新建文件夹，命名随意，在此文件夹下创建genesis.json文件和data文件夹<br>genesis.json内容如下</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;config&quot;: &#123;</span><br><span class=\"line\">        &quot;chainId&quot;: 0,</span><br><span class=\"line\">        &quot;homesteadBlock&quot;: 0,</span><br><span class=\"line\">        &quot;eip155Block&quot;: 0,</span><br><span class=\"line\">        &quot;eip158Block&quot;: 0</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">  &quot;alloc&quot;      : &#123;&#125;,</span><br><span class=\"line\">  &quot;coinbase&quot;   : &quot;0x0000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;difficulty&quot; : &quot;0x20000&quot;,</span><br><span class=\"line\">  &quot;extraData&quot;  : &quot;&quot;,</span><br><span class=\"line\">  &quot;gasLimit&quot;   : &quot;0x2fefd8&quot;,</span><br><span class=\"line\">  &quot;nonce&quot;      : &quot;0x0000000000000042&quot;,</span><br><span class=\"line\">  &quot;mixhash&quot;    : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</span><br><span class=\"line\">  &quot;timestamp&quot;  : &quot;0x00&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-6403c3c435533892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"创世块.png\"></p>\n<h4 id=\"初始化：\"><a href=\"#初始化：\" class=\"headerlink\" title=\"初始化：\"></a>初始化：</h4><p>在命令行下进入刚才创建的文件夹，输入如下命令：<br><code>geth --datadir ./data/00 init genesis.json</code><br>各参数代表的含义如下：</p>\n<ul>\n<li>init 表示初始化区块，后面跟着创世块的配置文件genesis.json</li>\n<li>datadir 数据存放的位置<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">privatechain</span><br><span class=\"line\">├── data</span><br><span class=\"line\">│   ├── geth</span><br><span class=\"line\">│   │   └── chaindata</span><br><span class=\"line\">│   │       ├── 000002.log</span><br><span class=\"line\">│   │       ├── CURRENT</span><br><span class=\"line\">│   │       ├── LOCK</span><br><span class=\"line\">│   │       ├── LOG</span><br><span class=\"line\">│   │       └── MANIFEST-000003</span><br><span class=\"line\">│   └── keystore</span><br><span class=\"line\">└── genesis.json</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"启动节点：\"><a href=\"#启动节点：\" class=\"headerlink\" title=\"启动节点：\"></a>启动节点：</h4><p><code>geth --datadir ./data/00 --networkid 15 console</code><br>各参数代表的含义如下：</p>\n<ul>\n<li>networkid 设置当前区块链的网络ID，用于区分不同的网络，1表示公链</li>\n<li>console 表示启动命令行模式，可以在Geth中执行命令<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-c1ac395ce5bdd206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"启动节点.png\"></li>\n</ul>\n<h4 id=\"控制台\"><a href=\"#控制台\" class=\"headerlink\" title=\"控制台\"></a>控制台</h4><ul>\n<li>创建账号：<code>personal.newAccount()</code>或者 <code>personal.newAccount(&quot;123456&quot;)</code></li>\n<li>查看节点信息:<code>admin.nodeInfo</code></li>\n<li>挖矿<ul>\n<li>开始挖矿 <code>miner.start()</code></li>\n<li>停止挖矿 <code>miner.stop()</code></li>\n</ul>\n</li>\n<li>查看账号：<code>eth.accounts</code></li>\n<li>查看账号余额：<code>eth.getBalance(&quot;账号名称（eth.accounts[0]）&quot;)</code></li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"clupb7lre0001obnsfxtzhvki","category_id":"clupb7lrk0004obns4ah5fmjd","_id":"clupb7lru000gobns3lq41jqo"},{"post_id":"clupb7lri0003obnsbsiv4x2c","category_id":"clupb7lrk0004obns4ah5fmjd","_id":"clupb7lrw000lobnsbwom48pl"},{"post_id":"clupb7lrn0007obns1r2bgsz0","category_id":"clupb7lru000fobns46xb51vt","_id":"clupb7lrz000robns6rsb6isp"},{"post_id":"clupb7lrx000oobns97wy65vk","category_id":"clupb7lrk0004obns4ah5fmjd","_id":"clupb7ls1000vobnsfcncduk6"},{"post_id":"clupb7lro0008obns94uf3cn1","category_id":"clupb7lru000fobns46xb51vt","_id":"clupb7ls2000zobnsg13s2gui"},{"post_id":"clupb7lrz000tobns1g99e2jm","category_id":"clupb7lrk0004obns4ah5fmjd","_id":"clupb7ls30012obns2i887b0o"},{"post_id":"clupb7lrp0009obns5xdx727j","category_id":"clupb7lru000fobns46xb51vt","_id":"clupb7ls40017obns62k91skt"},{"post_id":"clupb7ls0000uobns67q65wzh","category_id":"clupb7lrk0004obns4ah5fmjd","_id":"clupb7ls40019obns1me30upu"},{"post_id":"clupb7lrs000dobns5ndb9ou6","category_id":"clupb7ls1000wobnsclhydulh","_id":"clupb7ls5001cobnsf95y6in2"},{"post_id":"clupb7lrv000jobnsgtmf10h7","category_id":"clupb7ls30013obns4n8n8zbr","_id":"clupb7ls5001eobns9msc6kgn"},{"post_id":"clupb7lrw000kobnsbcdc335m","category_id":"clupb7ls30013obns4n8n8zbr","_id":"clupb7ls7001jobns3ylvek3m"},{"post_id":"clupb7lry000pobns0nl3ewpm","category_id":"clupb7ls30013obns4n8n8zbr","_id":"clupb7ls7001mobns8a7i19l3"},{"post_id":"clupb7ls1000yobns6s3l1h0c","category_id":"clupb7ls6001iobnsgepr34cr","_id":"clupb7ls8001qobnsddok2foa"},{"post_id":"clupb7ls20011obnsbh3a3lc3","category_id":"clupb7ls6001iobnsgepr34cr","_id":"clupb7ls9001tobns7etghegw"},{"post_id":"clupb7ls30016obns5vsw7rie","category_id":"clupb7ls6001iobnsgepr34cr","_id":"clupb7ls9001xobns5luraewx"},{"post_id":"clupb7lsf002qobns2evp1gyg","category_id":"clupb7ls6001iobnsgepr34cr","_id":"clupb7lsi002wobnsgco53k2d"},{"post_id":"clupb7lsg002robns0rjq4r03","category_id":"clupb7lsh002uobns79ox7lxb","_id":"clupb7lsi002zobns8a6z308m"},{"post_id":"clupb7lsh002tobns7jzt6h5b","category_id":"clupb7lsi002xobnsgu10c0by","_id":"clupb7lsj0033obns1v9t8jcg"}],"PostTag":[{"post_id":"clupb7lre0001obnsfxtzhvki","tag_id":"clupb7lrm0005obns48pxcanh","_id":"clupb7lrs000cobns46iialrr"},{"post_id":"clupb7lri0003obnsbsiv4x2c","tag_id":"clupb7lrm0005obns48pxcanh","_id":"clupb7lrv000iobns78cd59eo"},{"post_id":"clupb7lrn0007obns1r2bgsz0","tag_id":"clupb7lru000hobns8ruo7kix","_id":"clupb7ls20010obnshmziax13"},{"post_id":"clupb7lrn0007obns1r2bgsz0","tag_id":"clupb7lrx000nobns3tfka5ec","_id":"clupb7ls30014obns5xdx4xlo"},{"post_id":"clupb7lrn0007obns1r2bgsz0","tag_id":"clupb7lrz000sobns0owec6tn","_id":"clupb7ls40018obnshyb9cq4j"},{"post_id":"clupb7lro0008obns94uf3cn1","tag_id":"clupb7lru000hobns8ruo7kix","_id":"clupb7ls6001gobnshtl4dd78"},{"post_id":"clupb7lro0008obns94uf3cn1","tag_id":"clupb7lrx000nobns3tfka5ec","_id":"clupb7ls6001hobns1r490jc1"},{"post_id":"clupb7lro0008obns94uf3cn1","tag_id":"clupb7lrz000sobns0owec6tn","_id":"clupb7ls7001lobns7c76amge"},{"post_id":"clupb7lrp0009obns5xdx727j","tag_id":"clupb7lru000hobns8ruo7kix","_id":"clupb7ls9001sobns8rg8fm2p"},{"post_id":"clupb7lrp0009obns5xdx727j","tag_id":"clupb7ls7001kobns704n3tu5","_id":"clupb7ls9001uobnsf2cb1vsf"},{"post_id":"clupb7lrp0009obns5xdx727j","tag_id":"clupb7ls7001oobns2lpw8f66","_id":"clupb7ls9001wobnsgwmhgs9q"},{"post_id":"clupb7lrs000dobns5ndb9ou6","tag_id":"clupb7ls8001robns0i9pem2e","_id":"clupb7ls9001yobns5p3r1glm"},{"post_id":"clupb7lrt000eobns4hs7edfn","tag_id":"clupb7ls9001vobnsg9hxaz1s","_id":"clupb7lsa0020obns38twdryl"},{"post_id":"clupb7lrv000jobnsgtmf10h7","tag_id":"clupb7ls9001zobnsetgw1lai","_id":"clupb7lsa0022obns7iy77rc8"},{"post_id":"clupb7lrw000kobnsbcdc335m","tag_id":"clupb7ls9001zobnsetgw1lai","_id":"clupb7lsa0024obns1uyj50sm"},{"post_id":"clupb7lrx000oobns97wy65vk","tag_id":"clupb7lsa0023obns596ig73g","_id":"clupb7lsb0027obns03w9guxm"},{"post_id":"clupb7lrx000oobns97wy65vk","tag_id":"clupb7lsa0025obnscdu1ey5w","_id":"clupb7lsb0028obnshyro2dk7"},{"post_id":"clupb7lry000pobns0nl3ewpm","tag_id":"clupb7lsb0026obns3izi5a5e","_id":"clupb7lsc002bobnscayqh1pj"},{"post_id":"clupb7lry000pobns0nl3ewpm","tag_id":"clupb7ls9001zobnsetgw1lai","_id":"clupb7lsc002cobnscyin701j"},{"post_id":"clupb7lrz000tobns1g99e2jm","tag_id":"clupb7lsa0023obns596ig73g","_id":"clupb7lsc002fobnsdq9w2nn4"},{"post_id":"clupb7lrz000tobns1g99e2jm","tag_id":"clupb7lsa0025obnscdu1ey5w","_id":"clupb7lsc002gobnsdhdgfdgh"},{"post_id":"clupb7ls0000uobns67q65wzh","tag_id":"clupb7lsa0023obns596ig73g","_id":"clupb7lsd002jobnseibv7810"},{"post_id":"clupb7ls0000uobns67q65wzh","tag_id":"clupb7lsa0025obnscdu1ey5w","_id":"clupb7lsd002kobns85a76h80"},{"post_id":"clupb7ls1000yobns6s3l1h0c","tag_id":"clupb7lsd002iobnsf6v36g4t","_id":"clupb7lsd002mobns3ikt3yq5"},{"post_id":"clupb7ls20011obnsbh3a3lc3","tag_id":"clupb7lsd002iobnsf6v36g4t","_id":"clupb7lse002oobns3ryv112l"},{"post_id":"clupb7ls30016obns5vsw7rie","tag_id":"clupb7lsd002iobnsf6v36g4t","_id":"clupb7lse002pobnsgv8paft7"},{"post_id":"clupb7lsf002qobns2evp1gyg","tag_id":"clupb7lsd002iobnsf6v36g4t","_id":"clupb7lsh002sobns6vq4bjju"},{"post_id":"clupb7lsg002robns0rjq4r03","tag_id":"clupb7lsi002vobns5nw91icc","_id":"clupb7lsj0031obnsbnvh982h"},{"post_id":"clupb7lsg002robns0rjq4r03","tag_id":"clupb7lsi002yobnserzq5jpm","_id":"clupb7lsj0032obns7ymbhnv7"},{"post_id":"clupb7lsh002tobns7jzt6h5b","tag_id":"clupb7lsj0030obnsatwu0iiv","_id":"clupb7lsj0035obnsgrfiafss"},{"post_id":"clupb7lsh002tobns7jzt6h5b","tag_id":"clupb7lsj0034obns79eqednf","_id":"clupb7lsj0036obns9ju0ek0g"}],"Tag":[{"name":"java-nio","_id":"clupb7lrm0005obns48pxcanh"},{"name":"java","_id":"clupb7lru000hobns8ruo7kix"},{"name":"java基础","_id":"clupb7lrx000nobns3tfka5ec"},{"name":"jvm","_id":"clupb7lrz000sobns0owec6tn"},{"name":"thread","_id":"clupb7ls7001kobns704n3tu5"},{"name":"Java基础","_id":"clupb7ls7001oobns2lpw8f66"},{"name":"kafka","_id":"clupb7ls8001robns0i9pem2e"},{"name":"rpc","_id":"clupb7ls9001vobnsg9hxaz1s"},{"name":"spark","_id":"clupb7ls9001zobnsetgw1lai"},{"name":"spring-boot","_id":"clupb7lsa0023obns596ig73g"},{"name":"spring","_id":"clupb7lsa0025obnscdu1ey5w"},{"name":"sparkstream","_id":"clupb7lsb0026obns3izi5a5e"},{"name":"tensorflow","_id":"clupb7lsd002iobnsf6v36g4t"},{"name":"yarn","_id":"clupb7lsi002vobns5nw91icc"},{"name":"hadoop","_id":"clupb7lsi002yobnserzq5jpm"},{"name":"以太坊","_id":"clupb7lsj0030obnsatwu0iiv"},{"name":"区块链","_id":"clupb7lsj0034obns79eqednf"}]}}