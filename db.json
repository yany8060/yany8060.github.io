{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"source/img/bak/hxh_s.jpg","path":"img/bak/hxh_s.jpg","modified":1,"renderable":0},{"_id":"source/img/work/14853239880359.jpg","path":"img/work/14853239880359.jpg","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-clear-20170212.png","path":"img/work/Buffer-clear-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-process-20170212.jpeg","path":"img/work/Buffer-process-20170212.jpeg","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-get-20170212.png","path":"img/work/Buffer-get-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-flip-20170212.png","path":"img/work/Buffer-flip-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/Buffer-put-20170212.png","path":"img/work/Buffer-put-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/channels-buffers-20170212.png","path":"img/work/channels-buffers-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka-producer-20170213.png.png","path":"img/work/kafka-producer-20170213.png.png","modified":1,"renderable":0},{"_id":"source/img/work/selectors-20170212.png","path":"img/work/selectors-20170212.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka_log_anatomy_20170213.png","path":"img/work/kafka_log_anatomy_20170213.png","modified":1,"renderable":0},{"_id":"source/img/work/sparkRdd_20170207.jpg","path":"img/work/sparkRdd_20170207.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/img/bak/hxh_n.jpg","path":"img/bak/hxh_n.jpg","modified":1,"renderable":0},{"_id":"source/img/work/catalog_20170204.png","path":"img/work/catalog_20170204.png","modified":1,"renderable":0},{"_id":"source/img/work/kafka-tupe-20170213.png","path":"img/work/kafka-tupe-20170213.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/img/work/shuffle-write-no-consolidation_20170206.png","path":"img/work/shuffle-write-no-consolidation_20170206.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"358fdaf656a4c65e3b90cc318c7933d36cbda17a","modified":1483445808000},{"_id":"source/.DS_Store","hash":"3ad84c22c6f46d43a8f2337a2e1e39d05ac27c63","modified":1486987924000},{"_id":"source/favicon.ico","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1485326556000},{"_id":"themes/next/.DS_Store","hash":"0ccf129999265a095c473c0ce95c36e07969042d","modified":1485326809000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1483444391000},{"_id":"themes/next/_config.yml","hash":"19b3477ea0ba5e0ad2ac2214ff0b670d5ee87934","modified":1486368180000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1483444391000},{"_id":"themes/next/gulpfile.coffee","hash":"61ef0606a8134894d7ac796bc8d0fa4ba6a94483","modified":1483444391000},{"_id":"themes/next/package.json","hash":"877cb98025e59015532c4c9a04a33e2af4ad56f9","modified":1483444391000},{"_id":"source/_posts/Kafka-概述.md","hash":"9518204f18087480388aa836c54a7214df682f8d","modified":1486988532000},{"_id":"source/_posts/.DS_Store","hash":"a1cb9e058916e32f4a83cef0e54c6686bf9100a4","modified":1486178172000},{"_id":"source/_posts/Java-NIO-ByteBuffer概述.md","hash":"4ff79b026674ad4d73091d989d8b45ad6b49cafe","modified":1486891948000},{"_id":"source/_posts/Java-NIO简述.md","hash":"800544a653a076e32c809f4f90f0dede85ca3188","modified":1486891866000},{"_id":"source/_posts/Spark-RDD详解.md","hash":"d90ff4e64eae1741a27f5f6bb16a655c1ce042d3","modified":1486468845000},{"_id":"source/_posts/Spark-Shuffle基础.md","hash":"d4894c4694f78d8a61cc9870656b64659a2dbd3d","modified":1486369177000},{"_id":"source/_posts/SparkStream-函数详解-Transformations.md","hash":"b685bc6e6fc6989f31dd25c214fdcde8452645d3","modified":1486344706000},{"_id":"source/_posts/Spring-Boot-入门学习.md","hash":"e46b77e72d48c8a4fd30281d32f46a0d7792a1a0","modified":1486178573000},{"_id":"source/_posts/Spring-boot-MyBatis配置-2.md","hash":"11d8591b81b173172a44242a91f0914790520a7c","modified":1486197730000},{"_id":"source/about/index.md","hash":"73a36f69473d605af41737baaa9e5ed9b4591637","modified":1484012617000},{"_id":"source/_posts/Spring-boot-MyBatis配置-1.md","hash":"e4f928aea74d9cb1eef04132bb940ed83fa9309b","modified":1486197530000},{"_id":"source/categories/index.md","hash":"e871e33c461ed118e39964408971bda4728b6b59","modified":1484012340000},{"_id":"source/img/.DS_Store","hash":"3adb1f232ecea6c163229d3f10bb9bf4ad6f47cd","modified":1486988478000},{"_id":"source/tags/index.md","hash":"2646a7daf99cff3c74c494ce1319abd2600a63da","modified":1484012455000},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1483444391000},{"_id":"themes/next/languages/default.yml","hash":"767470a80dc257e23e14c3a78e8c52a46c9d6209","modified":1483444391000},{"_id":"themes/next/languages/en.yml","hash":"40057d6608e825d06e0864bac4dcd27ed88ada87","modified":1483444391000},{"_id":"themes/next/languages/fr-FR.yml","hash":"9fca01ef917d33ae2ae6bc04561ec6799dff5351","modified":1483444391000},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1483444391000},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1483444391000},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1483444391000},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1483444391000},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1483444391000},{"_id":"themes/next/languages/ru.yml","hash":"257d11e626cbe4b9b78785a764190b9278f95c28","modified":1483444391000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"f6c9fafa0f5f0050cd07ca2cf5e38fbae3e28145","modified":1483444391000},{"_id":"themes/next/languages/zh-hk.yml","hash":"34c84c6d04447a25bd5eac576922a13947c000e2","modified":1483444391000},{"_id":"themes/next/languages/zh-tw.yml","hash":"c97a5c41149de9b17f33439b0ecf0eff6fdae50e","modified":1483444391000},{"_id":"themes/next/layout/_layout.swig","hash":"7a1e4443c3ba1e08c20e64ddbf0b8255d034dab0","modified":1483444391000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1483444391000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1483444391000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1483444391000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1483444391000},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1483444391000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1483444391000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1483444391000},{"_id":"themes/next/source/.DS_Store","hash":"94821b495cd6fb5db2f6be287ff0f2aff261d725","modified":1485326809000},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1483444391000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1483444391000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1483444391000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1483444391000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"source/img/bak/.DS_Store","hash":"6119a8666cdb76c254507eb563698b8f60842831","modified":1485326684000},{"_id":"source/img/bak/hxh_s.jpg","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1485326556000},{"_id":"source/img/work/.DS_Store","hash":"60c0442149a43dd7d48acf95d59a311cd2830cf6","modified":1486891838000},{"_id":"source/img/work/14853239880359.jpg","hash":"8ba6fcca9fc03ee5b99ec646b8d0d1ba61cc10dd","modified":1485323988000},{"_id":"source/img/work/Buffer-clear-20170212.png","hash":"cafb60b7e6a7516dd18dc38c1676f47572d034ff","modified":1486890261000},{"_id":"source/img/work/Buffer-process-20170212.jpeg","hash":"2610f8916944b59c7859aabf4df981007d42c2ef","modified":1486890028000},{"_id":"source/img/work/Buffer-get-20170212.png","hash":"f44111acd754300cc53329926190d5129648c4af","modified":1486890251000},{"_id":"source/img/work/Buffer-flip-20170212.png","hash":"d710f9be2e24d076190ada8280be3b2d419d94e2","modified":1486890232000},{"_id":"source/img/work/Buffer-put-20170212.png","hash":"177eadfa459694390c3d4accffa40e0304755243","modified":1486890210000},{"_id":"source/img/work/channels-buffers-20170212.png","hash":"91767a1f3c322cf4ff1936c24143724195b54fc9","modified":1486887545000},{"_id":"source/img/work/kafka-producer-20170213.png.png","hash":"d418cb89944102d0875ce7d0aaf1068ad616a651","modified":1486986882000},{"_id":"source/img/work/selectors-20170212.png","hash":"d2019fc0847fad5488c199d4f6bb07a3dceba7ce","modified":1486888847000},{"_id":"source/img/work/kafka_log_anatomy_20170213.png","hash":"7d387eec0de1ebfbcd98a3b0c87aa008b2dbe476","modified":1486985921000},{"_id":"source/img/work/sparkRdd_20170207.jpg","hash":"356667fdad339020fb9c94358ee46fe11deec498","modified":1486466462000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1483444391000},{"_id":"themes/next/layout/_macro/post.swig","hash":"39a8efd961ea2c5758ca4231d3cc4108c1856930","modified":1483444391000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1483444391000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"43d8830bb19da4fc7a5773866be19fa066b62645","modified":1483444391000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1483444391000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1483444391000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1483444391000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4eb278f7b1b7dfe1088c2b411778cc6129df82bb","modified":1483444391000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1483444391000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"0242722eaf6294cc8ec2f0da6996acf44c8f17dc","modified":1484049005000},{"_id":"themes/next/layout/_partials/head.swig","hash":"ca56f92e2fa82b03853869f5073ee1a5626a4796","modified":1483444391000},{"_id":"themes/next/layout/_partials/header.swig","hash":"5696ee15fc21eb3a6416902afcf1df454497c552","modified":1483444391000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"39d613e5a9f8389d4ea52d6082502af8e833b9f2","modified":1483444391000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1483444391000},{"_id":"themes/next/layout/_partials/search.swig","hash":"1431719d1dbba3f5ee385eebc46376d1a960b2d5","modified":1483444391000},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1483444391000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1483444391000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1483444391000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"0b91cadecead8e0b5211cc42b085998d94af503a","modified":1483444391000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1483444391000},{"_id":"themes/next/source/images/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1485326858000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1483444391000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1483444391000},{"_id":"themes/next/source/images/avatar.gif","hash":"53c664c7e2569f0e57abce2e357a04c19ce66122","modified":1485326556000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1483444391000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1483444391000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1483444391000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1483444391000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1483444391000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1483444391000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1483444391000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1483444391000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1483444391000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1483444391000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1483444391000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1483444391000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1483444391000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1483444391000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1483444391000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1483444391000},{"_id":"source/img/bak/hxh_n.jpg","hash":"251a9630d76739e2ef3489f3c9d2b4d922278f72","modified":1485326512000},{"_id":"source/img/work/catalog_20170204.png","hash":"af1d55a452c12677971ae42b643000d453e3804f","modified":1486175482000},{"_id":"source/img/work/kafka-tupe-20170213.png","hash":"24d5b6c8a7e0ecd183f6225a8038846aa8e3efaf","modified":1486987814000},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1483444391000},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"636f1181dd5887a70b4a08ca8f655d4e46635792","modified":1483444391000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1483444391000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1483444391000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1483444391000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1483444391000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1483444391000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1483444391000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1483444391000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1483444391000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1483444391000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1483444391000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"394d9fff7951287cc90f52acc2d4cbfd1bae079d","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"417e16a0fbdcb1b87987787bd7f9fa17eefc2d2b","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"b460e27db3dcd4ab40b17d8926a5c4e624f293a9","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1483444391000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"ca843bbd5e275ec5de6b4f875aaad6dbfe76b849","modified":1484049891000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1483444391000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1483444391000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1483444391000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"3f0d6aa424f434e82ea507f740eeff110f996269","modified":1483444391000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"bef514826ebf9eb6e99bb2b0d72285106658a1ec","modified":1483444391000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1483444391000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"96b29f69b8b916b22f62c9959a117b5a968200a5","modified":1483444391000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1483444391000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1483444391000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1483444391000},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1483444391000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1483444391000},{"_id":"themes/next/source/js/src/utils.js","hash":"384e17ff857f073060f5bf8c6e4f4b7353236331","modified":1483444391000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"c1072942459fa0880e8a33a1bd929176b62b4171","modified":1483444391000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1483444391000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1483444391000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1483444391000},{"_id":"source/img/work/shuffle-write-no-consolidation_20170206.png","hash":"6dd2872aa672f77e9b7e762f71f1733934e825cc","modified":1486364644000},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"402da84687753156b6336eeacffbf3eeb0829d34","modified":1484050320000},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1483444391000},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1483444391000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1483444391000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1483444391000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1483444391000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1483444391000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1483444391000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"4303776991ef28f5742ca51c7dffe6f12f0acf34","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"ff9f163bb05c0709577040a875924d36c9ab99d6","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"dcf9fe43b2ef78b923118ba39efedb38760e76b1","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d09280e5b79f3b573edb30f30c7a5f03ac640986","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_full-image.styl","hash":"938d39eedc6e3d33918c1145a5bf1e79991d3fcf","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9b63bd8effc7cf4b96acdea4d73add7df934a222","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"7e899c4c0aa1312666f2534a4700667d1bfd88da","modified":1483444391000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"9ccee9189c910b8a264802d7b2ec305d12dedcd0","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1483444391000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"4eda182cbcc046dbf449aef97c02c230cf80a494","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"fb5b49426dee7f1508500e698d1b3c6b04c8fcce","modified":1483444391000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1483444391000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1483444391000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4b7f81e1006e7acee3d1c840ccba155239f830cc","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"7f1aab694caf603809e33cff82beea84cd0128fd","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-more-link.styl","hash":"15063d79b5befc21820baf05d6f20cc1c1787477","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"c6dab7661a6b8c678b21b7eb273cef7100f970f6","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"4eb18b12fa0ea6c35925d9a64f64e2a7dae8c7fd","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"618f73450cf541f88a4fddc3d22898aee49d105d","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"3eb73cee103b810fa56901577ecb9c9bb1793cff","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"eba491ae624b4c843c8be4c94a044085dad4ba0f","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"637c6b32c58ecf40041be6e911471cd82671919b","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1483444391000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1483444391000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1483444391000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1483444391000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1483444391000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1483444391000}],"Category":[{"name":"kafka","_id":"ciz42l2jn0004bc0v4w6g8n0y"},{"name":"java","_id":"ciz42l2k70009bc0vriq1twrb"},{"name":"spark","_id":"ciz42l2kh000lbc0v2xyg5uif"}],"Data":[],"Page":[{"title":"about","date":"2017-01-10T01:43:37.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-01-10 09:43:37\n---\n","updated":"2017-01-10T01:43:37.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"ciz42l2jj0001bc0vkrmo4q2v","content":"","excerpt":"","more":""},{"title":"categories","date":"2017-01-10T01:37:50.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-01-10 09:37:50\ntype: \"categories\"\ncomments: false\n---\n","updated":"2017-01-10T01:39:00.000Z","path":"categories/index.html","layout":"page","_id":"ciz42l2jm0003bc0vh8wnuc34","content":"","excerpt":"","more":""},{"title":"tags","date":"2017-01-10T01:40:36.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-01-10 09:40:36\ntype: \"tags\"\ncomments: false\n---\n","updated":"2017-01-10T01:40:55.000Z","path":"tags/index.html","layout":"page","_id":"ciz42l2oc001gbc0vmga4u96v","content":"","excerpt":"","more":""}],"Post":[{"title":"Kafka 概述","date":"2017-02-13T12:11:17.000Z","_content":"### Kafka架构\n* Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker\n* Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）/Users/yanyong/WebStudy/Blog/YanY/source/_posts/Kafka-概述.md\n\n* Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。\n* Producer：负责发布消息到Kafka broker\n* Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。\n* Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nKafka拓扑结构：\n\n![kafka-tupe-20170213.png](http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### Kafka的Topic & Partition\nTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。\n消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：\n![kafka_log_anatomy_20170213.png](http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。\n\n### Kafka 分区机制\nKafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有**消息（.log）**和**索引文件（.index）**，这使得Kafka的吞吐率可以水平扩展。\n生产者在生产数据的时候，可以**为每条消息指定Key**，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。\n默认kakfa.producer.Partitioner接口的类\n```\nclass DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner {\n  def partition(key: Any, numPartitions: Int): Int = {\n    Utils.abs(key.hashCode) % numPartitions\n  }\n}\n```\n\n### Consumer Group\nKafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。\n* 单播：所有Consumer在同一个Croup里\n* 广播：每个Consumer有一个独立的Group\n\n使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。\n\n### Producer\nProducer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。\nproducer 写入消息序列图:\n\n![kafka-producer-20170213.png.png](http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n流程说明:\n```\n1. producer 先从 zookeeper 的 \"/brokers/.../state\" 节点找到该 partition 的 leader\n2. producer 将消息发送给该 leader\n3. leader 将消息写入本地 log\n4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK\n5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK\n```\n\n### Kafka Shell\n启动：\n```\n ./kafka-server-start.sh ../config/server.properties > /dev/null &\n```\n查看topic：\n```\n./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list\n```\n创建topic：\n```\n./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY\n#  replication-factor 副本参数\n#  partitions 分区参数\n\n```\n删除topic:\n```\n./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete\n```\n消费者：\n```\n./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning\n```\n生产者：\n```\n./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY\n```\n    \n\n参考：\nhttp://www.infoq.com/cn/articles/kafka-analysis-part-1/\nhttp://www.cnblogs.com/cyfonly/p/5954614.html\n\n","source":"_posts/Kafka-概述.md","raw":"---\ntitle: Kafka 概述\ndate: 2017-02-13 20:11:17\ntags: [kafka]\ncategories: [kafka]\n---\n### Kafka架构\n* Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker\n* Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）/Users/yanyong/WebStudy/Blog/YanY/source/_posts/Kafka-概述.md\n\n* Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。\n* Producer：负责发布消息到Kafka broker\n* Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。\n* Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。\n\nKafka拓扑结构：\n\n![kafka-tupe-20170213.png](http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### Kafka的Topic & Partition\nTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。\n消息发送时都被发送到一个topic，其本质就是一个目录，而__topic是由一些Partition Logs(分区日志)组成__，其组织结构如下图所示：\n![kafka_log_anatomy_20170213.png](http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。\n\n### Kafka 分区机制\nKafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有**消息（.log）**和**索引文件（.index）**，这使得Kafka的吞吐率可以水平扩展。\n生产者在生产数据的时候，可以**为每条消息指定Key**，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。\n默认kakfa.producer.Partitioner接口的类\n```\nclass DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner {\n  def partition(key: Any, numPartitions: Int): Int = {\n    Utils.abs(key.hashCode) % numPartitions\n  }\n}\n```\n\n### Consumer Group\nKafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。\n* 单播：所有Consumer在同一个Croup里\n* 广播：每个Consumer有一个独立的Group\n\n使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。\n\n### Producer\nProducer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。\nproducer 写入消息序列图:\n\n![kafka-producer-20170213.png.png](http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n流程说明:\n```\n1. producer 先从 zookeeper 的 \"/brokers/.../state\" 节点找到该 partition 的 leader\n2. producer 将消息发送给该 leader\n3. leader 将消息写入本地 log\n4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK\n5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK\n```\n\n### Kafka Shell\n启动：\n```\n ./kafka-server-start.sh ../config/server.properties > /dev/null &\n```\n查看topic：\n```\n./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list\n```\n创建topic：\n```\n./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY\n#  replication-factor 副本参数\n#  partitions 分区参数\n\n```\n删除topic:\n```\n./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete\n```\n消费者：\n```\n./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning\n```\n生产者：\n```\n./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY\n```\n    \n\n参考：\nhttp://www.infoq.com/cn/articles/kafka-analysis-part-1/\nhttp://www.cnblogs.com/cyfonly/p/5954614.html\n\n","slug":"Kafka-概述","published":1,"updated":"2017-02-13T12:22:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2je0000bc0v2xticoxc","content":"<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><ul>\n<li>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</li>\n<li><p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）/Users/yanyong/WebStudy/Blog/YanY/source/_posts/Kafka-概述.md</p>\n</li>\n<li><p>Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。</p>\n</li>\n<li>Producer：负责发布消息到Kafka broker</li>\n<li>Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。</li>\n<li>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</li>\n</ul>\n<p>Kafka拓扑结构：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-tupe-20170213.png\"></p>\n<h3 id=\"Kafka的Topic-amp-Partition\"><a href=\"#Kafka的Topic-amp-Partition\" class=\"headerlink\" title=\"Kafka的Topic &amp; Partition\"></a>Kafka的Topic &amp; Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。<br>消息发送时都被发送到一个topic，其本质就是一个目录，而<strong>topic是由一些Partition Logs(分区日志)组成</strong>，其组织结构如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka_log_anatomy_20170213.png\"><br>因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>\n<h3 id=\"Kafka-分区机制\"><a href=\"#Kafka-分区机制\" class=\"headerlink\" title=\"Kafka 分区机制\"></a>Kafka 分区机制</h3><p>Kafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有<strong>消息（.log）</strong>和<strong>索引文件（.index）</strong>，这使得Kafka的吞吐率可以水平扩展。<br>生产者在生产数据的时候，可以<strong>为每条消息指定Key</strong>，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。<br>默认kakfa.producer.Partitioner接口的类<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">class DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner &#123;</div><div class=\"line\">  def partition(key: Any, numPartitions: Int): Int = &#123;</div><div class=\"line\">    Utils.abs(key.hashCode) % numPartitions</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"Consumer-Group\"><a href=\"#Consumer-Group\" class=\"headerlink\" title=\"Consumer Group\"></a>Consumer Group</h3><p>Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>\n<ul>\n<li>单播：所有Consumer在同一个Croup里</li>\n<li>广播：每个Consumer有一个独立的Group</li>\n</ul>\n<p>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。</p>\n<h3 id=\"Producer\"><a href=\"#Producer\" class=\"headerlink\" title=\"Producer\"></a>Producer</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。<br>producer 写入消息序列图:</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-producer-20170213.png.png\"></p>\n<p>流程说明:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader</div><div class=\"line\">2. producer 将消息发送给该 leader</div><div class=\"line\">3. leader 将消息写入本地 log</div><div class=\"line\">4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</div><div class=\"line\">5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</div></pre></td></tr></table></figure></p>\n<h3 id=\"Kafka-Shell\"><a href=\"#Kafka-Shell\" class=\"headerlink\" title=\"Kafka Shell\"></a>Kafka Shell</h3><p>启动：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-server-start.sh ../config/server.properties &gt; /dev/null &amp;</div></pre></td></tr></table></figure></p>\n<p>查看topic：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list</div></pre></td></tr></table></figure></p>\n<p>创建topic：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY</div><div class=\"line\">#  replication-factor 副本参数</div><div class=\"line\">#  partitions 分区参数</div></pre></td></tr></table></figure></p>\n<p>删除topic:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete</div></pre></td></tr></table></figure></p>\n<p>消费者：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning</div></pre></td></tr></table></figure></p>\n<p>生产者：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY</div></pre></td></tr></table></figure></p>\n<p>参考：<br><a href=\"http://www.infoq.com/cn/articles/kafka-analysis-part-1/\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/articles/kafka-analysis-part-1/</a><br><a href=\"http://www.cnblogs.com/cyfonly/p/5954614.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/cyfonly/p/5954614.html</a></p>\n","excerpt":"","more":"<h3 id=\"Kafka架构\"><a href=\"#Kafka架构\" class=\"headerlink\" title=\"Kafka架构\"></a>Kafka架构</h3><ul>\n<li>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</li>\n<li><p>Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）/Users/yanyong/WebStudy/Blog/YanY/source/_posts/Kafka-概述.md</p>\n</li>\n<li><p>Partition：Partition（分片）是物理上的概念，每个Topic包含一个或多个Partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件。</p>\n</li>\n<li>Producer：负责发布消息到Kafka broker</li>\n<li>Consumer：消息消费者，向kafka broker读取消息的客户端。每个consumer属于一个特定的consuer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。</li>\n<li>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</li>\n</ul>\n<p>Kafka拓扑结构：</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2deb6eeb9853f960.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-tupe-20170213.png\"></p>\n<h3 id=\"Kafka的Topic-amp-Partition\"><a href=\"#Kafka的Topic-amp-Partition\" class=\"headerlink\" title=\"Kafka的Topic &amp; Partition\"></a>Kafka的Topic &amp; Partition</h3><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。<br>消息发送时都被发送到一个topic，其本质就是一个目录，而<strong>topic是由一些Partition Logs(分区日志)组成</strong>，其组织结构如下图所示：<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-0da76ff4a59ef941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka_log_anatomy_20170213.png\"><br>因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>\n<h3 id=\"Kafka-分区机制\"><a href=\"#Kafka-分区机制\" class=\"headerlink\" title=\"Kafka 分区机制\"></a>Kafka 分区机制</h3><p>Kafka中可以将Topic从物理上划分成一个或多个分区（Partition），每个分区在物理上对应一个文件夹，以“topicName_partitionIndex”的命名方式命名，该文件夹下存储这个分区的所有<strong>消息（.log）</strong>和<strong>索引文件（.index）</strong>，这使得Kafka的吞吐率可以水平扩展。<br>生产者在生产数据的时候，可以<strong>为每条消息指定Key</strong>，这样消息被发送到broker时，会根据分区规则选择被存储到哪一个分区中，如果分区规则设置的合理，那么所有的消息会将被均分的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，在消费端，同一个消费组可以多线程并发的从多个分区中同时消费数据。<br>默认kakfa.producer.Partitioner接口的类<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">class DefaultPartitioner(props: VerifiableProperties = null) extends Partitioner &#123;</div><div class=\"line\">  def partition(key: Any, numPartitions: Int): Int = &#123;</div><div class=\"line\">    Utils.abs(key.hashCode) % numPartitions</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"Consumer-Group\"><a href=\"#Consumer-Group\" class=\"headerlink\" title=\"Consumer Group\"></a>Consumer Group</h3><p>Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。</p>\n<ul>\n<li>单播：所有Consumer在同一个Croup里</li>\n<li>广播：每个Consumer有一个独立的Group</li>\n</ul>\n<p>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。</p>\n<h3 id=\"Producer\"><a href=\"#Producer\" class=\"headerlink\" title=\"Producer\"></a>Producer</h3><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。<br>producer 写入消息序列图:</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-1e171553afe93789.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kafka-producer-20170213.png.png\"></p>\n<p>流程说明:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader</div><div class=\"line\">2. producer 将消息发送给该 leader</div><div class=\"line\">3. leader 将消息写入本地 log</div><div class=\"line\">4. followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</div><div class=\"line\">5. leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</div></pre></td></tr></table></figure></p>\n<h3 id=\"Kafka-Shell\"><a href=\"#Kafka-Shell\" class=\"headerlink\" title=\"Kafka Shell\"></a>Kafka Shell</h3><p>启动：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-server-start.sh ../config/server.properties &gt; /dev/null &amp;</div></pre></td></tr></table></figure></p>\n<p>查看topic：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topics.sh --zookeeper 10.211.55.5:2181 --list</div></pre></td></tr></table></figure></p>\n<p>创建topic：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topics.sh --create --zookeeper 10.211.55.5:2181 --replication-factor 2 --partitions 1 --topic testYanY</div><div class=\"line\">#  replication-factor 副本参数</div><div class=\"line\">#  partitions 分区参数</div></pre></td></tr></table></figure></p>\n<p>删除topic:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-topic.sh --zookeeper 10.211.55.5:2181 --topic  testYanY --delete</div></pre></td></tr></table></figure></p>\n<p>消费者：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-console-consumer.sh --zookeeper 10.211.55.5:2181 --topic testYanY --from-beginning</div></pre></td></tr></table></figure></p>\n<p>生产者：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./kafka-console-producer.sh --broker-list 10.211.55.5:9092 --topic testYanY</div></pre></td></tr></table></figure></p>\n<p>参考：<br><a href=\"http://www.infoq.com/cn/articles/kafka-analysis-part-1/\">http://www.infoq.com/cn/articles/kafka-analysis-part-1/</a><br><a href=\"http://www.cnblogs.com/cyfonly/p/5954614.html\">http://www.cnblogs.com/cyfonly/p/5954614.html</a></p>\n"},{"title":"Java NIO ByteBuffer概述","date":"2017-02-12T09:29:27.000Z","_content":"### Buffer的基本用法\n使用Buffer读写数据一般遵循以下四个步骤：\n1、分配空间\n2、写入数据到Buffer\n3、调用flip()方法\n4、从Buffer中读取数据\n5、调用clear()方法或者compact()方法\n当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过`flip()`方法`将Buffer从写模式切换到读模式`。在读模式下，可以读取之前写入到buffer的所有数据。\n一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。`clear()方法会清空整个缓冲区`。`compact()方法只会清除已经读过的数据`。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。\n\n![Buffer-process.jpeg](http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 四个主要属性：\n* capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n* position：\n  * 当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。\n  * 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n* limit：\n  * 在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。\n  * 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\n* mark：为某一读过的位置做标记，便于某些时候回退到该位置。\n\n### Buffer基本函数\n#### Buffer的分配\n```\nByteBuffer buf = ByteBuffer.allocate(48);\n```\n\n#### 向Buffer中写数据\n从Channel写到Buffer的例子\n```\nint bytesRead = inChannel.read(buf); //read into buffer.\n```\n通过put方法写Buffer\n```\nbuf.put(127);\n```\n\n#### 从Buffer中读取数据\n从Buffer读取数据到Channel的例子：\n```\n//read from buffer into channel.\nint bytesWritten = inChannel.write(buf);\n```\n使用get()方法从Buffer中读取数据的例子\n```\nbyte aByte = buf.get();\n```\n#### flip()方法\nflip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。\n换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。\n#### clear()与compact()方法\n一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。\n如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。\ncompact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n\n### 图解ByteBuffer函数过程\n#### put\nWrites the given byte into this buffer at the current position, and then increments the position. \n写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。 \n![Buffer-put.png](http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### flip\nFlips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.\nAfter a sequence of channel-read or `put` operations, invoke this method to prepare for a sequence of channel-write or relative  `get` operations.（即读写模型切换）\n写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion\n```\npublic final Buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n }\n```\n![Buffer-flip.png](http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### get\nReads the byte at this buffer's current position, and then increments the position.\n从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置\n![Buffer-get.png](http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### clear\nClears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.\nInvoke this method before using a sequence of channel-read or `put` operations to fill this buffer.\n将position置为0，并不清除buffer内容。\n```java\n    public final Buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n```\n![Buffer-clear.png](http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n参考：\nhttp://ifeve.com/buffers/\n博客：\nhttp://yany8060.xyz/","source":"_posts/Java-NIO-ByteBuffer概述.md","raw":"---\ntitle: Java NIO ByteBuffer概述\ndate: 2017-02-12 17:29:27\ntags: [java-nio]\ncategories: [java]\n---\n### Buffer的基本用法\n使用Buffer读写数据一般遵循以下四个步骤：\n1、分配空间\n2、写入数据到Buffer\n3、调用flip()方法\n4、从Buffer中读取数据\n5、调用clear()方法或者compact()方法\n当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过`flip()`方法`将Buffer从写模式切换到读模式`。在读模式下，可以读取之前写入到buffer的所有数据。\n一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。`clear()方法会清空整个缓冲区`。`compact()方法只会清除已经读过的数据`。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。\n\n![Buffer-process.jpeg](http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 四个主要属性：\n* capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n* position：\n  * 当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。\n  * 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n* limit：\n  * 在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。\n  * 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\n* mark：为某一读过的位置做标记，便于某些时候回退到该位置。\n\n### Buffer基本函数\n#### Buffer的分配\n```\nByteBuffer buf = ByteBuffer.allocate(48);\n```\n\n#### 向Buffer中写数据\n从Channel写到Buffer的例子\n```\nint bytesRead = inChannel.read(buf); //read into buffer.\n```\n通过put方法写Buffer\n```\nbuf.put(127);\n```\n\n#### 从Buffer中读取数据\n从Buffer读取数据到Channel的例子：\n```\n//read from buffer into channel.\nint bytesWritten = inChannel.write(buf);\n```\n使用get()方法从Buffer中读取数据的例子\n```\nbyte aByte = buf.get();\n```\n#### flip()方法\nflip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。\n换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。\n#### clear()与compact()方法\n一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。\n如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。\ncompact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n\n### 图解ByteBuffer函数过程\n#### put\nWrites the given byte into this buffer at the current position, and then increments the position. \n写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。 \n![Buffer-put.png](http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### flip\nFlips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.\nAfter a sequence of channel-read or `put` operations, invoke this method to prepare for a sequence of channel-write or relative  `get` operations.（即读写模型切换）\n写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion\n```\npublic final Buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n }\n```\n![Buffer-flip.png](http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### get\nReads the byte at this buffer's current position, and then increments the position.\n从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置\n![Buffer-get.png](http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### clear\nClears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.\nInvoke this method before using a sequence of channel-read or `put` operations to fill this buffer.\n将position置为0，并不清除buffer内容。\n```java\n    public final Buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n```\n![Buffer-clear.png](http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n参考：\nhttp://ifeve.com/buffers/\n博客：\nhttp://yany8060.xyz/","slug":"Java-NIO-ByteBuffer概述","published":1,"updated":"2017-02-12T09:32:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2jk0002bc0v5m7t3hjv","content":"<h3 id=\"Buffer的基本用法\"><a href=\"#Buffer的基本用法\" class=\"headerlink\" title=\"Buffer的基本用法\"></a>Buffer的基本用法</h3><p>使用Buffer读写数据一般遵循以下四个步骤：<br>1、分配空间<br>2、写入数据到Buffer<br>3、调用flip()方法<br>4、从Buffer中读取数据<br>5、调用clear()方法或者compact()方法<br>当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过<code>flip()</code>方法<code>将Buffer从写模式切换到读模式</code>。在读模式下，可以读取之前写入到buffer的所有数据。<br>一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。<code>clear()方法会清空整个缓冲区</code>。<code>compact()方法只会清除已经读过的数据</code>。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-process.jpeg\"></p>\n<h3 id=\"四个主要属性：\"><a href=\"#四个主要属性：\" class=\"headerlink\" title=\"四个主要属性：\"></a>四个主要属性：</h3><ul>\n<li>capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。</li>\n<li>position：<ul>\n<li>当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。</li>\n<li>当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。</li>\n</ul>\n</li>\n<li>limit：<ul>\n<li>在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。</li>\n<li>当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）</li>\n</ul>\n</li>\n<li>mark：为某一读过的位置做标记，便于某些时候回退到该位置。</li>\n</ul>\n<h3 id=\"Buffer基本函数\"><a href=\"#Buffer基本函数\" class=\"headerlink\" title=\"Buffer基本函数\"></a>Buffer基本函数</h3><h4 id=\"Buffer的分配\"><a href=\"#Buffer的分配\" class=\"headerlink\" title=\"Buffer的分配\"></a>Buffer的分配</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ByteBuffer buf = ByteBuffer.allocate(48);</div></pre></td></tr></table></figure>\n<h4 id=\"向Buffer中写数据\"><a href=\"#向Buffer中写数据\" class=\"headerlink\" title=\"向Buffer中写数据\"></a>向Buffer中写数据</h4><p>从Channel写到Buffer的例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">int bytesRead = inChannel.read(buf); //read into buffer.</div></pre></td></tr></table></figure></p>\n<p>通过put方法写Buffer<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">buf.put(127);</div></pre></td></tr></table></figure></p>\n<h4 id=\"从Buffer中读取数据\"><a href=\"#从Buffer中读取数据\" class=\"headerlink\" title=\"从Buffer中读取数据\"></a>从Buffer中读取数据</h4><p>从Buffer读取数据到Channel的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//read from buffer into channel.</div><div class=\"line\">int bytesWritten = inChannel.write(buf);</div></pre></td></tr></table></figure></p>\n<p>使用get()方法从Buffer中读取数据的例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">byte aByte = buf.get();</div></pre></td></tr></table></figure></p>\n<h4 id=\"flip-方法\"><a href=\"#flip-方法\" class=\"headerlink\" title=\"flip()方法\"></a>flip()方法</h4><p>flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。<br>换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。</p>\n<h4 id=\"clear-与compact-方法\"><a href=\"#clear-与compact-方法\" class=\"headerlink\" title=\"clear()与compact()方法\"></a>clear()与compact()方法</h4><p>一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。<br>如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。<br>如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。<br>如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。<br>compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。</p>\n<h3 id=\"图解ByteBuffer函数过程\"><a href=\"#图解ByteBuffer函数过程\" class=\"headerlink\" title=\"图解ByteBuffer函数过程\"></a>图解ByteBuffer函数过程</h3><h4 id=\"put\"><a href=\"#put\" class=\"headerlink\" title=\"put\"></a>put</h4><p>Writes the given byte into this buffer at the current position, and then increments the position.<br>写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-put.png\"></p>\n<h4 id=\"flip\"><a href=\"#flip\" class=\"headerlink\" title=\"flip\"></a>flip</h4><p>Flips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.<br>After a sequence of channel-read or <code>put</code> operations, invoke this method to prepare for a sequence of channel-write or relative  <code>get</code> operations.（即读写模型切换）<br>写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">public final Buffer flip() &#123;</div><div class=\"line\">        limit = position;</div><div class=\"line\">        position = 0;</div><div class=\"line\">        mark = -1;</div><div class=\"line\">        return this;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-flip.png\"></p>\n<h4 id=\"get\"><a href=\"#get\" class=\"headerlink\" title=\"get\"></a>get</h4><p>Reads the byte at this buffer’s current position, and then increments the position.<br>从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-get.png\"></p>\n<h4 id=\"clear\"><a href=\"#clear\" class=\"headerlink\" title=\"clear\"></a>clear</h4><p>Clears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.<br>Invoke this method before using a sequence of channel-read or <code>put</code> operations to fill this buffer.<br>将position置为0，并不清除buffer内容。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Buffer <span class=\"title\">clear</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    position = <span class=\"number\">0</span>;</div><div class=\"line\">    limit = capacity;</div><div class=\"line\">    mark = -<span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-clear.png\"></p>\n<p>参考：<br><a href=\"http://ifeve.com/buffers/\" target=\"_blank\" rel=\"external\">http://ifeve.com/buffers/</a><br>博客：<br><a href=\"http://yany8060.xyz/\" target=\"_blank\" rel=\"external\">http://yany8060.xyz/</a></p>\n","excerpt":"","more":"<h3 id=\"Buffer的基本用法\"><a href=\"#Buffer的基本用法\" class=\"headerlink\" title=\"Buffer的基本用法\"></a>Buffer的基本用法</h3><p>使用Buffer读写数据一般遵循以下四个步骤：<br>1、分配空间<br>2、写入数据到Buffer<br>3、调用flip()方法<br>4、从Buffer中读取数据<br>5、调用clear()方法或者compact()方法<br>当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过<code>flip()</code>方法<code>将Buffer从写模式切换到读模式</code>。在读模式下，可以读取之前写入到buffer的所有数据。<br>一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。<code>clear()方法会清空整个缓冲区</code>。<code>compact()方法只会清除已经读过的数据</code>。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-d832d71903ddb1e1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-process.jpeg\"></p>\n<h3 id=\"四个主要属性：\"><a href=\"#四个主要属性：\" class=\"headerlink\" title=\"四个主要属性：\"></a>四个主要属性：</h3><ul>\n<li>capacity：作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。</li>\n<li>position：<ul>\n<li>当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1。</li>\n<li>当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。</li>\n</ul>\n</li>\n<li>limit：<ul>\n<li>在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。</li>\n<li>当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）</li>\n</ul>\n</li>\n<li>mark：为某一读过的位置做标记，便于某些时候回退到该位置。</li>\n</ul>\n<h3 id=\"Buffer基本函数\"><a href=\"#Buffer基本函数\" class=\"headerlink\" title=\"Buffer基本函数\"></a>Buffer基本函数</h3><h4 id=\"Buffer的分配\"><a href=\"#Buffer的分配\" class=\"headerlink\" title=\"Buffer的分配\"></a>Buffer的分配</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ByteBuffer buf = ByteBuffer.allocate(48);</div></pre></td></tr></table></figure>\n<h4 id=\"向Buffer中写数据\"><a href=\"#向Buffer中写数据\" class=\"headerlink\" title=\"向Buffer中写数据\"></a>向Buffer中写数据</h4><p>从Channel写到Buffer的例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">int bytesRead = inChannel.read(buf); //read into buffer.</div></pre></td></tr></table></figure></p>\n<p>通过put方法写Buffer<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">buf.put(127);</div></pre></td></tr></table></figure></p>\n<h4 id=\"从Buffer中读取数据\"><a href=\"#从Buffer中读取数据\" class=\"headerlink\" title=\"从Buffer中读取数据\"></a>从Buffer中读取数据</h4><p>从Buffer读取数据到Channel的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//read from buffer into channel.</div><div class=\"line\">int bytesWritten = inChannel.write(buf);</div></pre></td></tr></table></figure></p>\n<p>使用get()方法从Buffer中读取数据的例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">byte aByte = buf.get();</div></pre></td></tr></table></figure></p>\n<h4 id=\"flip-方法\"><a href=\"#flip-方法\" class=\"headerlink\" title=\"flip()方法\"></a>flip()方法</h4><p>flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。<br>换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。</p>\n<h4 id=\"clear-与compact-方法\"><a href=\"#clear-与compact-方法\" class=\"headerlink\" title=\"clear()与compact()方法\"></a>clear()与compact()方法</h4><p>一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。<br>如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。<br>如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。<br>如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。<br>compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。</p>\n<h3 id=\"图解ByteBuffer函数过程\"><a href=\"#图解ByteBuffer函数过程\" class=\"headerlink\" title=\"图解ByteBuffer函数过程\"></a>图解ByteBuffer函数过程</h3><h4 id=\"put\"><a href=\"#put\" class=\"headerlink\" title=\"put\"></a>put</h4><p>Writes the given byte into this buffer at the current position, and then increments the position.<br>写模式下，往buffer里写一个字节，并把postion移动一位。写模式下，一般limit与capacity相等。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae958aab995c9963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-put.png\"></p>\n<h4 id=\"flip\"><a href=\"#flip\" class=\"headerlink\" title=\"flip\"></a>flip</h4><p>Flips this buffer.  The limit is set to the current position and then the position is set to zero.  If the mark is defined then it is discarded.<br>After a sequence of channel-read or <code>put</code> operations, invoke this method to prepare for a sequence of channel-write or relative  <code>get</code> operations.（即读写模型切换）<br>写完数据，需要开始读的时候，将postion复位到0，并将limit设为当前postion<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">public final Buffer flip() &#123;</div><div class=\"line\">        limit = position;</div><div class=\"line\">        position = 0;</div><div class=\"line\">        mark = -1;</div><div class=\"line\">        return this;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-804ddf879bbc2cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-flip.png\"></p>\n<h4 id=\"get\"><a href=\"#get\" class=\"headerlink\" title=\"get\"></a>get</h4><p>Reads the byte at this buffer’s current position, and then increments the position.<br>从buffer里读一个字节，并把postion移动一位。上限是limit，即写入数据的最后位置<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-2bbde49b79b2f805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-get.png\"></p>\n<h4 id=\"clear\"><a href=\"#clear\" class=\"headerlink\" title=\"clear\"></a>clear</h4><p>Clears this buffer.  The position is set to zero, the limit is set to the capacity, and the mark is discarded.<br>Invoke this method before using a sequence of channel-read or <code>put</code> operations to fill this buffer.<br>将position置为0，并不清除buffer内容。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">final</span> Buffer <span class=\"title\">clear</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    position = <span class=\"number\">0</span>;</div><div class=\"line\">    limit = capacity;</div><div class=\"line\">    mark = -<span class=\"number\">1</span>;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-9a36d39cd8787e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"Buffer-clear.png\"></p>\n<p>参考：<br><a href=\"http://ifeve.com/buffers/\">http://ifeve.com/buffers/</a><br>博客：<br><a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a></p>\n"},{"title":"Java NIO简述","date":"2017-02-12T08:45:16.000Z","_content":"### 概述\nJava NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。\n\n### I/O模型\n* 同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！\n* 同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。\n* 异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！\n* 异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 \n\n### BIO、NIO、AIO\n* Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。\n* Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I/O请求时才启动一个线程进行处理。\n* Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。\n\n### Java NIO由以下几个核心部分组成：\n* Channel（通道）\n* Buffer（缓冲区）\n* Selector（选择器）\n\n#### channel\nJava NIO的通道类似流，但又有些不同：\n* 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。\n* 通道可以异步地读写。\n* 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入\n![overview-channels-buffers.png](http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nChannel的实现\n* `FileChannel` 从文件中读写数据\n* `DatagramChannel`能通过UDP读写网络中的数据\n* `SocketChannel` 能通过TCP读写网络中的数据\n* `ServerSocketChannel` 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。\n\n#### Buffer\nJava NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。\n缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。\n\n#### Selector\nSelector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。\n![selectors.png](http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。\n\n\nSelector的创建\n```\nSelector selector = Selector.open();\n```\n向Selector注册通道\n```\n// 与Selector一起使用时，Channel必须处于非阻塞模式下\nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector,Selectionkey.OP_READ);\n```\n通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道\n```\nint select() //阻塞到至少有一个通道在你注册的事件上就绪了\nint select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)\nint selectNow()  //不会阻塞，不管什么通道就绪都立刻返回\n```","source":"_posts/Java-NIO简述.md","raw":"---\ntitle: Java NIO简述\ndate: 2017-02-12 16:45:16\ntags: [java-nio]\ncategories: [java]\n---\n### 概述\nJava NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。\n\n### I/O模型\n* 同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！\n* 同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。\n* 异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！\n* 异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 \n\n### BIO、NIO、AIO\n* Java BIO ： __同步并阻塞__，服务器实现模式为__一个连接一个线程__，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。\n* Java NIO：__同步非阻塞__，服务器实现模式为__一个请求一个线程__，即客户端发送的连接请求都会注册到多路复用器上，多路复用器__轮询__到连接有I/O请求时才启动一个线程进行处理。\n* Java AIO：__异步非阻塞__，服务器实现模式为__一个有效请求一个线程__，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。\n\n### Java NIO由以下几个核心部分组成：\n* Channel（通道）\n* Buffer（缓冲区）\n* Selector（选择器）\n\n#### channel\nJava NIO的通道类似流，但又有些不同：\n* 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。\n* 通道可以异步地读写。\n* 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入\n![overview-channels-buffers.png](http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nChannel的实现\n* `FileChannel` 从文件中读写数据\n* `DatagramChannel`能通过UDP读写网络中的数据\n* `SocketChannel` 能通过TCP读写网络中的数据\n* `ServerSocketChannel` 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。\n\n#### Buffer\nJava NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。\n缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。\n\n#### Selector\nSelector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。\n![selectors.png](http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。\n\n\nSelector的创建\n```\nSelector selector = Selector.open();\n```\n向Selector注册通道\n```\n// 与Selector一起使用时，Channel必须处于非阻塞模式下\nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector,Selectionkey.OP_READ);\n```\n通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道\n```\nint select() //阻塞到至少有一个通道在你注册的事件上就绪了\nint select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)\nint selectNow()  //不会阻塞，不管什么通道就绪都立刻返回\n```","slug":"Java-NIO简述","published":1,"updated":"2017-02-12T09:31:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2js0006bc0va23xrgtt","content":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。</p>\n<h3 id=\"I-O模型\"><a href=\"#I-O模型\" class=\"headerlink\" title=\"I/O模型\"></a>I/O模型</h3><ul>\n<li>同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！</li>\n<li>同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。</li>\n<li>异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！</li>\n<li>异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 </li>\n</ul>\n<h3 id=\"BIO、NIO、AIO\"><a href=\"#BIO、NIO、AIO\" class=\"headerlink\" title=\"BIO、NIO、AIO\"></a>BIO、NIO、AIO</h3><ul>\n<li>Java BIO ： <strong>同步并阻塞</strong>，服务器实现模式为<strong>一个连接一个线程</strong>，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。</li>\n<li>Java NIO：<strong>同步非阻塞</strong>，服务器实现模式为<strong>一个请求一个线程</strong>，即客户端发送的连接请求都会注册到多路复用器上，多路复用器<strong>轮询</strong>到连接有I/O请求时才启动一个线程进行处理。</li>\n<li>Java AIO：<strong>异步非阻塞</strong>，服务器实现模式为<strong>一个有效请求一个线程</strong>，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。</li>\n</ul>\n<h3 id=\"Java-NIO由以下几个核心部分组成：\"><a href=\"#Java-NIO由以下几个核心部分组成：\" class=\"headerlink\" title=\"Java NIO由以下几个核心部分组成：\"></a>Java NIO由以下几个核心部分组成：</h3><ul>\n<li>Channel（通道）</li>\n<li>Buffer（缓冲区）</li>\n<li>Selector（选择器）</li>\n</ul>\n<h4 id=\"channel\"><a href=\"#channel\" class=\"headerlink\" title=\"channel\"></a>channel</h4><p>Java NIO的通道类似流，但又有些不同：</p>\n<ul>\n<li>既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。</li>\n<li>通道可以异步地读写。</li>\n<li>通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"overview-channels-buffers.png\"></li>\n</ul>\n<p>Channel的实现</p>\n<ul>\n<li><code>FileChannel</code> 从文件中读写数据</li>\n<li><code>DatagramChannel</code>能通过UDP读写网络中的数据</li>\n<li><code>SocketChannel</code> 能通过TCP读写网络中的数据</li>\n<li><code>ServerSocketChannel</code> 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。</li>\n</ul>\n<h4 id=\"Buffer\"><a href=\"#Buffer\" class=\"headerlink\" title=\"Buffer\"></a>Buffer</h4><p>Java NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。<br>缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。</p>\n<h4 id=\"Selector\"><a href=\"#Selector\" class=\"headerlink\" title=\"Selector\"></a>Selector</h4><p>Selector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"selectors.png\"><br>要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。</p>\n<p>Selector的创建<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Selector selector = Selector.open();</div></pre></td></tr></table></figure></p>\n<p>向Selector注册通道<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 与Selector一起使用时，Channel必须处于非阻塞模式下</div><div class=\"line\">channel.configureBlocking(false);</div><div class=\"line\">SelectionKey key = channel.register(selector,Selectionkey.OP_READ);</div></pre></td></tr></table></figure></p>\n<p>通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">int select() //阻塞到至少有一个通道在你注册的事件上就绪了</div><div class=\"line\">int select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)</div><div class=\"line\">int selectNow()  //不会阻塞，不管什么通道就绪都立刻返回</div></pre></td></tr></table></figure></p>\n","excerpt":"","more":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。</p>\n<h3 id=\"I-O模型\"><a href=\"#I-O模型\" class=\"headerlink\" title=\"I/O模型\"></a>I/O模型</h3><ul>\n<li>同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！</li>\n<li>同步非阻塞IO：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。</li>\n<li>异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！</li>\n<li>异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 </li>\n</ul>\n<h3 id=\"BIO、NIO、AIO\"><a href=\"#BIO、NIO、AIO\" class=\"headerlink\" title=\"BIO、NIO、AIO\"></a>BIO、NIO、AIO</h3><ul>\n<li>Java BIO ： <strong>同步并阻塞</strong>，服务器实现模式为<strong>一个连接一个线程</strong>，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。</li>\n<li>Java NIO：<strong>同步非阻塞</strong>，服务器实现模式为<strong>一个请求一个线程</strong>，即客户端发送的连接请求都会注册到多路复用器上，多路复用器<strong>轮询</strong>到连接有I/O请求时才启动一个线程进行处理。</li>\n<li>Java AIO：<strong>异步非阻塞</strong>，服务器实现模式为<strong>一个有效请求一个线程</strong>，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。</li>\n</ul>\n<h3 id=\"Java-NIO由以下几个核心部分组成：\"><a href=\"#Java-NIO由以下几个核心部分组成：\" class=\"headerlink\" title=\"Java NIO由以下几个核心部分组成：\"></a>Java NIO由以下几个核心部分组成：</h3><ul>\n<li>Channel（通道）</li>\n<li>Buffer（缓冲区）</li>\n<li>Selector（选择器）</li>\n</ul>\n<h4 id=\"channel\"><a href=\"#channel\" class=\"headerlink\" title=\"channel\"></a>channel</h4><p>Java NIO的通道类似流，但又有些不同：</p>\n<ul>\n<li>既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。</li>\n<li>通道可以异步地读写。</li>\n<li>通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-75f7966a4f4478ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"overview-channels-buffers.png\"></li>\n</ul>\n<p>Channel的实现</p>\n<ul>\n<li><code>FileChannel</code> 从文件中读写数据</li>\n<li><code>DatagramChannel</code>能通过UDP读写网络中的数据</li>\n<li><code>SocketChannel</code> 能通过TCP读写网络中的数据</li>\n<li><code>ServerSocketChannel</code> 可以监听新进来的TCP连接，像Web服务器那样，对每一个新进来的连接都创建一个SocketChannel。</li>\n</ul>\n<h4 id=\"Buffer\"><a href=\"#Buffer\" class=\"headerlink\" title=\"Buffer\"></a>Buffer</h4><p>Java NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。<br>缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。</p>\n<h4 id=\"Selector\"><a href=\"#Selector\" class=\"headerlink\" title=\"Selector\"></a>Selector</h4><p>Selector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。<br><img src=\"http://upload-images.jianshu.io/upload_images/1419542-683c996697d33078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"selectors.png\"><br>要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。</p>\n<p>Selector的创建<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Selector selector = Selector.open();</div></pre></td></tr></table></figure></p>\n<p>向Selector注册通道<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 与Selector一起使用时，Channel必须处于非阻塞模式下</div><div class=\"line\">channel.configureBlocking(false);</div><div class=\"line\">SelectionKey key = channel.register(selector,Selectionkey.OP_READ);</div></pre></td></tr></table></figure></p>\n<p>通过Selector选择通道：一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">int select() //阻塞到至少有一个通道在你注册的事件上就绪了</div><div class=\"line\">int select(long timeout) //和select()一样，除了最长会阻塞timeout毫秒(参数)</div><div class=\"line\">int selectNow()  //不会阻塞，不管什么通道就绪都立刻返回</div></pre></td></tr></table></figure></p>\n"},{"title":"Spark RDD详解","date":"2017-02-07T11:58:41.000Z","_content":"### RDD基本概念：\n* RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。\n* RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                \n* DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖\n* 窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data\n* 宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖\n\n<!-- more -->\n\n### RDD构建图\n![sparkRdd.jpg](http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n### RDD来源：\n*  parallelizing an existing collection in your driver program（程序内部已经存在的数据集）\n* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）\n\n\n\n### RDD特点：\n1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。\n2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。\n3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。\n4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。\n```\n// return the set of partitions in this RDD\nprotected def getPartitions: Array[Partition]\n/* 分片列表 */\n\n// compute a given partition\ndef compute(split: Partition, context: TaskContext): Iterator[T]\n/* compute 对分片进行计算,得出一个可遍历的结果 */\n\n// return how this RDD depends on parent RDDs\nprotected def getDependencies: Seq[Dependency[_]] = deps\n/* 只计算一次，计算RDD对父RDD的依赖 */\n\n@transient val partitioner: Option[Partitioner] = None\n /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */\n\nprotected def getPreferredLocations(split: Partition): Seq[String] = Nil\n/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/\n```\n\n\n### RDD操作\n* transformations：接受RDD并返回RDD\nTransformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。\n* action：接受RDD但是返回非RDD\nAction调用会直接计算。\n\n### RDD优化技巧\n* RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。\n可以通过`rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)`或`rdd.cache()`（就是调用persist）来缓存数据\n* 转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并\n* 减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。\n\n### RDD运行过程（具体在Spark任务调度中详细说明）\n1、创建RDD对象\n2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG\n3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销\n\n\n\n参考：\nhttp://www.cnblogs.com/bourneli/p/4394271.html\nhttp://www.jianshu.com/p/4ff6afbbafe4\nhttp://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\n\n### 附\nTransformation具体内容\n`map(func)` :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成\n`filter(func) `: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成\n`flatMap(func) `: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）\n`sample(withReplacement, frac, seed) `:根据给定的随机种子seed，随机抽样出数量为frac的数据\n`union(otherDataset)` : 返回一个新的数据集，由原数据集和参数联合而成\n`groupByKey([numTasks])` :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task\n`reduceByKey(func, [numTasks]) `: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。\n`join(otherDataset, [numTasks])` ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集\n`groupWith(otherDataset, [numTasks])` ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup\n`cartesian(otherDataset) `: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。\n\nActions具体内容\n`reduce(func) `: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行\n`collect()` : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM\n`count()` : 返回数据集的元素个数\n`take(n)` : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）\n`first()` : 返回数据集的第一个元素（类似于take(1)）\n`saveAsTextFile(path) `: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本\n`saveAsSequenceFile(path)` : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）\n`foreach(func)` : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互\n","source":"_posts/Spark-RDD详解.md","raw":"---\ntitle: Spark RDD详解\ndate: 2017-02-07 19:58:41\ntags: [spark]\ncategories: [spark]\n---\n### RDD基本概念：\n* RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。\n* RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                \n* DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖\n* 窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data\n* 宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖\n\n<!-- more -->\n\n### RDD构建图\n![sparkRdd.jpg](http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n\n### RDD来源：\n*  parallelizing an existing collection in your driver program（程序内部已经存在的数据集）\n* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）\n\n\n\n### RDD特点：\n1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。\n2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。\n3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。\n4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。\n```\n// return the set of partitions in this RDD\nprotected def getPartitions: Array[Partition]\n/* 分片列表 */\n\n// compute a given partition\ndef compute(split: Partition, context: TaskContext): Iterator[T]\n/* compute 对分片进行计算,得出一个可遍历的结果 */\n\n// return how this RDD depends on parent RDDs\nprotected def getDependencies: Seq[Dependency[_]] = deps\n/* 只计算一次，计算RDD对父RDD的依赖 */\n\n@transient val partitioner: Option[Partitioner] = None\n /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */\n\nprotected def getPreferredLocations(split: Partition): Seq[String] = Nil\n/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/\n```\n\n\n### RDD操作\n* transformations：接受RDD并返回RDD\nTransformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。\n* action：接受RDD但是返回非RDD\nAction调用会直接计算。\n\n### RDD优化技巧\n* RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。\n可以通过`rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)`或`rdd.cache()`（就是调用persist）来缓存数据\n* 转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并\n* 减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。\n\n### RDD运行过程（具体在Spark任务调度中详细说明）\n1、创建RDD对象\n2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG\n3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销\n\n\n\n参考：\nhttp://www.cnblogs.com/bourneli/p/4394271.html\nhttp://www.jianshu.com/p/4ff6afbbafe4\nhttp://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\n\n### 附\nTransformation具体内容\n`map(func)` :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成\n`filter(func) `: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成\n`flatMap(func) `: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）\n`sample(withReplacement, frac, seed) `:根据给定的随机种子seed，随机抽样出数量为frac的数据\n`union(otherDataset)` : 返回一个新的数据集，由原数据集和参数联合而成\n`groupByKey([numTasks])` :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task\n`reduceByKey(func, [numTasks]) `: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。\n`join(otherDataset, [numTasks])` ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集\n`groupWith(otherDataset, [numTasks])` ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup\n`cartesian(otherDataset) `: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。\n\nActions具体内容\n`reduce(func) `: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行\n`collect()` : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM\n`count()` : 返回数据集的元素个数\n`take(n)` : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）\n`first()` : 返回数据集的第一个元素（类似于take(1)）\n`saveAsTextFile(path) `: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本\n`saveAsSequenceFile(path)` : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）\n`foreach(func)` : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互\n","slug":"Spark-RDD详解","published":1,"updated":"2017-02-07T12:00:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2ju0007bc0vfbdhpurk","content":"<h3 id=\"RDD基本概念：\"><a href=\"#RDD基本概念：\" class=\"headerlink\" title=\"RDD基本概念：\"></a>RDD基本概念：</h3><ul>\n<li>RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。</li>\n<li>RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                </li>\n<li>DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖</li>\n<li>窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data</li>\n<li>宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖</li>\n</ul>\n<a id=\"more\"></a>\n<h3 id=\"RDD构建图\"><a href=\"#RDD构建图\" class=\"headerlink\" title=\"RDD构建图\"></a>RDD构建图</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"sparkRdd.jpg\"></p>\n<h3 id=\"RDD来源：\"><a href=\"#RDD来源：\" class=\"headerlink\" title=\"RDD来源：\"></a>RDD来源：</h3><ul>\n<li>parallelizing an existing collection in your driver program（程序内部已经存在的数据集）</li>\n<li>referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）</li>\n</ul>\n<h3 id=\"RDD特点：\"><a href=\"#RDD特点：\" class=\"headerlink\" title=\"RDD特点：\"></a>RDD特点：</h3><p>1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。<br>2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。<br>3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。<br>4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。<br>5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">// return the set of partitions in this RDD</div><div class=\"line\">protected def getPartitions: Array[Partition]</div><div class=\"line\">/* 分片列表 */</div><div class=\"line\"></div><div class=\"line\">// compute a given partition</div><div class=\"line\">def compute(split: Partition, context: TaskContext): Iterator[T]</div><div class=\"line\">/* compute 对分片进行计算,得出一个可遍历的结果 */</div><div class=\"line\"></div><div class=\"line\">// return how this RDD depends on parent RDDs</div><div class=\"line\">protected def getDependencies: Seq[Dependency[_]] = deps</div><div class=\"line\">/* 只计算一次，计算RDD对父RDD的依赖 */</div><div class=\"line\"></div><div class=\"line\">@transient val partitioner: Option[Partitioner] = None</div><div class=\"line\"> /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */</div><div class=\"line\"></div><div class=\"line\">protected def getPreferredLocations(split: Partition): Seq[String] = Nil</div><div class=\"line\">/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/</div></pre></td></tr></table></figure></p>\n<h3 id=\"RDD操作\"><a href=\"#RDD操作\" class=\"headerlink\" title=\"RDD操作\"></a>RDD操作</h3><ul>\n<li>transformations：接受RDD并返回RDD<br>Transformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。</li>\n<li>action：接受RDD但是返回非RDD<br>Action调用会直接计算。</li>\n</ul>\n<h3 id=\"RDD优化技巧\"><a href=\"#RDD优化技巧\" class=\"headerlink\" title=\"RDD优化技巧\"></a>RDD优化技巧</h3><ul>\n<li>RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。<br>可以通过<code>rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)</code>或<code>rdd.cache()</code>（就是调用persist）来缓存数据</li>\n<li>转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并</li>\n<li>减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。</li>\n</ul>\n<h3 id=\"RDD运行过程（具体在Spark任务调度中详细说明）\"><a href=\"#RDD运行过程（具体在Spark任务调度中详细说明）\" class=\"headerlink\" title=\"RDD运行过程（具体在Spark任务调度中详细说明）\"></a>RDD运行过程（具体在Spark任务调度中详细说明）</h3><p>1、创建RDD对象<br>2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG<br>3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销</p>\n<p>参考：<br><a href=\"http://www.cnblogs.com/bourneli/p/4394271.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/bourneli/p/4394271.html</a><br><a href=\"http://www.jianshu.com/p/4ff6afbbafe4\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/4ff6afbbafe4</a><br><a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\" rel=\"external\">http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds</a></p>\n<h3 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h3><p>Transformation具体内容<br><code>map(func)</code> :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成<br><code>filter(func)</code>: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成<br><code>flatMap(func)</code>: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）<br><code>sample(withReplacement, frac, seed)</code>:根据给定的随机种子seed，随机抽样出数量为frac的数据<br><code>union(otherDataset)</code> : 返回一个新的数据集，由原数据集和参数联合而成<br><code>groupByKey([numTasks])</code> :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task<br><code>reduceByKey(func, [numTasks])</code>: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。<br><code>join(otherDataset, [numTasks])</code> ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集<br><code>groupWith(otherDataset, [numTasks])</code> ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup<br><code>cartesian(otherDataset)</code>: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。</p>\n<p>Actions具体内容<br><code>reduce(func)</code>: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行<br><code>collect()</code> : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM<br><code>count()</code> : 返回数据集的元素个数<br><code>take(n)</code> : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）<br><code>first()</code> : 返回数据集的第一个元素（类似于take(1)）<br><code>saveAsTextFile(path)</code>: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本<br><code>saveAsSequenceFile(path)</code> : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）<br><code>foreach(func)</code> : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p>\n","excerpt":"<h3 id=\"RDD基本概念：\"><a href=\"#RDD基本概念：\" class=\"headerlink\" title=\"RDD基本概念：\"></a>RDD基本概念：</h3><ul>\n<li>RDD（ resilient distributed dataset，弹性分布式数据集）：spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作）。</li>\n<li>RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。                </li>\n<li>DAG （Directed Acycle graph，有向无环图）：反应RDD之间的依赖</li>\n<li>窄依赖（Narrow dependency）：子RDD依赖于父RDD中固定的data</li>\n<li>宽依赖（Wide Dependency）：子RDD对父RDD中的所有data partition都有依赖</li>\n</ul>","more":"<h3 id=\"RDD构建图\"><a href=\"#RDD构建图\" class=\"headerlink\" title=\"RDD构建图\"></a>RDD构建图</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-ae8b332de4f1f91f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"sparkRdd.jpg\"></p>\n<h3 id=\"RDD来源：\"><a href=\"#RDD来源：\" class=\"headerlink\" title=\"RDD来源：\"></a>RDD来源：</h3><ul>\n<li>parallelizing an existing collection in your driver program（程序内部已经存在的数据集）</li>\n<li>referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat（外部存储系统）</li>\n</ul>\n<h3 id=\"RDD特点：\"><a href=\"#RDD特点：\" class=\"headerlink\" title=\"RDD特点：\"></a>RDD特点：</h3><p>1、有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。<br>2、有一个函数计算每一个分片，这里指的是下面会提到的compute函数。<br>3、对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。<br>4、可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。<br>5、可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">// return the set of partitions in this RDD</div><div class=\"line\">protected def getPartitions: Array[Partition]</div><div class=\"line\">/* 分片列表 */</div><div class=\"line\"></div><div class=\"line\">// compute a given partition</div><div class=\"line\">def compute(split: Partition, context: TaskContext): Iterator[T]</div><div class=\"line\">/* compute 对分片进行计算,得出一个可遍历的结果 */</div><div class=\"line\"></div><div class=\"line\">// return how this RDD depends on parent RDDs</div><div class=\"line\">protected def getDependencies: Seq[Dependency[_]] = deps</div><div class=\"line\">/* 只计算一次，计算RDD对父RDD的依赖 */</div><div class=\"line\"></div><div class=\"line\">@transient val partitioner: Option[Partitioner] = None</div><div class=\"line\"> /* 可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce */</div><div class=\"line\"></div><div class=\"line\">protected def getPreferredLocations(split: Partition): Seq[String] = Nil</div><div class=\"line\">/* 可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置*/</div></pre></td></tr></table></figure></p>\n<h3 id=\"RDD操作\"><a href=\"#RDD操作\" class=\"headerlink\" title=\"RDD操作\"></a>RDD操作</h3><ul>\n<li>transformations：接受RDD并返回RDD<br>Transformation采用惰性调用机制，每个RDD记录父RDD转换的方法，这种调用链表称之为血缘（lineage）。</li>\n<li>action：接受RDD但是返回非RDD<br>Action调用会直接计算。</li>\n</ul>\n<h3 id=\"RDD优化技巧\"><a href=\"#RDD优化技巧\" class=\"headerlink\" title=\"RDD优化技巧\"></a>RDD优化技巧</h3><ul>\n<li>RDD缓存：需要使用多次的数据需要cache，否则会进行不必要的重复操作。<br>可以通过<code>rdd.persist(newLevel: StorageLevel, allowOverride: Boolean)</code>或<code>rdd.cache()</code>（就是调用persist）来缓存数据</li>\n<li>转换并行化：RDD的转换操作是并行化计算的，多个RDD的转换同样是可以并</li>\n<li>减少shuffle网络传输：网络I/O开销是很大的，减少网络开销，可以显著加快计算效率。</li>\n</ul>\n<h3 id=\"RDD运行过程（具体在Spark任务调度中详细说明）\"><a href=\"#RDD运行过程（具体在Spark任务调度中详细说明）\" class=\"headerlink\" title=\"RDD运行过程（具体在Spark任务调度中详细说明）\"></a>RDD运行过程（具体在Spark任务调度中详细说明）</h3><p>1、创建RDD对象<br>2、DAGScheduler模块介入运行，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG<br>3、每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销</p>\n<p>参考：<br><a href=\"http://www.cnblogs.com/bourneli/p/4394271.html\">http://www.cnblogs.com/bourneli/p/4394271.html</a><br><a href=\"http://www.jianshu.com/p/4ff6afbbafe4\">http://www.jianshu.com/p/4ff6afbbafe4</a><br><a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds</a></p>\n<h3 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h3><p>Transformation具体内容<br><code>map(func)</code> :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成<br><code>filter(func)</code>: 返回一个新的数据集，由经过func函数后返回值为true的原元素组成<br><code>flatMap(func)</code>: 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素）<br><code>sample(withReplacement, frac, seed)</code>:根据给定的随机种子seed，随机抽样出数量为frac的数据<br><code>union(otherDataset)</code> : 返回一个新的数据集，由原数据集和参数联合而成<br><code>groupByKey([numTasks])</code> :在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task<br><code>reduceByKey(func, [numTasks])</code>: 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。<br><code>join(otherDataset, [numTasks])</code> ：在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集<br><code>groupWith(otherDataset, [numTasks])</code> ：在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup<br><code>cartesian(otherDataset)</code>: 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。</p>\n<p>Actions具体内容<br><code>reduce(func)</code>: 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行<br><code>collect()</code> : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM<br><code>count()</code> : 返回数据集的元素个数<br><code>take(n)</code> : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用）<br><code>first()</code> : 返回数据集的第一个元素（类似于take(1)）<br><code>saveAsTextFile(path)</code>: 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本<br><code>saveAsSequenceFile(path)</code> : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等）<br><code>foreach(func)</code> : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p>"},{"title":"Spark Shuffle基础","date":"2017-02-06T07:45:36.000Z","_content":"### Shuffle 基本概念\n#### 概述：\n* Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。\n* 当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。\n* 由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。\n<!--more-->\n#### Spark 的Shuffle 分为 Write，Read 两阶段\n* Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责\n* Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的\n* 所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。\n\nShuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。\n\n### Spark的Shuffle机制\n> Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。\n\nShuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。\nShuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。\n\n\n### Shuffle的基本流程\n> bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等\n\n![shuffle-write-no-consolidation.png](http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n* 首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。\n* 其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。\n* 当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。\n\n### Spark中Shuffle类型\n* Hash Shuffle：\n第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。\n* Sort Shuffle：\n每个map产生一个文件，彻底解决了扩展性问题\n\n\n本文只是对Shuffle作了初步的描述，了解基本概念\n\n\n### 问题\n今天遇到如下问题，特来了解一下。\n```\n17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)\njava.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)\n     at java.io.FileOutputStream.open(Native Method)\n     at java.io.FileOutputStream.<init>(FileOutputStream.java:212)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n     at java.lang.Thread.run(Thread.java:722)\n```\n一般造成此问题的是系统资源不够用\n参考网上的解决方案,修改启动参数：\n* 添加：--conf spark.shuffle.manager=SORT\n Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销\n* 是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。\n     \n\n\n参考：\nhttp://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\nhttp://www.jianshu.com/p/c83bb237caa8\nhttps://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\n","source":"_posts/Spark-Shuffle基础.md","raw":"---\ntitle: Spark Shuffle基础\ndate: 2017-02-06 15:45:36\ntags: [spark]\ncategories: [spark]\n---\n### Shuffle 基本概念\n#### 概述：\n* Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。\n* 当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。\n* 由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。\n<!--more-->\n#### Spark 的Shuffle 分为 Write，Read 两阶段\n* Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责\n* Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的\n* 所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。\n\nShuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。\n\n### Spark的Shuffle机制\n> Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。\n\nShuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。\nShuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。\n\n\n### Shuffle的基本流程\n> bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等\n\n![shuffle-write-no-consolidation.png](http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n* 首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。\n* 其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。\n* 当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。\n\n### Spark中Shuffle类型\n* Hash Shuffle：\n第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。\n* Sort Shuffle：\n每个map产生一个文件，彻底解决了扩展性问题\n\n\n本文只是对Shuffle作了初步的描述，了解基本概念\n\n\n### 问题\n今天遇到如下问题，特来了解一下。\n```\n17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)\njava.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)\n     at java.io.FileOutputStream.open(Native Method)\n     at java.io.FileOutputStream.<init>(FileOutputStream.java:212)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)\n     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n     at java.lang.Thread.run(Thread.java:722)\n```\n一般造成此问题的是系统资源不够用\n参考网上的解决方案,修改启动参数：\n* 添加：--conf spark.shuffle.manager=SORT\n Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销\n* 是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。\n     \n\n\n参考：\nhttp://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\nhttp://www.jianshu.com/p/c83bb237caa8\nhttps://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\n","slug":"Spark-Shuffle基础","published":1,"updated":"2017-02-06T08:19:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2jw0008bc0vq3s19b2p","content":"<h3 id=\"Shuffle-基本概念\"><a href=\"#Shuffle-基本概念\" class=\"headerlink\" title=\"Shuffle 基本概念\"></a>Shuffle 基本概念</h3><h4 id=\"概述：\"><a href=\"#概述：\" class=\"headerlink\" title=\"概述：\"></a>概述：</h4><ul>\n<li>Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。</li>\n<li>当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</li>\n<li>由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。<a id=\"more\"></a>\n<h4 id=\"Spark-的Shuffle-分为-Write，Read-两阶段\"><a href=\"#Spark-的Shuffle-分为-Write，Read-两阶段\" class=\"headerlink\" title=\"Spark 的Shuffle 分为 Write，Read 两阶段\"></a>Spark 的Shuffle 分为 Write，Read 两阶段</h4></li>\n<li>Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责</li>\n<li>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</li>\n<li>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</li>\n</ul>\n<p>Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>\n<h3 id=\"Spark的Shuffle机制\"><a href=\"#Spark的Shuffle机制\" class=\"headerlink\" title=\"Spark的Shuffle机制\"></a>Spark的Shuffle机制</h3><blockquote>\n<p>Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>\n</blockquote>\n<p>Shuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。<br>Shuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。</p>\n<h3 id=\"Shuffle的基本流程\"><a href=\"#Shuffle的基本流程\" class=\"headerlink\" title=\"Shuffle的基本流程\"></a>Shuffle的基本流程</h3><blockquote>\n<p>bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等</p>\n</blockquote>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"shuffle-write-no-consolidation.png\"></p>\n<ul>\n<li>首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</li>\n<li>其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。</li>\n<li>当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</li>\n</ul>\n<h3 id=\"Spark中Shuffle类型\"><a href=\"#Spark中Shuffle类型\" class=\"headerlink\" title=\"Spark中Shuffle类型\"></a>Spark中Shuffle类型</h3><ul>\n<li>Hash Shuffle：<br>第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。</li>\n<li>Sort Shuffle：<br>每个map产生一个文件，彻底解决了扩展性问题</li>\n</ul>\n<p>本文只是对Shuffle作了初步的描述，了解基本概念</p>\n<h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>今天遇到如下问题，特来了解一下。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)</div><div class=\"line\">java.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)</div><div class=\"line\">     at java.io.FileOutputStream.open(Native Method)</div><div class=\"line\">     at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)</div><div class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)</div><div class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)</div><div class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)</div><div class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)</div><div class=\"line\">     at org.apache.spark.scheduler.Task.run(Task.scala:85)</div><div class=\"line\">     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)</div><div class=\"line\">     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class=\"line\">     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class=\"line\">     at java.lang.Thread.run(Thread.java:722)</div></pre></td></tr></table></figure></p>\n<p>一般造成此问题的是系统资源不够用<br>参考网上的解决方案,修改启动参数：</p>\n<ul>\n<li>添加：–conf spark.shuffle.manager=SORT<br>Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销</li>\n<li>是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。</li>\n</ul>\n<p>参考：<br><a href=\"http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\" target=\"_blank\" rel=\"external\">http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/</a><br><a href=\"http://www.jianshu.com/p/c83bb237caa8\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/c83bb237caa8</a><br><a href=\"https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\" target=\"_blank\" rel=\"external\">https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md</a></p>\n","excerpt":"<h3 id=\"Shuffle-基本概念\"><a href=\"#Shuffle-基本概念\" class=\"headerlink\" title=\"Shuffle 基本概念\"></a>Shuffle 基本概念</h3><h4 id=\"概述：\"><a href=\"#概述：\" class=\"headerlink\" title=\"概述：\"></a>概述：</h4><ul>\n<li>Shuffle描述着数据从map task输出到reduce task 输入的这段过程。在分布式情况下，reduce task需要跨节点拉取其它节点上的map task结果。</li>\n<li>当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</li>\n<li>由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。","more":"<h4 id=\"Spark-的Shuffle-分为-Write，Read-两阶段\"><a href=\"#Spark-的Shuffle-分为-Write，Read-两阶段\" class=\"headerlink\" title=\"Spark 的Shuffle 分为 Write，Read 两阶段\"></a>Spark 的Shuffle 分为 Write，Read 两阶段</h4></li>\n<li>Write 对应的是ShuffleMapTask，具体的写操作ExternalSorter来负责</li>\n<li>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</li>\n<li>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</li>\n</ul>\n<p>Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>\n<h3 id=\"Spark的Shuffle机制\"><a href=\"#Spark的Shuffle机制\" class=\"headerlink\" title=\"Spark的Shuffle机制\"></a>Spark的Shuffle机制</h3><blockquote>\n<p>Spark中的Shuffle是把一组无规则的数据尽量转换成一组具有一定规则的数据。</p>\n</blockquote>\n<p>Shuffle就是包裹在各种需要重分区的算子之下的一个对数据进行重新组合的过程。<br>Shuffle将数据进行收集分配到指定Reduce分区，Reduce阶段根据函数对相应的分区做Reduce所需的函数处理。</p>\n<h3 id=\"Shuffle的基本流程\"><a href=\"#Shuffle的基本流程\" class=\"headerlink\" title=\"Shuffle的基本流程\"></a>Shuffle的基本流程</h3><blockquote>\n<p>bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等</p>\n</blockquote>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-47813c3a4aeccf1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"shuffle-write-no-consolidation.png\"></p>\n<ul>\n<li>首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</li>\n<li>其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。</li>\n<li>当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</li>\n</ul>\n<h3 id=\"Spark中Shuffle类型\"><a href=\"#Spark中Shuffle类型\" class=\"headerlink\" title=\"Spark中Shuffle类型\"></a>Spark中Shuffle类型</h3><ul>\n<li>Hash Shuffle：<br>第一版是每个map产生r个文件，一共产生mr个文件，但是产生的中间文件太大影响扩展性。而后进行修改，让一个core上的map共用文件，减少文件数目，这样共产生core个文件，但中间文件数目仍随任务数线性增加，仍然难以对应大作业。</li>\n<li>Sort Shuffle：<br>每个map产生一个文件，彻底解决了扩展性问题</li>\n</ul>\n<p>本文只是对Shuffle作了初步的描述，了解基本概念</p>\n<h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>今天遇到如下问题，特来了解一下。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">17/02/06 11:50:21 ERROR Executor: Exception in task 0.0 in stage 857456.0 (TID 437542)</div><div class=\"line\">java.io.FileNotFoundException: /tmp/spark-be115c66-a319-4931-a2ca-81ae9e7a6198/executor-54de96d2-5256-4637-b474-4342b00e755a/blockmgr-0c1c3d9f-c5d7-4b1c-bc12-7773083fa181/18/shuffle_426055_0_0.data.5874ce88-94f5-4c34-b56a-f729d4d4e393 (No such file or directory)</div><div class=\"line\">     at java.io.FileOutputStream.open(Native Method)</div><div class=\"line\">     at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)</div><div class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:182)</div><div class=\"line\">     at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:159)</div><div class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)</div><div class=\"line\">     at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)</div><div class=\"line\">     at org.apache.spark.scheduler.Task.run(Task.scala:85)</div><div class=\"line\">     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)</div><div class=\"line\">     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class=\"line\">     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class=\"line\">     at java.lang.Thread.run(Thread.java:722)</div></pre></td></tr></table></figure></p>\n<p>一般造成此问题的是系统资源不够用<br>参考网上的解决方案,修改启动参数：</p>\n<ul>\n<li>添加：–conf spark.shuffle.manager=SORT<br>Spark默认的shuffle采用Hash模式，会产生相当规模的文件，与此同时带来了大量的内存开销</li>\n<li>是因为一个excutor给分配的内存不够，此时，减少excutor-core的数量，加大excutor-memory的值应该就没有问题。</li>\n</ul>\n<p>参考：<br><a href=\"http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/\">http://blog.jasonding.top/2015/07/14/Spark/【Spark】Spark的Shuffle机制/</a><br><a href=\"http://www.jianshu.com/p/c83bb237caa8\">http://www.jianshu.com/p/c83bb237caa8</a><br><a href=\"https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md\">https://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.md</a></p>"},{"title":"SparkStream 函数详解-Transformations","date":"2017-02-05T12:36:59.000Z","_content":"一个DStream对象可以调用多种操作，主要分为如下几类：\n* Transformations\n* Window Operations\n* Join Operations\n* Output Operations\n\n### Transformations\n#### map(func)\n> Return a new DStream by passing each element of the source DStream through a function func.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//拼接一个”_NEW”字符串\nval linesNew = lines.map(lines => lines + \"_NEW\" )\n```\nmap中操作复杂可把函数抽出来在外部定义：\n```\n    val mapResult = stream.map(new Transformations().customeMap(_))\n\n    def customeMap(word: String): String = {\n        word + \"ss\"\n    }\n```\n\n#### filter(func)\n> Return a new DStream by selecting only the records of the source DStream on which func returns true.\n\n对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。\n```\nval filterWords = words.filter(_ != \"hello\" )\n```\n在定义函数式时，返回值必须是boolean类型，\n\n#### flatMap(func)\n> Similar to map, but each input item can be mapped to 0 or more output items.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//将lines根据空格进行分割，分割成若干个单词\nval words = lines.flatMap(_.split( \" \" ))\n```\n同map若函数复杂可提出去\n\n#### union(otherStream)\n> Return a new DStream that contains the union of the elements in the source DStream and otherDStream.\n\n这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。\n```\nval wordsOne = words.map(_ + \"_one\" )\nval wordsTwo = words.map(_ + \"_two\" )\nval unionWords = wordsOne.union(wordsTwo)\n```\n输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream\n\n```\nhello_one\nyany_one\ntian_one\nhello_two\nyany_two\ntian_two\n```\n\n#### repartition(numPartitions)\n> Changes the level of parallelism in this DStream by creating more or fewer partitions.\n\nReturn a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.\n\n#### count()\n> Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n\n统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。\n```\nval wordsCount = words.count()\n```\n\n#### reduce(func)\n> Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n\n返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。\n```\nval reduceWords = words.reduce(_ + \"-\" + _)\n// 或\n\ndef customerReduce(line1: String, line2: String): String = {\n    line1 + \"|||\" + line2\n  }\n // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));\n    val reduceResult = flatMapResult.reduce((x, y) => new Transformations().customerReduce(x, y));\n```\n\n#### countByValue()\n> When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n\n某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。\n```\nval countByValueWords = words.countByValue()\n```\n\n#### reduceByKey(func, [numTasks])\n> When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n\n调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。\n\n注：reduceByKey使用时DStream需要以(K, V)的形式出现\n```\nval pairs = words.map(word => (word , 1))\nval wordCounts = pairs.reduceByKey(_ + _)\n//===========\n/**\n    * \n    * reducebykey 通过对于两两的value进行操作,可自定义\n    * @param line1\n    * @param line2\n    * @return\n    */\n  def customerReduceByKey(line1: String, line2: String): String = {\n    \"ss\"\n  }\n    val reduceByKeyResult = flatMapResult.map((_, \"ss\")).reduceByKey(new Transformations().customerReduceByKey(_, _))\n```\n\n#### join(otherStream, [numTasks])\n> When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n\n由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.join(wordsTwo)\n\n```\n结果\n```\n// 输入 hello world hello join\n\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(join,(join_one,join_two))\n(world,(world_one,world_two))\n```\n如果key相同，出现笛卡尔积现象\n\n#### cogroup(otherStream, [numTasks])\n> When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n\n由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.cogroup(wordsTwo)\n```\n\n结果：\n```\n// 输入 hello world hello cogroup\n\n(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))\n(world,(CompactBuffer(world_one),CompactBuffer(world_two)))\n(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))\n```\n\n#### transform(func)\n#### updateStateByKey(func)\n\n\n参考：\nhttp://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\n\n博客：http://yany8060.xyz/\ngithup: https://github.com/yany8060/SparkDemo","source":"_posts/SparkStream-函数详解-Transformations.md","raw":"---\ntitle: SparkStream 函数详解-Transformations\ndate: 2017-02-05 20:36:59\ntags: [sparkstream,spark]\ncategories: [spark]\n---\n一个DStream对象可以调用多种操作，主要分为如下几类：\n* Transformations\n* Window Operations\n* Join Operations\n* Output Operations\n\n### Transformations\n#### map(func)\n> Return a new DStream by passing each element of the source DStream through a function func.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//拼接一个”_NEW”字符串\nval linesNew = lines.map(lines => lines + \"_NEW\" )\n```\nmap中操作复杂可把函数抽出来在外部定义：\n```\n    val mapResult = stream.map(new Transformations().customeMap(_))\n\n    def customeMap(word: String): String = {\n        word + \"ss\"\n    }\n```\n\n#### filter(func)\n> Return a new DStream by selecting only the records of the source DStream on which func returns true.\n\n对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。\n```\nval filterWords = words.filter(_ != \"hello\" )\n```\n在定义函数式时，返回值必须是boolean类型，\n\n#### flatMap(func)\n> Similar to map, but each input item can be mapped to 0 or more output items.\n\n主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。\n```\n//将lines根据空格进行分割，分割成若干个单词\nval words = lines.flatMap(_.split( \" \" ))\n```\n同map若函数复杂可提出去\n\n#### union(otherStream)\n> Return a new DStream that contains the union of the elements in the source DStream and otherDStream.\n\n这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。\n```\nval wordsOne = words.map(_ + \"_one\" )\nval wordsTwo = words.map(_ + \"_two\" )\nval unionWords = wordsOne.union(wordsTwo)\n```\n输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream\n\n```\nhello_one\nyany_one\ntian_one\nhello_two\nyany_two\ntian_two\n```\n\n#### repartition(numPartitions)\n> Changes the level of parallelism in this DStream by creating more or fewer partitions.\n\nReturn a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.\n\n#### count()\n> Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n\n统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。\n```\nval wordsCount = words.count()\n```\n\n#### reduce(func)\n> Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n\n返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。\n```\nval reduceWords = words.reduce(_ + \"-\" + _)\n// 或\n\ndef customerReduce(line1: String, line2: String): String = {\n    line1 + \"|||\" + line2\n  }\n // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));\n    val reduceResult = flatMapResult.reduce((x, y) => new Transformations().customerReduce(x, y));\n```\n\n#### countByValue()\n> When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n\n某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。\n```\nval countByValueWords = words.countByValue()\n```\n\n#### reduceByKey(func, [numTasks])\n> When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n\n调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。\n\n注：reduceByKey使用时DStream需要以(K, V)的形式出现\n```\nval pairs = words.map(word => (word , 1))\nval wordCounts = pairs.reduceByKey(_ + _)\n//===========\n/**\n    * \n    * reducebykey 通过对于两两的value进行操作,可自定义\n    * @param line1\n    * @param line2\n    * @return\n    */\n  def customerReduceByKey(line1: String, line2: String): String = {\n    \"ss\"\n  }\n    val reduceByKeyResult = flatMapResult.map((_, \"ss\")).reduceByKey(new Transformations().customerReduceByKey(_, _))\n```\n\n#### join(otherStream, [numTasks])\n> When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n\n由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.join(wordsTwo)\n\n```\n结果\n```\n// 输入 hello world hello join\n\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(hello,(hello_one,hello_two))\n(join,(join_one,join_two))\n(world,(world_one,world_two))\n```\n如果key相同，出现笛卡尔积现象\n\n#### cogroup(otherStream, [numTasks])\n> When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n\n由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。\n\n```\nval wordsOne = words.map(word => (word , word + \"_one\" ))\nval wordsTwo = words.map(word => (word , word + \"_two\" ))\nval joinWords = wordsOne.cogroup(wordsTwo)\n```\n\n结果：\n```\n// 输入 hello world hello cogroup\n\n(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))\n(world,(CompactBuffer(world_one),CompactBuffer(world_two)))\n(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))\n```\n\n#### transform(func)\n#### updateStateByKey(func)\n\n\n参考：\nhttp://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&utm_medium=referral\n\n博客：http://yany8060.xyz/\ngithup: https://github.com/yany8060/SparkDemo","slug":"SparkStream-函数详解-Transformations","published":1,"updated":"2017-02-06T01:31:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2k8000cbc0vn8zwi6rg","content":"<p>一个DStream对象可以调用多种操作，主要分为如下几类：</p>\n<ul>\n<li>Transformations</li>\n<li>Window Operations</li>\n<li>Join Operations</li>\n<li>Output Operations</li>\n</ul>\n<h3 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h3><h4 id=\"map-func\"><a href=\"#map-func\" class=\"headerlink\" title=\"map(func)\"></a>map(func)</h4><blockquote>\n<p>Return a new DStream by passing each element of the source DStream through a function func.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//拼接一个”_NEW”字符串</div><div class=\"line\">val linesNew = lines.map(lines =&gt; lines + &quot;_NEW&quot; )</div></pre></td></tr></table></figure></p>\n<p>map中操作复杂可把函数抽出来在外部定义：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">val mapResult = stream.map(new Transformations().customeMap(_))</div><div class=\"line\"></div><div class=\"line\">def customeMap(word: String): String = &#123;</div><div class=\"line\">    word + &quot;ss&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"filter-func\"><a href=\"#filter-func\" class=\"headerlink\" title=\"filter(func)\"></a>filter(func)</h4><blockquote>\n<p>Return a new DStream by selecting only the records of the source DStream on which func returns true.</p>\n</blockquote>\n<p>对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val filterWords = words.filter(_ != &quot;hello&quot; )</div></pre></td></tr></table></figure></p>\n<p>在定义函数式时，返回值必须是boolean类型，</p>\n<h4 id=\"flatMap-func\"><a href=\"#flatMap-func\" class=\"headerlink\" title=\"flatMap(func)\"></a>flatMap(func)</h4><blockquote>\n<p>Similar to map, but each input item can be mapped to 0 or more output items.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//将lines根据空格进行分割，分割成若干个单词</div><div class=\"line\">val words = lines.flatMap(_.split( &quot; &quot; ))</div></pre></td></tr></table></figure></p>\n<p>同map若函数复杂可提出去</p>\n<h4 id=\"union-otherStream\"><a href=\"#union-otherStream\" class=\"headerlink\" title=\"union(otherStream)\"></a>union(otherStream)</h4><blockquote>\n<p>Return a new DStream that contains the union of the elements in the source DStream and otherDStream.</p>\n</blockquote>\n<p>这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(_ + &quot;_one&quot; )</div><div class=\"line\">val wordsTwo = words.map(_ + &quot;_two&quot; )</div><div class=\"line\">val unionWords = wordsOne.union(wordsTwo)</div></pre></td></tr></table></figure></p>\n<p>输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">hello_one</div><div class=\"line\">yany_one</div><div class=\"line\">tian_one</div><div class=\"line\">hello_two</div><div class=\"line\">yany_two</div><div class=\"line\">tian_two</div></pre></td></tr></table></figure>\n<h4 id=\"repartition-numPartitions\"><a href=\"#repartition-numPartitions\" class=\"headerlink\" title=\"repartition(numPartitions)\"></a>repartition(numPartitions)</h4><blockquote>\n<p>Changes the level of parallelism in this DStream by creating more or fewer partitions.</p>\n</blockquote>\n<p>Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.</p>\n<h4 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count()\"></a>count()</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</p>\n</blockquote>\n<p>统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsCount = words.count()</div></pre></td></tr></table></figure></p>\n<h4 id=\"reduce-func\"><a href=\"#reduce-func\" class=\"headerlink\" title=\"reduce(func)\"></a>reduce(func)</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</p>\n</blockquote>\n<p>返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">val reduceWords = words.reduce(_ + &quot;-&quot; + _)</div><div class=\"line\">// 或</div><div class=\"line\"></div><div class=\"line\">def customerReduce(line1: String, line2: String): String = &#123;</div><div class=\"line\">    line1 + &quot;|||&quot; + line2</div><div class=\"line\">  &#125;</div><div class=\"line\"> // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));</div><div class=\"line\">    val reduceResult = flatMapResult.reduce((x, y) =&gt; new Transformations().customerReduce(x, y));</div></pre></td></tr></table></figure></p>\n<h4 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue()\"></a>countByValue()</h4><blockquote>\n<p>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</p>\n</blockquote>\n<p>某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val countByValueWords = words.countByValue()</div></pre></td></tr></table></figure></p>\n<h4 id=\"reduceByKey-func-numTasks\"><a href=\"#reduceByKey-func-numTasks\" class=\"headerlink\" title=\"reduceByKey(func, [numTasks])\"></a>reduceByKey(func, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</p>\n</blockquote>\n<p>调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。</p>\n<p>注：reduceByKey使用时DStream需要以(K, V)的形式出现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">val pairs = words.map(word =&gt; (word , 1))</div><div class=\"line\">val wordCounts = pairs.reduceByKey(_ + _)</div><div class=\"line\">//===========</div><div class=\"line\">/**</div><div class=\"line\">    * </div><div class=\"line\">    * reducebykey 通过对于两两的value进行操作,可自定义</div><div class=\"line\">    * @param line1</div><div class=\"line\">    * @param line2</div><div class=\"line\">    * @return</div><div class=\"line\">    */</div><div class=\"line\">  def customerReduceByKey(line1: String, line2: String): String = &#123;</div><div class=\"line\">    &quot;ss&quot;</div><div class=\"line\">  &#125;</div><div class=\"line\">    val reduceByKeyResult = flatMapResult.map((_, &quot;ss&quot;)).reduceByKey(new Transformations().customerReduceByKey(_, _))</div></pre></td></tr></table></figure></p>\n<h4 id=\"join-otherStream-numTasks\"><a href=\"#join-otherStream-numTasks\" class=\"headerlink\" title=\"join(otherStream, [numTasks])\"></a>join(otherStream, [numTasks])</h4><blockquote>\n<p>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</div><div class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</div><div class=\"line\">val joinWords = wordsOne.join(wordsTwo)</div></pre></td></tr></table></figure></p>\n<p>结果<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 输入 hello world hello join</div><div class=\"line\"></div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(join,(join_one,join_two))</div><div class=\"line\">(world,(world_one,world_two))</div></pre></td></tr></table></figure></p>\n<p>如果key相同，出现笛卡尔积现象</p>\n<h4 id=\"cogroup-otherStream-numTasks\"><a href=\"#cogroup-otherStream-numTasks\" class=\"headerlink\" title=\"cogroup(otherStream, [numTasks])\"></a>cogroup(otherStream, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</div><div class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</div><div class=\"line\">val joinWords = wordsOne.cogroup(wordsTwo)</div></pre></td></tr></table></figure>\n<p>结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 输入 hello world hello cogroup</div><div class=\"line\"></div><div class=\"line\">(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))</div><div class=\"line\">(world,(CompactBuffer(world_one),CompactBuffer(world_two)))</div><div class=\"line\">(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))</div></pre></td></tr></table></figure></p>\n<h4 id=\"transform-func\"><a href=\"#transform-func\" class=\"headerlink\" title=\"transform(func)\"></a>transform(func)</h4><h4 id=\"updateStateByKey-func\"><a href=\"#updateStateByKey-func\" class=\"headerlink\" title=\"updateStateByKey(func)\"></a>updateStateByKey(func)</h4><p>参考：<br><a href=\"http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<p>博客：<a href=\"http://yany8060.xyz/\" target=\"_blank\" rel=\"external\">http://yany8060.xyz/</a><br>githup: <a href=\"https://github.com/yany8060/SparkDemo\" target=\"_blank\" rel=\"external\">https://github.com/yany8060/SparkDemo</a></p>\n","excerpt":"","more":"<p>一个DStream对象可以调用多种操作，主要分为如下几类：</p>\n<ul>\n<li>Transformations</li>\n<li>Window Operations</li>\n<li>Join Operations</li>\n<li>Output Operations</li>\n</ul>\n<h3 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h3><h4 id=\"map-func\"><a href=\"#map-func\" class=\"headerlink\" title=\"map(func)\"></a>map(func)</h4><blockquote>\n<p>Return a new DStream by passing each element of the source DStream through a function func.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成新的元素，得到的DStream对象b中包含这些新的元素。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//拼接一个”_NEW”字符串</div><div class=\"line\">val linesNew = lines.map(lines =&gt; lines + &quot;_NEW&quot; )</div></pre></td></tr></table></figure></p>\n<p>map中操作复杂可把函数抽出来在外部定义：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">val mapResult = stream.map(new Transformations().customeMap(_))</div><div class=\"line\"></div><div class=\"line\">def customeMap(word: String): String = &#123;</div><div class=\"line\">    word + &quot;ss&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"filter-func\"><a href=\"#filter-func\" class=\"headerlink\" title=\"filter(func)\"></a>filter(func)</h4><blockquote>\n<p>Return a new DStream by selecting only the records of the source DStream on which func returns true.</p>\n</blockquote>\n<p>对DStream a中的每一个元素，应用func方法进行计算，如果func函数返回结果为true，则保留该元素，否则丢弃该元素，返回一个新的DStream b。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val filterWords = words.filter(_ != &quot;hello&quot; )</div></pre></td></tr></table></figure></p>\n<p>在定义函数式时，返回值必须是boolean类型，</p>\n<h4 id=\"flatMap-func\"><a href=\"#flatMap-func\" class=\"headerlink\" title=\"flatMap(func)\"></a>flatMap(func)</h4><blockquote>\n<p>Similar to map, but each input item can be mapped to 0 or more output items.</p>\n</blockquote>\n<p>主要作用是，对DStream对象a，将func函数作用到a中的每一个元素上并生成0个或多个新的元素，得到的DStream对象b中包含这些新的元素。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">//将lines根据空格进行分割，分割成若干个单词</div><div class=\"line\">val words = lines.flatMap(_.split( &quot; &quot; ))</div></pre></td></tr></table></figure></p>\n<p>同map若函数复杂可提出去</p>\n<h4 id=\"union-otherStream\"><a href=\"#union-otherStream\" class=\"headerlink\" title=\"union(otherStream)\"></a>union(otherStream)</h4><blockquote>\n<p>Return a new DStream that contains the union of the elements in the source DStream and otherDStream.</p>\n</blockquote>\n<p>这个操作将两个DStream进行合并，生成一个包含着两个DStream中所有元素的新DStream对象。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(_ + &quot;_one&quot; )</div><div class=\"line\">val wordsTwo = words.map(_ + &quot;_two&quot; )</div><div class=\"line\">val unionWords = wordsOne.union(wordsTwo)</div></pre></td></tr></table></figure></p>\n<p>输入 hello yany tian，结果是将wordsOne和wordsTwo合成一个unionWords的DStream</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">hello_one</div><div class=\"line\">yany_one</div><div class=\"line\">tian_one</div><div class=\"line\">hello_two</div><div class=\"line\">yany_two</div><div class=\"line\">tian_two</div></pre></td></tr></table></figure>\n<h4 id=\"repartition-numPartitions\"><a href=\"#repartition-numPartitions\" class=\"headerlink\" title=\"repartition(numPartitions)\"></a>repartition(numPartitions)</h4><blockquote>\n<p>Changes the level of parallelism in this DStream by creating more or fewer partitions.</p>\n</blockquote>\n<p>Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions.</p>\n<h4 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count()\"></a>count()</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</p>\n</blockquote>\n<p>统计DStream中每个RDD包含的元素的个数，得到一个新的DStream，这个DStream中只包含一个元素，这个元素是对应语句单词统计数值。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsCount = words.count()</div></pre></td></tr></table></figure></p>\n<h4 id=\"reduce-func\"><a href=\"#reduce-func\" class=\"headerlink\" title=\"reduce(func)\"></a>reduce(func)</h4><blockquote>\n<p>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</p>\n</blockquote>\n<p>返回一个包含一个元素的DStream，传入的func方法会作用在调用者的每一个元素上，将其中的元素顺次的两两进行计算。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">val reduceWords = words.reduce(_ + &quot;-&quot; + _)</div><div class=\"line\">// 或</div><div class=\"line\"></div><div class=\"line\">def customerReduce(line1: String, line2: String): String = &#123;</div><div class=\"line\">    line1 + &quot;|||&quot; + line2</div><div class=\"line\">  &#125;</div><div class=\"line\"> // val reduceResult = flatMapResult.reduce(new Transformations().customerReduce(_, _));</div><div class=\"line\">    val reduceResult = flatMapResult.reduce((x, y) =&gt; new Transformations().customerReduce(x, y));</div></pre></td></tr></table></figure></p>\n<h4 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue()\"></a>countByValue()</h4><blockquote>\n<p>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</p>\n</blockquote>\n<p>某个DStream中的元素类型为K，调用这个方法后，返回的DStream的元素为(K, Long)对，后面这个Long值是原DStream中每个RDD元素key出现的频率。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">val countByValueWords = words.countByValue()</div></pre></td></tr></table></figure></p>\n<h4 id=\"reduceByKey-func-numTasks\"><a href=\"#reduceByKey-func-numTasks\" class=\"headerlink\" title=\"reduceByKey(func, [numTasks])\"></a>reduceByKey(func, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.</p>\n</blockquote>\n<p>调用这个操作的DStream是以(K, V)的形式出现，返回一个新的元素格式为(K, V)的DStream。返回结果中，K为原来的K，V是由K经过传入func计算得到的。还可以传入一个并行计算的参数，在local模式下，默认为2。在其他模式下，默认值由参数 spark.default.parallelism 确定。</p>\n<p>注：reduceByKey使用时DStream需要以(K, V)的形式出现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">val pairs = words.map(word =&gt; (word , 1))</div><div class=\"line\">val wordCounts = pairs.reduceByKey(_ + _)</div><div class=\"line\">//===========</div><div class=\"line\">/**</div><div class=\"line\">    * </div><div class=\"line\">    * reducebykey 通过对于两两的value进行操作,可自定义</div><div class=\"line\">    * @param line1</div><div class=\"line\">    * @param line2</div><div class=\"line\">    * @return</div><div class=\"line\">    */</div><div class=\"line\">  def customerReduceByKey(line1: String, line2: String): String = &#123;</div><div class=\"line\">    &quot;ss&quot;</div><div class=\"line\">  &#125;</div><div class=\"line\">    val reduceByKeyResult = flatMapResult.map((_, &quot;ss&quot;)).reduceByKey(new Transformations().customerReduceByKey(_, _))</div></pre></td></tr></table></figure></p>\n<h4 id=\"join-otherStream-numTasks\"><a href=\"#join-otherStream-numTasks\" class=\"headerlink\" title=\"join(otherStream, [numTasks])\"></a>join(otherStream, [numTasks])</h4><blockquote>\n<p>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为 (k, V) ，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (V, W)) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</div><div class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</div><div class=\"line\">val joinWords = wordsOne.join(wordsTwo)</div></pre></td></tr></table></figure></p>\n<p>结果<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 输入 hello world hello join</div><div class=\"line\"></div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(hello,(hello_one,hello_two))</div><div class=\"line\">(join,(join_one,join_two))</div><div class=\"line\">(world,(world_one,world_two))</div></pre></td></tr></table></figure></p>\n<p>如果key相同，出现笛卡尔积现象</p>\n<h4 id=\"cogroup-otherStream-numTasks\"><a href=\"#cogroup-otherStream-numTasks\" class=\"headerlink\" title=\"cogroup(otherStream, [numTasks])\"></a>cogroup(otherStream, [numTasks])</h4><blockquote>\n<p>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</p>\n</blockquote>\n<p>由一个DStream对象调用该方法，元素内容为(k, V)，传入另一个DStream对象，元素内容为(k, W)，返回的DStream中包含的内容是 (k, (Seq[V], Seq[W])) 。这个方法也可以传入一个并行计算的参数，该参数与reduceByKey中是相同的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val wordsOne = words.map(word =&gt; (word , word + &quot;_one&quot; ))</div><div class=\"line\">val wordsTwo = words.map(word =&gt; (word , word + &quot;_two&quot; ))</div><div class=\"line\">val joinWords = wordsOne.cogroup(wordsTwo)</div></pre></td></tr></table></figure>\n<p>结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 输入 hello world hello cogroup</div><div class=\"line\"></div><div class=\"line\">(hello,(CompactBuffer(hello_one, hello_one),CompactBuffer(hello_two, hello_two)))</div><div class=\"line\">(world,(CompactBuffer(world_one),CompactBuffer(world_two)))</div><div class=\"line\">(cogroup,(CompactBuffer(cogroup_one),CompactBuffer(cogroup_two)))</div></pre></td></tr></table></figure></p>\n<h4 id=\"transform-func\"><a href=\"#transform-func\" class=\"headerlink\" title=\"transform(func)\"></a>transform(func)</h4><h4 id=\"updateStateByKey-func\"><a href=\"#updateStateByKey-func\" class=\"headerlink\" title=\"updateStateByKey(func)\"></a>updateStateByKey(func)</h4><p>参考：<br><a href=\"http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral\">http://blog.csdn.net/dabokele/article/details/52602412?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<p>博客：<a href=\"http://yany8060.xyz/\">http://yany8060.xyz/</a><br>githup: <a href=\"https://github.com/yany8060/SparkDemo\">https://github.com/yany8060/SparkDemo</a></p>\n"},{"title":"Spring-Boot 入门学习","date":"2017-01-25T06:03:40.000Z","_content":"> Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \"just run\". We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.\n> 简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置\n\n### 特征\n* Create stand-alone Spring applications\n* Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)\n* Provide opinionated 'starter' POMs to simplify your Maven configuration\n* Automatically configure Spring whenever possible\n* Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）\n* Absolutely no code generation and no requirement for XML configuration\n\n\n### Spring-Boot工程简单搭建\n> Spring-boot文档：http://docs.spring.io/spring-boot/docs/current/reference/html/\n> 具体代码详见：https://github.com/yany8060/SpringDemo\n\n#### pom.xml详情（本工程以maven方式搭建）：\n官方建议引入父工程\n```\n<properties> \n\t<java.version>1.7</java.version>\n\t<spring.boot.version>1.4.3.RELEASE</spring.boot.version>\n</properties>\n\n\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-parent</artifactId>\n\t<version>${spring.boot.version}</version>\n\t<type>pom</type>\n\t<scope>import</scope>\n</dependency>\n```\n\n在子工程中引入自己所需要的依赖（版本号有父工程管理）：\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-test</artifactId>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n\n```\n并添加maven编译插件：\n```\n<plugin>\n\t<groupId>org.apache.maven.plugins</groupId>\n\t<artifactId>maven-compiler-plugin</artifactId>\n</plugin>\n```\n更多SpringBoot Maven插件详见：http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\n\n#### 创建一个启动入口（应用类）\n我们创建一个Application类：\n```java\n@EnableAutoConfiguration\n@ComponentScan(basePackages = \"com.yany\")\n@Configuration\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class);\n    }\n\n}\n```\n`@EnableAutoConfiguration`\n开启自动配置\n这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring\n`@ComponentScan`\nSpring原注解，扫描包加载bean\n`@Configuration`\n标注一个类为配置类\n\n我们再创建一个Controller层：\n```java\n@RestController\npublic class Example {\n\n    @RequestMapping(value = \"/hello\", method = {RequestMethod.GET})\n    public String test() {\n        return \"Hello world\";\n    }\n\n}\n```\n`@RestController`\n相当于同时添加@Controller和@ResponseBody注解。\n\n这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：http://localhost:8080/SpringBoot/hello 来得到返回值\n\n#### 启动\n以main方法方式启动Application类，\n\n在浏览器中输入：http://localhost:8080/SpringBoot/hello\n得到返回：Hello world\n![](/img/work/14853239880359.jpg)\n\n\n具体输入日志如下：\n```\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v1.4.3.RELEASE)\n\n2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)\n2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default\n2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in '[com.yany]' package. Please check your configuration.\n2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)\n2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)\n2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat\n2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms\n2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [/]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\n2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/hello],methods=[GET]}\" onto public java.lang.String com.yany.controller.Example.test()\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup\n2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)\n2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)\n\n```\n\n\n","source":"_posts/Spring-Boot-入门学习.md","raw":"---\ntitle: Spring-Boot 入门学习\ndate: 2017-01-25 14:03:40\ntags: [spring-boot,spring]\ncategories: [java]\n---\n> Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can \"just run\". We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.\n> 简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置\n\n### 特征\n* Create stand-alone Spring applications\n* Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)\n* Provide opinionated 'starter' POMs to simplify your Maven configuration\n* Automatically configure Spring whenever possible\n* Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）\n* Absolutely no code generation and no requirement for XML configuration\n\n\n### Spring-Boot工程简单搭建\n> Spring-boot文档：http://docs.spring.io/spring-boot/docs/current/reference/html/\n> 具体代码详见：https://github.com/yany8060/SpringDemo\n\n#### pom.xml详情（本工程以maven方式搭建）：\n官方建议引入父工程\n```\n<properties> \n\t<java.version>1.7</java.version>\n\t<spring.boot.version>1.4.3.RELEASE</spring.boot.version>\n</properties>\n\n\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-parent</artifactId>\n\t<version>${spring.boot.version}</version>\n\t<type>pom</type>\n\t<scope>import</scope>\n</dependency>\n```\n\n在子工程中引入自己所需要的依赖（版本号有父工程管理）：\n```\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-test</artifactId>\n    <scope>test</scope>\n</dependency>\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n\n```\n并添加maven编译插件：\n```\n<plugin>\n\t<groupId>org.apache.maven.plugins</groupId>\n\t<artifactId>maven-compiler-plugin</artifactId>\n</plugin>\n```\n更多SpringBoot Maven插件详见：http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\n\n#### 创建一个启动入口（应用类）\n我们创建一个Application类：\n```java\n@EnableAutoConfiguration\n@ComponentScan(basePackages = \"com.yany\")\n@Configuration\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class);\n    }\n\n}\n```\n`@EnableAutoConfiguration`\n开启自动配置\n这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring\n`@ComponentScan`\nSpring原注解，扫描包加载bean\n`@Configuration`\n标注一个类为配置类\n\n我们再创建一个Controller层：\n```java\n@RestController\npublic class Example {\n\n    @RequestMapping(value = \"/hello\", method = {RequestMethod.GET})\n    public String test() {\n        return \"Hello world\";\n    }\n\n}\n```\n`@RestController`\n相当于同时添加@Controller和@ResponseBody注解。\n\n这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：http://localhost:8080/SpringBoot/hello 来得到返回值\n\n#### 启动\n以main方法方式启动Application类，\n\n在浏览器中输入：http://localhost:8080/SpringBoot/hello\n得到返回：Hello world\n![](/img/work/14853239880359.jpg)\n\n\n具体输入日志如下：\n```\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v1.4.3.RELEASE)\n\n2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)\n2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default\n2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in '[com.yany]' package. Please check your configuration.\n2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)\n2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)\n2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat\n2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext\n2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms\n2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [/]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\n2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]\n2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\n2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy\n2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/hello],methods=[GET]}\" onto public java.lang.String com.yany.controller.Example.test()\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\n2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup\n2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)\n2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)\n\n```\n\n\n","slug":"Spring-Boot-入门学习","published":1,"updated":"2017-02-04T03:22:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2ka000ebc0v6gw89ooi","content":"<blockquote>\n<p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.<br>简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置</p>\n</blockquote>\n<h3 id=\"特征\"><a href=\"#特征\" class=\"headerlink\" title=\"特征\"></a>特征</h3><ul>\n<li>Create stand-alone Spring applications</li>\n<li>Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>\n<li>Provide opinionated ‘starter’ POMs to simplify your Maven configuration</li>\n<li>Automatically configure Spring whenever possible</li>\n<li>Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）</li>\n<li>Absolutely no code generation and no requirement for XML configuration</li>\n</ul>\n<h3 id=\"Spring-Boot工程简单搭建\"><a href=\"#Spring-Boot工程简单搭建\" class=\"headerlink\" title=\"Spring-Boot工程简单搭建\"></a>Spring-Boot工程简单搭建</h3><blockquote>\n<p>Spring-boot文档：<a href=\"http://docs.spring.io/spring-boot/docs/current/reference/html/\" target=\"_blank\" rel=\"external\">http://docs.spring.io/spring-boot/docs/current/reference/html/</a><br>具体代码详见：<a href=\"https://github.com/yany8060/SpringDemo\" target=\"_blank\" rel=\"external\">https://github.com/yany8060/SpringDemo</a></p>\n</blockquote>\n<h4 id=\"pom-xml详情（本工程以maven方式搭建）：\"><a href=\"#pom-xml详情（本工程以maven方式搭建）：\" class=\"headerlink\" title=\"pom.xml详情（本工程以maven方式搭建）：\"></a>pom.xml详情（本工程以maven方式搭建）：</h4><p>官方建议引入父工程<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;properties&gt; </div><div class=\"line\">\t&lt;java.version&gt;1.7&lt;/java.version&gt;</div><div class=\"line\">\t&lt;spring.boot.version&gt;1.4.3.RELEASE&lt;/spring.boot.version&gt;</div><div class=\"line\">&lt;/properties&gt;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</div><div class=\"line\">\t&lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;</div><div class=\"line\">\t&lt;type&gt;pom&lt;/type&gt;</div><div class=\"line\">\t&lt;scope&gt;import&lt;/scope&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<p>在子工程中引入自己所需要的依赖（版本号有父工程管理）：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</div><div class=\"line\">    &lt;scope&gt;test&lt;/scope&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<p>并添加maven编译插件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;plugin&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</div><div class=\"line\">&lt;/plugin&gt;</div></pre></td></tr></table></figure></p>\n<p>更多SpringBoot Maven插件详见：<a href=\"http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\" target=\"_blank\" rel=\"external\">http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html</a></p>\n<h4 id=\"创建一个启动入口（应用类）\"><a href=\"#创建一个启动入口（应用类）\" class=\"headerlink\" title=\"创建一个启动入口（应用类）\"></a>创建一个启动入口（应用类）</h4><p>我们创建一个Application类：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span></div><div class=\"line\"><span class=\"meta\">@ComponentScan</span>(basePackages = <span class=\"string\">\"com.yany\"</span>)</div><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Application</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">        SpringApplication.run(Application.class);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><code>@EnableAutoConfiguration</code><br>开启自动配置<br>这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring<br><code>@ComponentScan</code><br>Spring原注解，扫描包加载bean<br><code>@Configuration</code><br>标注一个类为配置类</p>\n<p>我们再创建一个Controller层：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RestController</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Example</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@RequestMapping</span>(value = <span class=\"string\">\"/hello\"</span>, method = &#123;RequestMethod.GET&#125;)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">test</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Hello world\"</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><code>@RestController</code><br>相当于同时添加@Controller和@ResponseBody注解。</p>\n<p>这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：<a href=\"http://localhost:8080/SpringBoot/hello\" target=\"_blank\" rel=\"external\">http://localhost:8080/SpringBoot/hello</a> 来得到返回值</p>\n<h4 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h4><p>以main方法方式启动Application类，</p>\n<p>在浏览器中输入：<a href=\"http://localhost:8080/SpringBoot/hello\" target=\"_blank\" rel=\"external\">http://localhost:8080/SpringBoot/hello</a><br>得到返回：Hello world<br><img src=\"/img/work/14853239880359.jpg\" alt=\"\"></p>\n<p>具体输入日志如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">  .   ____          _            __ _ _</div><div class=\"line\"> /\\\\ / ___&apos;_ __ _ _(_)_ __  __ _ \\ \\ \\ \\</div><div class=\"line\">( ( )\\___ | &apos;_ | &apos;_| | &apos;_ \\/ _` | \\ \\ \\ \\</div><div class=\"line\"> \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )</div><div class=\"line\">  &apos;  |____| .__|_| |_|_| |_\\__, | / / / /</div><div class=\"line\"> =========|_|==============|___/=/_/_/_/</div><div class=\"line\"> :: Spring Boot ::        (v1.4.3.RELEASE)</div><div class=\"line\"></div><div class=\"line\">2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)</div><div class=\"line\">2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default</div><div class=\"line\">2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</div><div class=\"line\">2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in &apos;[com.yany]&apos; package. Please check your configuration.</div><div class=\"line\">2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &apos;org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration&apos; of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)</div><div class=\"line\">2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)</div><div class=\"line\">2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat</div><div class=\"line\">2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6</div><div class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</div><div class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms</div><div class=\"line\">2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: &apos;dispatcherServlet&apos; to [/]</div><div class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;characterEncodingFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;hiddenHttpMethodFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;httpPutFormContentFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;requestContextFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</div><div class=\"line\">2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/hello],methods=[GET]&#125;&quot; onto public java.lang.String com.yany.controller.Example.test()</div><div class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error]&#125;&quot; onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)</div><div class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error],produces=[text/html]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)</div><div class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup</div><div class=\"line\">2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)</div><div class=\"line\">2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)</div></pre></td></tr></table></figure></p>\n","excerpt":"","more":"<blockquote>\n<p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.<br>简言之：方便创建一个最小规模的Spring工程，且它只需要很少的Spring配置</p>\n</blockquote>\n<h3 id=\"特征\"><a href=\"#特征\" class=\"headerlink\" title=\"特征\"></a>特征</h3><ul>\n<li>Create stand-alone Spring applications</li>\n<li>Embed（内置、嵌入） Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>\n<li>Provide opinionated ‘starter’ POMs to simplify your Maven configuration</li>\n<li>Automatically configure Spring whenever possible</li>\n<li>Provide production-ready features such as metrics, health checks and externalized configuration（提供产品化的功能）</li>\n<li>Absolutely no code generation and no requirement for XML configuration</li>\n</ul>\n<h3 id=\"Spring-Boot工程简单搭建\"><a href=\"#Spring-Boot工程简单搭建\" class=\"headerlink\" title=\"Spring-Boot工程简单搭建\"></a>Spring-Boot工程简单搭建</h3><blockquote>\n<p>Spring-boot文档：<a href=\"http://docs.spring.io/spring-boot/docs/current/reference/html/\">http://docs.spring.io/spring-boot/docs/current/reference/html/</a><br>具体代码详见：<a href=\"https://github.com/yany8060/SpringDemo\">https://github.com/yany8060/SpringDemo</a></p>\n</blockquote>\n<h4 id=\"pom-xml详情（本工程以maven方式搭建）：\"><a href=\"#pom-xml详情（本工程以maven方式搭建）：\" class=\"headerlink\" title=\"pom.xml详情（本工程以maven方式搭建）：\"></a>pom.xml详情（本工程以maven方式搭建）：</h4><p>官方建议引入父工程<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;properties&gt; </div><div class=\"line\">\t&lt;java.version&gt;1.7&lt;/java.version&gt;</div><div class=\"line\">\t&lt;spring.boot.version&gt;1.4.3.RELEASE&lt;/spring.boot.version&gt;</div><div class=\"line\">&lt;/properties&gt;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;</div><div class=\"line\">\t&lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;</div><div class=\"line\">\t&lt;type&gt;pom&lt;/type&gt;</div><div class=\"line\">\t&lt;scope&gt;import&lt;/scope&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<p>在子工程中引入自己所需要的依赖（版本号有父工程管理）：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;</div><div class=\"line\">    &lt;scope&gt;test&lt;/scope&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<p>并添加maven编译插件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;plugin&gt;</div><div class=\"line\">\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class=\"line\">\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</div><div class=\"line\">&lt;/plugin&gt;</div></pre></td></tr></table></figure></p>\n<p>更多SpringBoot Maven插件详见：<a href=\"http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html\">http://docs.spring.io/spring-boot/docs/1.4.3.RELEASE/maven-plugin/index.html</a></p>\n<h4 id=\"创建一个启动入口（应用类）\"><a href=\"#创建一个启动入口（应用类）\" class=\"headerlink\" title=\"创建一个启动入口（应用类）\"></a>创建一个启动入口（应用类）</h4><p>我们创建一个Application类：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@EnableAutoConfiguration</span></div><div class=\"line\"><span class=\"meta\">@ComponentScan</span>(basePackages = <span class=\"string\">\"com.yany\"</span>)</div><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Application</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">        SpringApplication.run(Application.class);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><code>@EnableAutoConfiguration</code><br>开启自动配置<br>这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring<br><code>@ComponentScan</code><br>Spring原注解，扫描包加载bean<br><code>@Configuration</code><br>标注一个类为配置类</p>\n<p>我们再创建一个Controller层：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RestController</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Example</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@RequestMapping</span>(value = <span class=\"string\">\"/hello\"</span>, method = &#123;RequestMethod.GET&#125;)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> String <span class=\"title\">test</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Hello world\"</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><code>@RestController</code><br>相当于同时添加@Controller和@ResponseBody注解。</p>\n<p>这时一个基本的Spring-Boot的工程就创建完了，里面包含一个了Mapping映射，可通过访问：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a> 来得到返回值</p>\n<h4 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h4><p>以main方法方式启动Application类，</p>\n<p>在浏览器中输入：<a href=\"http://localhost:8080/SpringBoot/hello\">http://localhost:8080/SpringBoot/hello</a><br>得到返回：Hello world<br><img src=\"/img/work/14853239880359.jpg\" alt=\"\"></p>\n<p>具体输入日志如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">  .   ____          _            __ _ _</div><div class=\"line\"> /\\\\ / ___&apos;_ __ _ _(_)_ __  __ _ \\ \\ \\ \\</div><div class=\"line\">( ( )\\___ | &apos;_ | &apos;_| | &apos;_ \\/ _` | \\ \\ \\ \\</div><div class=\"line\"> \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )</div><div class=\"line\">  &apos;  |____| .__|_| |_|_| |_\\__, | / / / /</div><div class=\"line\"> =========|_|==============|___/=/_/_/_/</div><div class=\"line\"> :: Spring Boot ::        (v1.4.3.RELEASE)</div><div class=\"line\"></div><div class=\"line\">2017-01-25 13:55:36.298  INFO 30221 --- [           main] com.yany.Application                     : Starting Application on yanyongdeMacBook-Pro.local with PID 30221 (/Users/yanyong/GitHub/YanY/SpringDemo/SpringBoot/target/classes started by yanyong in /Users/yanyong/GitHub/YanY/SpringDemo)</div><div class=\"line\">2017-01-25 13:55:36.301  INFO 30221 --- [           main] com.yany.Application                     : No active profile set, falling back to default profiles: default</div><div class=\"line\">2017-01-25 13:55:36.373  INFO 30221 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</div><div class=\"line\">2017-01-25 13:55:37.264  WARN 30221 --- [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in &apos;[com.yany]&apos; package. Please check your configuration.</div><div class=\"line\">2017-01-25 13:55:37.570  INFO 30221 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean &apos;org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration&apos; of type [class org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$baf0e866] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)</div><div class=\"line\">2017-01-25 13:55:37.997  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)</div><div class=\"line\">2017-01-25 13:55:38.009  INFO 30221 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat</div><div class=\"line\">2017-01-25 13:55:38.010  INFO 30221 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.6</div><div class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</div><div class=\"line\">2017-01-25 13:55:38.136  INFO 30221 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1767 ms</div><div class=\"line\">2017-01-25 13:55:38.298  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: &apos;dispatcherServlet&apos; to [/]</div><div class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;characterEncodingFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.302  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;hiddenHttpMethodFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;httpPutFormContentFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.303  INFO 30221 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: &apos;requestContextFilter&apos; to: [/*]</div><div class=\"line\">2017-01-25 13:55:38.613  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2d8b20a0: startup date [Wed Jan 25 13:55:36 CST 2017]; root of context hierarchy</div><div class=\"line\">2017-01-25 13:55:38.674  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/hello],methods=[GET]&#125;&quot; onto public java.lang.String com.yany.controller.Example.test()</div><div class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error]&#125;&quot; onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)</div><div class=\"line\">2017-01-25 13:55:38.678  INFO 30221 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error],produces=[text/html]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)</div><div class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:38.712  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:38.752  INFO 30221 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]</div><div class=\"line\">2017-01-25 13:55:39.333  INFO 30221 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup</div><div class=\"line\">2017-01-25 13:55:39.385  INFO 30221 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)</div><div class=\"line\">2017-01-25 13:55:39.388  INFO 30221 --- [           main] com.yany.Application                     : Started Application in 3.752 seconds (JVM running for 4.078)</div></pre></td></tr></table></figure></p>\n"},{"title":"Spring-boot-MyBatis配置-2","date":"2017-02-04T08:35:04.000Z","_content":"### 多数据源配置\n1. 分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建\n2. 注解\n3. AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;\n\n\n### 分包形式\n> 不同的数据源的sql操作分布在不同的路径下\n\n两个数据源的basepackage分别为：\n* com.yany.dao.multi.ads\n* com.yany.dao.multi.rds\n\n在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径\n```java\nmapperScannerConfigurer.setBasePackage(\"xxxx\");\n```\n\n对应的xml，即MAPPER_PATH\n* classpath:/com/yany/mapper/multi/ads/**.xml\n* classpath:/com/yany/mapper/multi/rds/**.xml\n\n在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径\n```java\n sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n```\n在使用时，不同数据源的操作在分别在不同的路径创建即可。\n\n### 注解形式\n#### 准备好两个注解类，分别对应于两个数据源：\n```java\npublic @interface RdsRepository {\n}\npublic @interface AdsRepository {\n}\n```\n同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。\n#### 在创建两个MapperScannerConfigurer\n```java\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationAdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationAdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationRdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationRdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n```\n上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类\n\n#### 使用时，在不同数据源的dao接口上添加对应的注解\n\n```java\n@RdsRepository\npublic interface AnnotationRdsDao {\n    int selectCount();\n}\n@AdsRepository\npublic interface AnnotationAdsDao {\n    int selectCount();\n}\n```\n而sql对应的xml不变，对应好namespace即可\n\n### AOP形式\n#### 创建一个动态数据源\n创建数据源类型的枚举类\n```java\npublic enum DatabaseType {\n    Ads, Rds\n}\n```\n\n创建一个线程安全的DatabaseType容器\n\n```java\npublic class DatabaseContextHolder {\n    private final static ThreadLocal<DatabaseType> contextHolder = new ThreadLocal<>();\n\n    public static DatabaseType getDatabaseType() {\n        return contextHolder.get();\n    }\n\n    public static void setDatabaseType(DatabaseType type) {\n        contextHolder.set(type);\n    }\n\n}\n```\nThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。\n\n创建动态数据源DynamicDataSource继承AbstractRoutingDataSource\n```java\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n\n    @Override\n    protected Object determineCurrentLookupKey() {\n        return DatabaseContextHolder.getDatabaseType();\n    }\n}\n```\n\n#### 创建AOP对应的MyBatis配置\n  创建动态数据源的bean\n```java\n    @Bean\n    public DynamicDataSource setDataSource() {\n        Map<Object, Object> targetDataSources = new HashMap<>();\n        targetDataSources.put(DatabaseType.Ads, adsDataSource);\n        targetDataSources.put(DatabaseType.Rds, rdsDataSource);\n\n        DynamicDataSource dataSource = new DynamicDataSource();\n        dataSource.setTargetDataSources(targetDataSources);\n        dataSource.setDefaultTargetDataSource(rdsDataSource);// 默认的datasource设置为rdsDataSource\n        return dataSource;\n    }\n```\n创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码\n\n#### 创建切边\n扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。\n```java\n@Aspect\n@Component\npublic class DataSourceAspect {\n    /**\n     * 使用空方法定义切点表达式\n     */\n    @Pointcut(\"execution(* com.yany.service.**.*(..))\")\n    public void declareJointPointExpression() {\n    }\n\n    @Before(\"declareJointPointExpression()\")\n    public void setDataSourceKey(JoinPoint point) {\n        if (point.getTarget() instanceof IAdsAopService ||\n                point.getTarget() instanceof AdsAopServiceImpl) {\n            //根据连接点所属的类实例，动态切换数据源\n            System.out.println(\"IAdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);\n        } else {//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）\n            System.out.println(\"IRdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);\n        }\n\n    }\n}\n```\n上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。\n\n具体代码将github：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz","source":"_posts/Spring-boot-MyBatis配置-2.md","raw":"---\ntitle: Spring-boot-MyBatis配置-2\ndate: 2017-02-04 16:35:04\ntags: [spring-boot,spring]\ncategories: [java]\n---\n### 多数据源配置\n1. 分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建\n2. 注解\n3. AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;\n\n\n### 分包形式\n> 不同的数据源的sql操作分布在不同的路径下\n\n两个数据源的basepackage分别为：\n* com.yany.dao.multi.ads\n* com.yany.dao.multi.rds\n\n在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径\n```java\nmapperScannerConfigurer.setBasePackage(\"xxxx\");\n```\n\n对应的xml，即MAPPER_PATH\n* classpath:/com/yany/mapper/multi/ads/**.xml\n* classpath:/com/yany/mapper/multi/rds/**.xml\n\n在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径\n```java\n sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n```\n在使用时，不同数据源的操作在分别在不同的路径创建即可。\n\n### 注解形式\n#### 准备好两个注解类，分别对应于两个数据源：\n```java\npublic @interface RdsRepository {\n}\npublic @interface AdsRepository {\n}\n```\n同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是__允许相同路径__。\n#### 在创建两个MapperScannerConfigurer\n```java\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationAdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationAdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n\n    /**\n     * 以注解的方式 进行多数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createAnnotatationRdsMapperScannerConfigurer() {\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.multi.annotation\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"annotationRdsSqlSessionFactory\");\n        mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);\n        return mapperScannerConfigurer;\n    }\n```\n上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类\n\n#### 使用时，在不同数据源的dao接口上添加对应的注解\n\n```java\n@RdsRepository\npublic interface AnnotationRdsDao {\n    int selectCount();\n}\n@AdsRepository\npublic interface AnnotationAdsDao {\n    int selectCount();\n}\n```\n而sql对应的xml不变，对应好namespace即可\n\n### AOP形式\n#### 创建一个动态数据源\n创建数据源类型的枚举类\n```java\npublic enum DatabaseType {\n    Ads, Rds\n}\n```\n\n创建一个线程安全的DatabaseType容器\n\n```java\npublic class DatabaseContextHolder {\n    private final static ThreadLocal<DatabaseType> contextHolder = new ThreadLocal<>();\n\n    public static DatabaseType getDatabaseType() {\n        return contextHolder.get();\n    }\n\n    public static void setDatabaseType(DatabaseType type) {\n        contextHolder.set(type);\n    }\n\n}\n```\nThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。\n\n创建动态数据源DynamicDataSource继承AbstractRoutingDataSource\n```java\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n\n    @Override\n    protected Object determineCurrentLookupKey() {\n        return DatabaseContextHolder.getDatabaseType();\n    }\n}\n```\n\n#### 创建AOP对应的MyBatis配置\n  创建动态数据源的bean\n```java\n    @Bean\n    public DynamicDataSource setDataSource() {\n        Map<Object, Object> targetDataSources = new HashMap<>();\n        targetDataSources.put(DatabaseType.Ads, adsDataSource);\n        targetDataSources.put(DatabaseType.Rds, rdsDataSource);\n\n        DynamicDataSource dataSource = new DynamicDataSource();\n        dataSource.setTargetDataSources(targetDataSources);\n        dataSource.setDefaultTargetDataSource(rdsDataSource);// 默认的datasource设置为rdsDataSource\n        return dataSource;\n    }\n```\n创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码\n\n#### 创建切边\n扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。\n```java\n@Aspect\n@Component\npublic class DataSourceAspect {\n    /**\n     * 使用空方法定义切点表达式\n     */\n    @Pointcut(\"execution(* com.yany.service.**.*(..))\")\n    public void declareJointPointExpression() {\n    }\n\n    @Before(\"declareJointPointExpression()\")\n    public void setDataSourceKey(JoinPoint point) {\n        if (point.getTarget() instanceof IAdsAopService ||\n                point.getTarget() instanceof AdsAopServiceImpl) {\n            //根据连接点所属的类实例，动态切换数据源\n            System.out.println(\"IAdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);\n        } else {//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）\n            System.out.println(\"IRdsAopService Aspect\");\n            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);\n        }\n\n    }\n}\n```\n上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。\n\n具体代码将github：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz","slug":"Spring-boot-MyBatis配置-2","published":1,"updated":"2017-02-04T08:42:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2kd000hbc0vh8djtkco","content":"<h3 id=\"多数据源配置\"><a href=\"#多数据源配置\" class=\"headerlink\" title=\"多数据源配置\"></a>多数据源配置</h3><ol>\n<li>分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建</li>\n<li>注解</li>\n<li>AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;</li>\n</ol>\n<h3 id=\"分包形式\"><a href=\"#分包形式\" class=\"headerlink\" title=\"分包形式\"></a>分包形式</h3><blockquote>\n<p>不同的数据源的sql操作分布在不同的路径下</p>\n</blockquote>\n<p>两个数据源的basepackage分别为：</p>\n<ul>\n<li>com.yany.dao.multi.ads</li>\n<li>com.yany.dao.multi.rds</li>\n</ul>\n<p>在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"xxxx\"</span>);</div></pre></td></tr></table></figure></p>\n<p>对应的xml，即MAPPER_PATH</p>\n<ul>\n<li>classpath:/com/yany/mapper/multi/ads/**.xml</li>\n<li>classpath:/com/yany/mapper/multi/rds/**.xml</li>\n</ul>\n<p>在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</div></pre></td></tr></table></figure></p>\n<p>在使用时，不同数据源的操作在分别在不同的路径创建即可。</p>\n<h3 id=\"注解形式\"><a href=\"#注解形式\" class=\"headerlink\" title=\"注解形式\"></a>注解形式</h3><h4 id=\"准备好两个注解类，分别对应于两个数据源：\"><a href=\"#准备好两个注解类，分别对应于两个数据源：\" class=\"headerlink\" title=\"准备好两个注解类，分别对应于两个数据源：\"></a>准备好两个注解类，分别对应于两个数据源：</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> RdsRepository &#123;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> AdsRepository &#123;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是<strong>允许相同路径</strong>。</p>\n<h4 id=\"在创建两个MapperScannerConfigurer\"><a href=\"#在创建两个MapperScannerConfigurer\" class=\"headerlink\" title=\"在创建两个MapperScannerConfigurer\"></a>在创建两个MapperScannerConfigurer</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * 以注解的方式 进行多数据源配置</div><div class=\"line\"> *</div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> */</div><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createAnnotatationAdsMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.multi.annotation\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"annotationAdsSqlSessionFactory\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);</div><div class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * 以注解的方式 进行多数据源配置</div><div class=\"line\"> *</div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> */</div><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createAnnotatationRdsMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.multi.annotation\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"annotationRdsSqlSessionFactory\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);</div><div class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类</p>\n<h4 id=\"使用时，在不同数据源的dao接口上添加对应的注解\"><a href=\"#使用时，在不同数据源的dao接口上添加对应的注解\" class=\"headerlink\" title=\"使用时，在不同数据源的dao接口上添加对应的注解\"></a>使用时，在不同数据源的dao接口上添加对应的注解</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RdsRepository</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">AnnotationRdsDao</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">selectCount</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"meta\">@AdsRepository</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">AnnotationAdsDao</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">selectCount</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>而sql对应的xml不变，对应好namespace即可</p>\n<h3 id=\"AOP形式\"><a href=\"#AOP形式\" class=\"headerlink\" title=\"AOP形式\"></a>AOP形式</h3><h4 id=\"创建一个动态数据源\"><a href=\"#创建一个动态数据源\" class=\"headerlink\" title=\"创建一个动态数据源\"></a>创建一个动态数据源</h4><p>创建数据源类型的枚举类<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">enum</span> DatabaseType &#123;</div><div class=\"line\">    Ads, Rds</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>创建一个线程安全的DatabaseType容器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DatabaseContextHolder</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> ThreadLocal&lt;DatabaseType&gt; contextHolder = <span class=\"keyword\">new</span> ThreadLocal&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> DatabaseType <span class=\"title\">getDatabaseType</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> contextHolder.get();</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">setDatabaseType</span><span class=\"params\">(DatabaseType type)</span> </span>&#123;</div><div class=\"line\">        contextHolder.set(type);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>ThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。</p>\n<p>创建动态数据源DynamicDataSource继承AbstractRoutingDataSource<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DynamicDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title\">AbstractRoutingDataSource</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> Object <span class=\"title\">determineCurrentLookupKey</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> DatabaseContextHolder.getDatabaseType();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"创建AOP对应的MyBatis配置\"><a href=\"#创建AOP对应的MyBatis配置\" class=\"headerlink\" title=\"创建AOP对应的MyBatis配置\"></a>创建AOP对应的MyBatis配置</h4><p>  创建动态数据源的bean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DynamicDataSource <span class=\"title\">setDataSource</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    Map&lt;Object, Object&gt; targetDataSources = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</div><div class=\"line\">    targetDataSources.put(DatabaseType.Ads, adsDataSource);</div><div class=\"line\">    targetDataSources.put(DatabaseType.Rds, rdsDataSource);</div><div class=\"line\"></div><div class=\"line\">    DynamicDataSource dataSource = <span class=\"keyword\">new</span> DynamicDataSource();</div><div class=\"line\">    dataSource.setTargetDataSources(targetDataSources);</div><div class=\"line\">    dataSource.setDefaultTargetDataSource(rdsDataSource);<span class=\"comment\">// 默认的datasource设置为rdsDataSource</span></div><div class=\"line\">    <span class=\"keyword\">return</span> dataSource;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码</p>\n<h4 id=\"创建切边\"><a href=\"#创建切边\" class=\"headerlink\" title=\"创建切边\"></a>创建切边</h4><p>扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Aspect</span></div><div class=\"line\"><span class=\"meta\">@Component</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataSourceAspect</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\">     * 使用空方法定义切点表达式</div><div class=\"line\">     */</div><div class=\"line\">    <span class=\"meta\">@Pointcut</span>(<span class=\"string\">\"execution(* com.yany.service.**.*(..))\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareJointPointExpression</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Before</span>(<span class=\"string\">\"declareJointPointExpression()\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDataSourceKey</span><span class=\"params\">(JoinPoint point)</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (point.getTarget() <span class=\"keyword\">instanceof</span> IAdsAopService ||</div><div class=\"line\">                point.getTarget() <span class=\"keyword\">instanceof</span> AdsAopServiceImpl) &#123;</div><div class=\"line\">            <span class=\"comment\">//根据连接点所属的类实例，动态切换数据源</span></div><div class=\"line\">            System.out.println(<span class=\"string\">\"IAdsAopService Aspect\"</span>);</div><div class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）</span></div><div class=\"line\">            System.out.println(<span class=\"string\">\"IRdsAopService Aspect\"</span>);</div><div class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。</p>\n<p>具体代码将github：<a href=\"https://github.com/yany8060/SpringDemo.git\" target=\"_blank\" rel=\"external\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz\" target=\"_blank\" rel=\"external\">http://yany8060.xyz</a></p>\n","excerpt":"","more":"<h3 id=\"多数据源配置\"><a href=\"#多数据源配置\" class=\"headerlink\" title=\"多数据源配置\"></a>多数据源配置</h3><ol>\n<li>分包: 不同数据源的在不同的目录下;事务的回滚需要创建根据数据源创建</li>\n<li>注解</li>\n<li>AOP: aop注解切面需要在Service层进行数据源切换;事务可以将多个数据源放在一个事务中;</li>\n</ol>\n<h3 id=\"分包形式\"><a href=\"#分包形式\" class=\"headerlink\" title=\"分包形式\"></a>分包形式</h3><blockquote>\n<p>不同的数据源的sql操作分布在不同的路径下</p>\n</blockquote>\n<p>两个数据源的basepackage分别为：</p>\n<ul>\n<li>com.yany.dao.multi.ads</li>\n<li>com.yany.dao.multi.rds</li>\n</ul>\n<p>在创建MapperScannerConfigurer时，对应不同的数据源扫描不同的basepackage路径<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"xxxx\"</span>);</div></pre></td></tr></table></figure></p>\n<p>对应的xml，即MAPPER_PATH</p>\n<ul>\n<li>classpath:/com/yany/mapper/multi/ads/**.xml</li>\n<li>classpath:/com/yany/mapper/multi/rds/**.xml</li>\n</ul>\n<p>在创建SqlSessionFactoryBean时，MAPPER_PATH对应分别对应于ads和rds的sql路径<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sessionFactory.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</div></pre></td></tr></table></figure></p>\n<p>在使用时，不同数据源的操作在分别在不同的路径创建即可。</p>\n<h3 id=\"注解形式\"><a href=\"#注解形式\" class=\"headerlink\" title=\"注解形式\"></a>注解形式</h3><h4 id=\"准备好两个注解类，分别对应于两个数据源：\"><a href=\"#准备好两个注解类，分别对应于两个数据源：\" class=\"headerlink\" title=\"准备好两个注解类，分别对应于两个数据源：\"></a>准备好两个注解类，分别对应于两个数据源：</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> RdsRepository &#123;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"meta\">@interface</span> AdsRepository &#123;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>同分包类似分别为Rds和Ads两个数据源创建两个SqlSessionFactoryBean和DataSourceTransactionManager，略微不同的是SqlSessionFactoryBean的setMapperLocations是<strong>允许相同路径</strong>。</p>\n<h4 id=\"在创建两个MapperScannerConfigurer\"><a href=\"#在创建两个MapperScannerConfigurer\" class=\"headerlink\" title=\"在创建两个MapperScannerConfigurer\"></a>在创建两个MapperScannerConfigurer</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * 以注解的方式 进行多数据源配置</div><div class=\"line\"> *</div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createAnnotatationAdsMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.multi.annotation\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"annotationAdsSqlSessionFactory\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setAnnotationClass(AdsRepository.class);</div><div class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * 以注解的方式 进行多数据源配置</div><div class=\"line\"> *</div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createAnnotatationRdsMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">    mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.multi.annotation\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"annotationRdsSqlSessionFactory\"</span>);</div><div class=\"line\">    mapperScannerConfigurer.setAnnotationClass(RdsRepository.class);</div><div class=\"line\">    <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述代码和以前的主要的区别在：setAnnotationClass设置对应不同的注解类</p>\n<h4 id=\"使用时，在不同数据源的dao接口上添加对应的注解\"><a href=\"#使用时，在不同数据源的dao接口上添加对应的注解\" class=\"headerlink\" title=\"使用时，在不同数据源的dao接口上添加对应的注解\"></a>使用时，在不同数据源的dao接口上添加对应的注解</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RdsRepository</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">AnnotationRdsDao</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">selectCount</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"meta\">@AdsRepository</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">interface</span> <span class=\"title\">AnnotationAdsDao</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">selectCount</span><span class=\"params\">()</span></span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>而sql对应的xml不变，对应好namespace即可</p>\n<h3 id=\"AOP形式\"><a href=\"#AOP形式\" class=\"headerlink\" title=\"AOP形式\"></a>AOP形式</h3><h4 id=\"创建一个动态数据源\"><a href=\"#创建一个动态数据源\" class=\"headerlink\" title=\"创建一个动态数据源\"></a>创建一个动态数据源</h4><p>创建数据源类型的枚举类<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">enum</span> DatabaseType &#123;</div><div class=\"line\">    Ads, Rds</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>创建一个线程安全的DatabaseType容器</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DatabaseContextHolder</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> ThreadLocal&lt;DatabaseType&gt; contextHolder = <span class=\"keyword\">new</span> ThreadLocal&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> DatabaseType <span class=\"title\">getDatabaseType</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> contextHolder.get();</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">setDatabaseType</span><span class=\"params\">(DatabaseType type)</span> </span>&#123;</div><div class=\"line\">        contextHolder.set(type);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>ThreadLocal类为每一个线程都维护了自己独有的变量拷贝，每个线程都拥有了自己独立的一个变量，避免并发问题。</p>\n<p>创建动态数据源DynamicDataSource继承AbstractRoutingDataSource<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DynamicDataSource</span> <span class=\"keyword\">extends</span> <span class=\"title\">AbstractRoutingDataSource</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> Object <span class=\"title\">determineCurrentLookupKey</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> DatabaseContextHolder.getDatabaseType();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"创建AOP对应的MyBatis配置\"><a href=\"#创建AOP对应的MyBatis配置\" class=\"headerlink\" title=\"创建AOP对应的MyBatis配置\"></a>创建AOP对应的MyBatis配置</h4><p>  创建动态数据源的bean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Bean</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DynamicDataSource <span class=\"title\">setDataSource</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    Map&lt;Object, Object&gt; targetDataSources = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</div><div class=\"line\">    targetDataSources.put(DatabaseType.Ads, adsDataSource);</div><div class=\"line\">    targetDataSources.put(DatabaseType.Rds, rdsDataSource);</div><div class=\"line\"></div><div class=\"line\">    DynamicDataSource dataSource = <span class=\"keyword\">new</span> DynamicDataSource();</div><div class=\"line\">    dataSource.setTargetDataSources(targetDataSources);</div><div class=\"line\">    dataSource.setDefaultTargetDataSource(rdsDataSource);<span class=\"comment\">// 默认的datasource设置为rdsDataSource</span></div><div class=\"line\">    <span class=\"keyword\">return</span> dataSource;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>创建SqlSessionFactoryBean和DataSourceTransactionManager以及这个和以前类似不再赘述，具体看github上代码</p>\n<h4 id=\"创建切边\"><a href=\"#创建切边\" class=\"headerlink\" title=\"创建切边\"></a>创建切边</h4><p>扫描对应的Service层，在执行具体的服务代码前，根据调用的Service类进行数据源的切换。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Aspect</span></div><div class=\"line\"><span class=\"meta\">@Component</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataSourceAspect</span> </span>&#123;</div><div class=\"line\">    <span class=\"comment\">/**</div><div class=\"line\">     * 使用空方法定义切点表达式</div><div class=\"line\">     */</span></div><div class=\"line\">    <span class=\"meta\">@Pointcut</span>(<span class=\"string\">\"execution(* com.yany.service.**.*(..))\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">declareJointPointExpression</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Before</span>(<span class=\"string\">\"declareJointPointExpression()\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setDataSourceKey</span><span class=\"params\">(JoinPoint point)</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (point.getTarget() <span class=\"keyword\">instanceof</span> IAdsAopService ||</div><div class=\"line\">                point.getTarget() <span class=\"keyword\">instanceof</span> AdsAopServiceImpl) &#123;</div><div class=\"line\">            <span class=\"comment\">//根据连接点所属的类实例，动态切换数据源</span></div><div class=\"line\">            System.out.println(<span class=\"string\">\"IAdsAopService Aspect\"</span>);</div><div class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Ads);</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//连接点所属的类实例是（当然，这一步也可以不写，因为defaultTargertDataSource就是该类所用的rdsDataSource）</span></div><div class=\"line\">            System.out.println(<span class=\"string\">\"IRdsAopService Aspect\"</span>);</div><div class=\"line\">            DatabaseContextHolder.setDatabaseType(DatabaseType.Rds);</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>上述切换规则比较简单，具体可根据业务情况，包目录结构，或者是类名规则等进行解析切换。</p>\n<p>具体代码将github：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz\">http://yany8060.xyz</a></p>\n"},{"title":"Spring-boot MyBatis配置-1","date":"2017-02-04T03:08:22.000Z","_content":"> 本例使用mysql作为数据库，使用druid作为数据库连接池\n> 主要有单数据源和多数据源实例\n> 多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式\n\n### 项目目录结构\n![catalog.png](http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324)\n\n[comment]: <> ( ![](/img/work/catalog.png) )\n\n### MyBatis配置实现\n> springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。\n\n1. DataSource的bean，主要配置数据来源\n2. SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加\n3. MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean\n4. DataSourceTransactionManager的bean，主要用设置事务\n\n### 添加maven依赖\n```\n        <!-- aop -->        \n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.4</version>\n        </dependency>\n        <!-- dataSource start -->\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n            <version>1.2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.0.20</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.pagehelper</groupId>\n            <artifactId>pagehelper</artifactId>\n            <version>5.0.0</version>\n        </dependency>\n        <!-- dataSource end -->\n```\n\n### 单数据源\n#### 基本配置\n在application.yml中添加datasource配置：\n```\nspring:\n  application:\n    name: SpringBoot\n  datasource:\n    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&characterEncoding=UTF-8&&useSSL=false\n    username: root\n    password: *****\n    driver-class-name: com.mysql.jdbc.Driver\n    # 使用druid数据源\n    type: com.alibaba.druid.pool.DruidDataSource\n    # 初始化大小，最小，最大\n    initialSize: 5\n    minIdle: 5\n    maxActive: 20\n    # 配置获取连接等待超时的时间\n    maxWait: 60000\n    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒\n    timeBetweenEvictionRunsMillis: 60000\n    # 配置一个连接在池中最小生存的时间，单位是毫秒\n    minEvictableIdleTimeMillis: 300000\n    validationQuery: SELECT 1 FROM DUAL\n    testWhileIdle: true\n    testOnBorrow: false\n    testOnReturn: false\n    # 打开PSCache，并且指定每个连接上PSCache的大小\n    poolPreparedStatements: true\n    maxPoolPreparedStatementPerConnectionSize: 20\n    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙\n    filters: stat,wall,log4j\n    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录\n    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000\n    # 合并多个DruidDataSource的监控数据\n    useGlobalDataSourceStat: true\n```\n配置mybatis-config:\n以下只是实例，可自定义添加一些别的配置\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <!-- 全局参数 -->\n    <settings>\n        <!-- 使全局的映射器启用或禁用缓存。 -->\n        <setting name=\"cacheEnabled\" value=\"true\"/>\n        <!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 -->\n        <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n        <!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 -->\n        <setting name=\"aggressiveLazyLoading\" value=\"true\"/>\n        <!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true -->\n        <setting name=\"multipleResultSetsEnabled\" value=\"true\"/>\n        <!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true -->\n        <setting name=\"useColumnLabel\" value=\"true\"/>\n        <!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  -->\n        <setting name=\"useGeneratedKeys\" value=\"true\"/>\n        <!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  -->\n        <setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/>\n        <!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  -->\n        <setting name=\"defaultExecutorType\" value=\"SIMPLE\"/>\n        <!-- 使用驼峰命名法转换字段。 -->\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n        <!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session -->\n        <setting name=\"localCacheScope\" value=\"SESSION\"/>\n        <!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 -->\n        <setting name=\"jdbcTypeForNull\" value=\"NULL\"/>\n    </settings>\n</configuration>\n```\n#### 代码实现\n##### SingleMyBatisConfig的类实现\n继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。\n```java\n@Configuration\npublic class SingleMyBatisConfig implements EnvironmentAware{\n  private final static Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);\n    private static String MYBATIS_CONFIG = \"mybatis-config.xml\";\n    //mybatis mapper resource 路径\n    private static String MAPPER_PATH = \"classpath:/com/yany/mapper/single/**.xml\";\n    \n    private RelaxedPropertyResolver propertyResolver;\n    @Override\n    public void setEnvironment(Environment environment) {\n        this.propertyResolver = new RelaxedPropertyResolver(environment, \"spring.datasource.\");\n    }\n\n  .......\n}\n```\n添加@Bean(name = \"singleDataSource\")，设置实现DataSource的bean\n```java\n    /**\n     * @return\n     * @Primary 优先方案，被注解的实现，优先被注入\n     */\n    @Primary\n    @Bean(name = \"singleDataSource\")\n    public DataSource singleDataSource() {\n        logger.info(\"datasource url:{}\", propertyResolver.getProperty(\"url\"));\n\n        DruidDataSource datasource = new DruidDataSource();\n        datasource.setUrl(propertyResolver.getProperty(\"url\"));\n        datasource.setDriverClassName(propertyResolver.getProperty(\"driver-class-name\"));\n        datasource.setUsername(propertyResolver.getProperty(\"username\"));\n        datasource.setPassword(propertyResolver.getProperty(\"password\"));\n\n\n        datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(\"initialSize\")));\n        datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(\"minIdle\")));\n        datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(\"maxWait\")));\n        datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(\"maxActive\")));\n        datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(\"timeBetweenEvictionRunsMillis\")));\n        datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(\"minEvictableIdleTimeMillis\")));\n        datasource.setValidationQuery(propertyResolver.getProperty(\"validationQuery\"));\n        datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(\"testWhileIdle\")));\n        datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnBorrow\")));\n        datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnReturn\")));\n        datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(\"poolPreparedStatements\")));\n        datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(\"maxPoolPreparedStatementPerConnectionSize\")));\n\n        try {\n            datasource.setFilters(propertyResolver.getProperty(\"filters\"));\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return datasource;\n\n    }\n```\n添加@Bean(name = \"singleSqlSessionFactory\")，设置实现SqlSessionFactoryBean\n```java\n/**\n     * 创建sqlSessionFactory实例\n     *\n     * @return\n     */\n    @Bean(name = \"singleSqlSessionFactory\")\n    @Primary\n    public SqlSessionFactoryBean createSqlSessionFactoryBean(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws IOException {\n        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();\n        //设置mybatis configuration 扫描路径\n        sqlSessionFactoryBean.setConfigLocation(new ClassPathResource(MYBATIS_CONFIG));\n        sqlSessionFactoryBean.setDataSource(singleDataSource);\n\n        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = new PathMatchingResourcePatternResolver();\n        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n        return sqlSessionFactoryBean;\n    }\n```\n添加@Bean(name = \"singleTransactionManager\")，设置实现事务DataSourceTransactionManager\n```java\n    /**\n     * 配置事务管理器\n     */\n    @Bean(name = \"singleTransactionManager\")\n    @Primary\n    public DataSourceTransactionManager transactionManager(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws Exception {\n        return new DataSourceTransactionManager(singleDataSource);\n    }\n```\n以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。\n\n#### MapperScannerConfig\n本来主要集中实现各个数据源的MapperScannerConfigurer\n```java\n@Configuration\npublic class MapperScannerConfig {\n\n    /**\n     * 单数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createSingleMapperScannerConfigurer() {\n        System.out.println(\"singleDataSource\");\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.single\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"singleSqlSessionFactory\");\n        return mapperScannerConfigurer;\n    }\n}\n```\n\n以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：https://github.com/yany8060/SpringDemo.git\ncom.yany.dao.single中编写单属于的到接口\ncom.yany.mapper.single中编写对应的sql的xml\n\n****\n由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码\n详情请见：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz\n","source":"_posts/Spring-boot-MyBatis配置-1.md","raw":"---\ntitle: Spring-boot MyBatis配置-1\ndate: 2017-02-04 11:08:22\ntags: [spring-boot,spring]\ncategories: [java]\n---\n> 本例使用mysql作为数据库，使用druid作为数据库连接池\n> 主要有单数据源和多数据源实例\n> 多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式\n\n### 项目目录结构\n![catalog.png](http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324)\n\n[comment]: <> ( ![](/img/work/catalog.png) )\n\n### MyBatis配置实现\n> springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。\n\n1. DataSource的bean，主要配置数据来源\n2. SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加\n3. MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean\n4. DataSourceTransactionManager的bean，主要用设置事务\n\n### 添加maven依赖\n```\n        <!-- aop -->        \n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.4</version>\n        </dependency>\n        <!-- dataSource start -->\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n            <version>1.2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n            <version>1.0.20</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.pagehelper</groupId>\n            <artifactId>pagehelper</artifactId>\n            <version>5.0.0</version>\n        </dependency>\n        <!-- dataSource end -->\n```\n\n### 单数据源\n#### 基本配置\n在application.yml中添加datasource配置：\n```\nspring:\n  application:\n    name: SpringBoot\n  datasource:\n    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&characterEncoding=UTF-8&&useSSL=false\n    username: root\n    password: *****\n    driver-class-name: com.mysql.jdbc.Driver\n    # 使用druid数据源\n    type: com.alibaba.druid.pool.DruidDataSource\n    # 初始化大小，最小，最大\n    initialSize: 5\n    minIdle: 5\n    maxActive: 20\n    # 配置获取连接等待超时的时间\n    maxWait: 60000\n    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒\n    timeBetweenEvictionRunsMillis: 60000\n    # 配置一个连接在池中最小生存的时间，单位是毫秒\n    minEvictableIdleTimeMillis: 300000\n    validationQuery: SELECT 1 FROM DUAL\n    testWhileIdle: true\n    testOnBorrow: false\n    testOnReturn: false\n    # 打开PSCache，并且指定每个连接上PSCache的大小\n    poolPreparedStatements: true\n    maxPoolPreparedStatementPerConnectionSize: 20\n    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙\n    filters: stat,wall,log4j\n    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录\n    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000\n    # 合并多个DruidDataSource的监控数据\n    useGlobalDataSourceStat: true\n```\n配置mybatis-config:\n以下只是实例，可自定义添加一些别的配置\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <!-- 全局参数 -->\n    <settings>\n        <!-- 使全局的映射器启用或禁用缓存。 -->\n        <setting name=\"cacheEnabled\" value=\"true\"/>\n        <!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 -->\n        <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n        <!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 -->\n        <setting name=\"aggressiveLazyLoading\" value=\"true\"/>\n        <!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true -->\n        <setting name=\"multipleResultSetsEnabled\" value=\"true\"/>\n        <!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true -->\n        <setting name=\"useColumnLabel\" value=\"true\"/>\n        <!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  -->\n        <setting name=\"useGeneratedKeys\" value=\"true\"/>\n        <!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  -->\n        <setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/>\n        <!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  -->\n        <setting name=\"defaultExecutorType\" value=\"SIMPLE\"/>\n        <!-- 使用驼峰命名法转换字段。 -->\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n        <!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session -->\n        <setting name=\"localCacheScope\" value=\"SESSION\"/>\n        <!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 -->\n        <setting name=\"jdbcTypeForNull\" value=\"NULL\"/>\n    </settings>\n</configuration>\n```\n#### 代码实现\n##### SingleMyBatisConfig的类实现\n继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。\n```java\n@Configuration\npublic class SingleMyBatisConfig implements EnvironmentAware{\n  private final static Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);\n    private static String MYBATIS_CONFIG = \"mybatis-config.xml\";\n    //mybatis mapper resource 路径\n    private static String MAPPER_PATH = \"classpath:/com/yany/mapper/single/**.xml\";\n    \n    private RelaxedPropertyResolver propertyResolver;\n    @Override\n    public void setEnvironment(Environment environment) {\n        this.propertyResolver = new RelaxedPropertyResolver(environment, \"spring.datasource.\");\n    }\n\n  .......\n}\n```\n添加@Bean(name = \"singleDataSource\")，设置实现DataSource的bean\n```java\n    /**\n     * @return\n     * @Primary 优先方案，被注解的实现，优先被注入\n     */\n    @Primary\n    @Bean(name = \"singleDataSource\")\n    public DataSource singleDataSource() {\n        logger.info(\"datasource url:{}\", propertyResolver.getProperty(\"url\"));\n\n        DruidDataSource datasource = new DruidDataSource();\n        datasource.setUrl(propertyResolver.getProperty(\"url\"));\n        datasource.setDriverClassName(propertyResolver.getProperty(\"driver-class-name\"));\n        datasource.setUsername(propertyResolver.getProperty(\"username\"));\n        datasource.setPassword(propertyResolver.getProperty(\"password\"));\n\n\n        datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(\"initialSize\")));\n        datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(\"minIdle\")));\n        datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(\"maxWait\")));\n        datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(\"maxActive\")));\n        datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(\"timeBetweenEvictionRunsMillis\")));\n        datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(\"minEvictableIdleTimeMillis\")));\n        datasource.setValidationQuery(propertyResolver.getProperty(\"validationQuery\"));\n        datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(\"testWhileIdle\")));\n        datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnBorrow\")));\n        datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(\"testOnReturn\")));\n        datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(\"poolPreparedStatements\")));\n        datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(\"maxPoolPreparedStatementPerConnectionSize\")));\n\n        try {\n            datasource.setFilters(propertyResolver.getProperty(\"filters\"));\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return datasource;\n\n    }\n```\n添加@Bean(name = \"singleSqlSessionFactory\")，设置实现SqlSessionFactoryBean\n```java\n/**\n     * 创建sqlSessionFactory实例\n     *\n     * @return\n     */\n    @Bean(name = \"singleSqlSessionFactory\")\n    @Primary\n    public SqlSessionFactoryBean createSqlSessionFactoryBean(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws IOException {\n        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();\n        //设置mybatis configuration 扫描路径\n        sqlSessionFactoryBean.setConfigLocation(new ClassPathResource(MYBATIS_CONFIG));\n        sqlSessionFactoryBean.setDataSource(singleDataSource);\n\n        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = new PathMatchingResourcePatternResolver();\n        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));\n        return sqlSessionFactoryBean;\n    }\n```\n添加@Bean(name = \"singleTransactionManager\")，设置实现事务DataSourceTransactionManager\n```java\n    /**\n     * 配置事务管理器\n     */\n    @Bean(name = \"singleTransactionManager\")\n    @Primary\n    public DataSourceTransactionManager transactionManager(@Qualifier(\"singleDataSource\") DataSource singleDataSource) throws Exception {\n        return new DataSourceTransactionManager(singleDataSource);\n    }\n```\n以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。\n\n#### MapperScannerConfig\n本来主要集中实现各个数据源的MapperScannerConfigurer\n```java\n@Configuration\npublic class MapperScannerConfig {\n\n    /**\n     * 单数据源配置\n     *\n     * @return\n     */\n    @Bean\n    public MapperScannerConfigurer createSingleMapperScannerConfigurer() {\n        System.out.println(\"singleDataSource\");\n        MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer();\n        mapperScannerConfigurer.setBasePackage(\"com.yany.dao.single\");\n        mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"singleSqlSessionFactory\");\n        return mapperScannerConfigurer;\n    }\n}\n```\n\n以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：https://github.com/yany8060/SpringDemo.git\ncom.yany.dao.single中编写单属于的到接口\ncom.yany.mapper.single中编写对应的sql的xml\n\n****\n由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码\n详情请见：https://github.com/yany8060/SpringDemo.git\n博客：http://yany8060.xyz\n","slug":"Spring-boot-MyBatis配置-1","published":1,"updated":"2017-02-04T08:38:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciz42l2kf000jbc0v4ii6qhr3","content":"<blockquote>\n<p>本例使用mysql作为数据库，使用druid作为数据库连接池<br>主要有单数据源和多数据源实例<br>多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式</p>\n</blockquote>\n<h3 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324\" alt=\"catalog.png\"></p>\n<h3 id=\"MyBatis配置实现\"><a href=\"#MyBatis配置实现\" class=\"headerlink\" title=\"MyBatis配置实现\"></a>MyBatis配置实现</h3><blockquote>\n<p>springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。</p>\n</blockquote>\n<ol>\n<li>DataSource的bean，主要配置数据来源</li>\n<li>SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加</li>\n<li>MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean</li>\n<li>DataSourceTransactionManager的bean，主要用设置事务</li>\n</ol>\n<h3 id=\"添加maven依赖\"><a href=\"#添加maven依赖\" class=\"headerlink\" title=\"添加maven依赖\"></a>添加maven依赖</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;!-- aop --&gt;        </div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.8.4&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;!-- dataSource start --&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.2.0&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;5.1.38&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;druid&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.0.20&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;5.0.0&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;!-- dataSource end --&gt;</div></pre></td></tr></table></figure>\n<h3 id=\"单数据源\"><a href=\"#单数据源\" class=\"headerlink\" title=\"单数据源\"></a>单数据源</h3><h4 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h4><p>在application.yml中添加datasource配置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring:</div><div class=\"line\">  application:</div><div class=\"line\">    name: SpringBoot</div><div class=\"line\">  datasource:</div><div class=\"line\">    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=false</div><div class=\"line\">    username: root</div><div class=\"line\">    password: *****</div><div class=\"line\">    driver-class-name: com.mysql.jdbc.Driver</div><div class=\"line\">    # 使用druid数据源</div><div class=\"line\">    type: com.alibaba.druid.pool.DruidDataSource</div><div class=\"line\">    # 初始化大小，最小，最大</div><div class=\"line\">    initialSize: 5</div><div class=\"line\">    minIdle: 5</div><div class=\"line\">    maxActive: 20</div><div class=\"line\">    # 配置获取连接等待超时的时间</div><div class=\"line\">    maxWait: 60000</div><div class=\"line\">    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒</div><div class=\"line\">    timeBetweenEvictionRunsMillis: 60000</div><div class=\"line\">    # 配置一个连接在池中最小生存的时间，单位是毫秒</div><div class=\"line\">    minEvictableIdleTimeMillis: 300000</div><div class=\"line\">    validationQuery: SELECT 1 FROM DUAL</div><div class=\"line\">    testWhileIdle: true</div><div class=\"line\">    testOnBorrow: false</div><div class=\"line\">    testOnReturn: false</div><div class=\"line\">    # 打开PSCache，并且指定每个连接上PSCache的大小</div><div class=\"line\">    poolPreparedStatements: true</div><div class=\"line\">    maxPoolPreparedStatementPerConnectionSize: 20</div><div class=\"line\">    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&apos;wall&apos;用于防火墙</div><div class=\"line\">    filters: stat,wall,log4j</div><div class=\"line\">    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录</div><div class=\"line\">    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000</div><div class=\"line\">    # 合并多个DruidDataSource的监控数据</div><div class=\"line\">    useGlobalDataSourceStat: true</div></pre></td></tr></table></figure></p>\n<p>配置mybatis-config:<br>以下只是实例，可自定义添加一些别的配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;</div><div class=\"line\">        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;</div><div class=\"line\">&lt;configuration&gt;</div><div class=\"line\">    &lt;!-- 全局参数 --&gt;</div><div class=\"line\">    &lt;settings&gt;</div><div class=\"line\">        &lt;!-- 使全局的映射器启用或禁用缓存。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true --&gt;</div><div class=\"line\">        &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true --&gt;</div><div class=\"line\">        &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt;</div><div class=\"line\">        &lt;!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt;</div><div class=\"line\">        &lt;!-- 使用驼峰命名法转换字段。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session --&gt;</div><div class=\"line\">        &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</div><div class=\"line\">        &lt;!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;NULL&quot;/&gt;</div><div class=\"line\">    &lt;/settings&gt;</div><div class=\"line\">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><h5 id=\"SingleMyBatisConfig的类实现\"><a href=\"#SingleMyBatisConfig的类实现\" class=\"headerlink\" title=\"SingleMyBatisConfig的类实现\"></a>SingleMyBatisConfig的类实现</h5><p>继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SingleMyBatisConfig</span> <span class=\"keyword\">implements</span> <span class=\"title\">EnvironmentAware</span></span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);</div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String MYBATIS_CONFIG = <span class=\"string\">\"mybatis-config.xml\"</span>;</div><div class=\"line\">    <span class=\"comment\">//mybatis mapper resource 路径</span></div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String MAPPER_PATH = <span class=\"string\">\"classpath:/com/yany/mapper/single/**.xml\"</span>;</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"keyword\">private</span> RelaxedPropertyResolver propertyResolver;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setEnvironment</span><span class=\"params\">(Environment environment)</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>.propertyResolver = <span class=\"keyword\">new</span> RelaxedPropertyResolver(environment, <span class=\"string\">\"spring.datasource.\"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">  .......</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleDataSource”)，设置实现DataSource的bean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> * <span class=\"doctag\">@Primary</span> 优先方案，被注解的实现，优先被注入</div><div class=\"line\"> */</div><div class=\"line\"><span class=\"meta\">@Primary</span></div><div class=\"line\"><span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleDataSource\"</span>)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">singleDataSource</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    logger.info(<span class=\"string\">\"datasource url:&#123;&#125;\"</span>, propertyResolver.getProperty(<span class=\"string\">\"url\"</span>));</div><div class=\"line\"></div><div class=\"line\">    DruidDataSource datasource = <span class=\"keyword\">new</span> DruidDataSource();</div><div class=\"line\">    datasource.setUrl(propertyResolver.getProperty(<span class=\"string\">\"url\"</span>));</div><div class=\"line\">    datasource.setDriverClassName(propertyResolver.getProperty(<span class=\"string\">\"driver-class-name\"</span>));</div><div class=\"line\">    datasource.setUsername(propertyResolver.getProperty(<span class=\"string\">\"username\"</span>));</div><div class=\"line\">    datasource.setPassword(propertyResolver.getProperty(<span class=\"string\">\"password\"</span>));</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"initialSize\"</span>)));</div><div class=\"line\">    datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"minIdle\"</span>)));</div><div class=\"line\">    datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxWait\"</span>)));</div><div class=\"line\">    datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxActive\"</span>)));</div><div class=\"line\">    datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"timeBetweenEvictionRunsMillis\"</span>)));</div><div class=\"line\">    datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"minEvictableIdleTimeMillis\"</span>)));</div><div class=\"line\">    datasource.setValidationQuery(propertyResolver.getProperty(<span class=\"string\">\"validationQuery\"</span>));</div><div class=\"line\">    datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testWhileIdle\"</span>)));</div><div class=\"line\">    datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testOnBorrow\"</span>)));</div><div class=\"line\">    datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testOnReturn\"</span>)));</div><div class=\"line\">    datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"poolPreparedStatements\"</span>)));</div><div class=\"line\">    datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxPoolPreparedStatementPerConnectionSize\"</span>)));</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        datasource.setFilters(propertyResolver.getProperty(<span class=\"string\">\"filters\"</span>));</div><div class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</div><div class=\"line\">        e.printStackTrace();</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> datasource;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleSqlSessionFactory”)，设置实现SqlSessionFactoryBean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">     * 创建sqlSessionFactory实例</div><div class=\"line\">     *</div><div class=\"line\">     * <span class=\"doctag\">@return</span></div><div class=\"line\">     */</div><div class=\"line\">    <span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleSqlSessionFactory\"</span>)</div><div class=\"line\">    <span class=\"meta\">@Primary</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> SqlSessionFactoryBean <span class=\"title\">createSqlSessionFactoryBean</span><span class=\"params\">(@Qualifier(<span class=\"string\">\"singleDataSource\"</span>)</span> DataSource singleDataSource) <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">        SqlSessionFactoryBean sqlSessionFactoryBean = <span class=\"keyword\">new</span> SqlSessionFactoryBean();</div><div class=\"line\">        <span class=\"comment\">//设置mybatis configuration 扫描路径</span></div><div class=\"line\">        sqlSessionFactoryBean.setConfigLocation(<span class=\"keyword\">new</span> ClassPathResource(MYBATIS_CONFIG));</div><div class=\"line\">        sqlSessionFactoryBean.setDataSource(singleDataSource);</div><div class=\"line\"></div><div class=\"line\">        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = <span class=\"keyword\">new</span> PathMatchingResourcePatternResolver();</div><div class=\"line\">        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</div><div class=\"line\">        <span class=\"keyword\">return</span> sqlSessionFactoryBean;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleTransactionManager”)，设置实现事务DataSourceTransactionManager<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * 配置事务管理器</div><div class=\"line\"> */</div><div class=\"line\"><span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleTransactionManager\"</span>)</div><div class=\"line\"><span class=\"meta\">@Primary</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DataSourceTransactionManager <span class=\"title\">transactionManager</span><span class=\"params\">(@Qualifier(<span class=\"string\">\"singleDataSource\"</span>)</span> DataSource singleDataSource) <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> DataSourceTransactionManager(singleDataSource);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。</p>\n<h4 id=\"MapperScannerConfig\"><a href=\"#MapperScannerConfig\" class=\"headerlink\" title=\"MapperScannerConfig\"></a>MapperScannerConfig</h4><p>本来主要集中实现各个数据源的MapperScannerConfigurer<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapperScannerConfig</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\">     * 单数据源配置</div><div class=\"line\">     *</div><div class=\"line\">     * <span class=\"doctag\">@return</span></div><div class=\"line\">     */</div><div class=\"line\">    <span class=\"meta\">@Bean</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createSingleMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        System.out.println(<span class=\"string\">\"singleDataSource\"</span>);</div><div class=\"line\">        MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">        mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.single\"</span>);</div><div class=\"line\">        mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"singleSqlSessionFactory\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：<a href=\"https://github.com/yany8060/SpringDemo.git\" target=\"_blank\" rel=\"external\">https://github.com/yany8060/SpringDemo.git</a><br>com.yany.dao.single中编写单属于的到接口<br>com.yany.mapper.single中编写对应的sql的xml</p>\n<hr>\n<p>由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码<br>详情请见：<a href=\"https://github.com/yany8060/SpringDemo.git\" target=\"_blank\" rel=\"external\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz\" target=\"_blank\" rel=\"external\">http://yany8060.xyz</a></p>\n","excerpt":"","more":"<blockquote>\n<p>本例使用mysql作为数据库，使用druid作为数据库连接池<br>主要有单数据源和多数据源实例<br>多数据源中又分为：1. 分包形式 2. aop形式 3. 注解形式</p>\n</blockquote>\n<h3 id=\"项目目录结构\"><a href=\"#项目目录结构\" class=\"headerlink\" title=\"项目目录结构\"></a>项目目录结构</h3><p><img src=\"http://upload-images.jianshu.io/upload_images/1419542-fa4ab411fc0cd9a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/324\" alt=\"catalog.png\"></p>\n<h3 id=\"MyBatis配置实现\"><a href=\"#MyBatis配置实现\" class=\"headerlink\" title=\"MyBatis配置实现\"></a>MyBatis配置实现</h3><blockquote>\n<p>springBoot相比于原来的Spring的模式就是减少xml配置，将它们用java代码实现。</p>\n</blockquote>\n<ol>\n<li>DataSource的bean，主要配置数据来源</li>\n<li>SqlSessionFactoryBean的bean，引用 datasource，MyBatis配置，sql的xml扫描，以及各个插件的添加</li>\n<li>MapperScannerConfigurer的bean的，主要设置基本扫描包，引用SqlSessionFactoryBean</li>\n<li>DataSourceTransactionManager的bean，主要用设置事务</li>\n</ol>\n<h3 id=\"添加maven依赖\"><a href=\"#添加maven依赖\" class=\"headerlink\" title=\"添加maven依赖\"></a>添加maven依赖</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;!-- aop --&gt;        </div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.8.4&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;!-- dataSource start --&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.2.0&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;mysql&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;5.1.38&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;druid&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;1.0.20&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;pagehelper&lt;/artifactId&gt;</div><div class=\"line\">    &lt;version&gt;5.0.0&lt;/version&gt;</div><div class=\"line\">&lt;/dependency&gt;</div><div class=\"line\">&lt;!-- dataSource end --&gt;</div></pre></td></tr></table></figure>\n<h3 id=\"单数据源\"><a href=\"#单数据源\" class=\"headerlink\" title=\"单数据源\"></a>单数据源</h3><h4 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h4><p>在application.yml中添加datasource配置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring:</div><div class=\"line\">  application:</div><div class=\"line\">    name: SpringBoot</div><div class=\"line\">  datasource:</div><div class=\"line\">    url: jdbc:mysql://localhost:3306/YanYPro?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=false</div><div class=\"line\">    username: root</div><div class=\"line\">    password: *****</div><div class=\"line\">    driver-class-name: com.mysql.jdbc.Driver</div><div class=\"line\">    # 使用druid数据源</div><div class=\"line\">    type: com.alibaba.druid.pool.DruidDataSource</div><div class=\"line\">    # 初始化大小，最小，最大</div><div class=\"line\">    initialSize: 5</div><div class=\"line\">    minIdle: 5</div><div class=\"line\">    maxActive: 20</div><div class=\"line\">    # 配置获取连接等待超时的时间</div><div class=\"line\">    maxWait: 60000</div><div class=\"line\">    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒</div><div class=\"line\">    timeBetweenEvictionRunsMillis: 60000</div><div class=\"line\">    # 配置一个连接在池中最小生存的时间，单位是毫秒</div><div class=\"line\">    minEvictableIdleTimeMillis: 300000</div><div class=\"line\">    validationQuery: SELECT 1 FROM DUAL</div><div class=\"line\">    testWhileIdle: true</div><div class=\"line\">    testOnBorrow: false</div><div class=\"line\">    testOnReturn: false</div><div class=\"line\">    # 打开PSCache，并且指定每个连接上PSCache的大小</div><div class=\"line\">    poolPreparedStatements: true</div><div class=\"line\">    maxPoolPreparedStatementPerConnectionSize: 20</div><div class=\"line\">    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&apos;wall&apos;用于防火墙</div><div class=\"line\">    filters: stat,wall,log4j</div><div class=\"line\">    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录</div><div class=\"line\">    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000</div><div class=\"line\">    # 合并多个DruidDataSource的监控数据</div><div class=\"line\">    useGlobalDataSourceStat: true</div></pre></td></tr></table></figure></p>\n<p>配置mybatis-config:<br>以下只是实例，可自定义添加一些别的配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;</div><div class=\"line\">        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;</div><div class=\"line\">&lt;configuration&gt;</div><div class=\"line\">    &lt;!-- 全局参数 --&gt;</div><div class=\"line\">    &lt;settings&gt;</div><div class=\"line\">        &lt;!-- 使全局的映射器启用或禁用缓存。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 全局启用或禁用延迟加载。当禁用时，所有关联对象都会即时加载。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 当启用时，有延迟加载属性的对象在被调用时将会完全加载任意属性。否则，每种属性将会按需要加载。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 是否允许单条sql 返回多个数据集  (取决于驱动的兼容性) default:true --&gt;</div><div class=\"line\">        &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 是否可以使用列的别名 (取决于驱动的兼容性) default:true --&gt;</div><div class=\"line\">        &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 允许JDBC 生成主键。需要驱动器支持。如果设为了true，这个设置将强制使用被生成的主键，有一些驱动器不兼容不过仍然可以执行。  default:false  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 指定 MyBatis 如何自动映射 数据基表的列 NONE：不隐射　PARTIAL:部分  FULL:全部  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt;</div><div class=\"line\">        &lt;!-- 这是默认的执行类型  （SIMPLE: 简单； REUSE: 执行器可能重复使用prepared statements语句；BATCH: 执行器可以重复执行语句和批量更新）  --&gt;</div><div class=\"line\">        &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt;</div><div class=\"line\">        &lt;!-- 使用驼峰命名法转换字段。 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt;</div><div class=\"line\">        &lt;!-- 设置本地缓存范围 session:就会有数据的共享  statement:语句范围 (这样就不会有数据的共享 ) defalut:session --&gt;</div><div class=\"line\">        &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;</div><div class=\"line\">        &lt;!-- 设置但JDBC类型为空时,某些驱动程序 要指定值,default:OTHER，插入空值时不需要指定类型 --&gt;</div><div class=\"line\">        &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;NULL&quot;/&gt;</div><div class=\"line\">    &lt;/settings&gt;</div><div class=\"line\">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>\n<h4 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h4><h5 id=\"SingleMyBatisConfig的类实现\"><a href=\"#SingleMyBatisConfig的类实现\" class=\"headerlink\" title=\"SingleMyBatisConfig的类实现\"></a>SingleMyBatisConfig的类实现</h5><p>继承EnvironmentAware并实现setEnvironment，为了获取默认配置文件application.yml的元素。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SingleMyBatisConfig</span> <span class=\"keyword\">implements</span> <span class=\"title\">EnvironmentAware</span></span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> Logger logger = LoggerFactory.getLogger(SingleMyBatisConfig.class);</div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String MYBATIS_CONFIG = <span class=\"string\">\"mybatis-config.xml\"</span>;</div><div class=\"line\">    <span class=\"comment\">//mybatis mapper resource 路径</span></div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> String MAPPER_PATH = <span class=\"string\">\"classpath:/com/yany/mapper/single/**.xml\"</span>;</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"keyword\">private</span> RelaxedPropertyResolver propertyResolver;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">setEnvironment</span><span class=\"params\">(Environment environment)</span> </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>.propertyResolver = <span class=\"keyword\">new</span> RelaxedPropertyResolver(environment, <span class=\"string\">\"spring.datasource.\"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">  .......</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleDataSource”)，设置实现DataSource的bean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * <span class=\"doctag\">@return</span></div><div class=\"line\"> * <span class=\"doctag\">@Primary</span> 优先方案，被注解的实现，优先被注入</div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"meta\">@Primary</span></div><div class=\"line\"><span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleDataSource\"</span>)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DataSource <span class=\"title\">singleDataSource</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    logger.info(<span class=\"string\">\"datasource url:&#123;&#125;\"</span>, propertyResolver.getProperty(<span class=\"string\">\"url\"</span>));</div><div class=\"line\"></div><div class=\"line\">    DruidDataSource datasource = <span class=\"keyword\">new</span> DruidDataSource();</div><div class=\"line\">    datasource.setUrl(propertyResolver.getProperty(<span class=\"string\">\"url\"</span>));</div><div class=\"line\">    datasource.setDriverClassName(propertyResolver.getProperty(<span class=\"string\">\"driver-class-name\"</span>));</div><div class=\"line\">    datasource.setUsername(propertyResolver.getProperty(<span class=\"string\">\"username\"</span>));</div><div class=\"line\">    datasource.setPassword(propertyResolver.getProperty(<span class=\"string\">\"password\"</span>));</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    datasource.setInitialSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"initialSize\"</span>)));</div><div class=\"line\">    datasource.setMinIdle(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"minIdle\"</span>)));</div><div class=\"line\">    datasource.setMaxWait(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxWait\"</span>)));</div><div class=\"line\">    datasource.setMaxActive(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxActive\"</span>)));</div><div class=\"line\">    datasource.setTimeBetweenEvictionRunsMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"timeBetweenEvictionRunsMillis\"</span>)));</div><div class=\"line\">    datasource.setMinEvictableIdleTimeMillis(Long.valueOf(propertyResolver.getProperty(<span class=\"string\">\"minEvictableIdleTimeMillis\"</span>)));</div><div class=\"line\">    datasource.setValidationQuery(propertyResolver.getProperty(<span class=\"string\">\"validationQuery\"</span>));</div><div class=\"line\">    datasource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testWhileIdle\"</span>)));</div><div class=\"line\">    datasource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testOnBorrow\"</span>)));</div><div class=\"line\">    datasource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"testOnReturn\"</span>)));</div><div class=\"line\">    datasource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty(<span class=\"string\">\"poolPreparedStatements\"</span>)));</div><div class=\"line\">    datasource.setMaxPoolPreparedStatementPerConnectionSize(Integer.valueOf(propertyResolver.getProperty(<span class=\"string\">\"maxPoolPreparedStatementPerConnectionSize\"</span>)));</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        datasource.setFilters(propertyResolver.getProperty(<span class=\"string\">\"filters\"</span>));</div><div class=\"line\">    &#125; <span class=\"keyword\">catch</span> (SQLException e) &#123;</div><div class=\"line\">        e.printStackTrace();</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> datasource;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleSqlSessionFactory”)，设置实现SqlSessionFactoryBean<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">     * 创建sqlSessionFactory实例</div><div class=\"line\">     *</div><div class=\"line\">     * <span class=\"doctag\">@return</span></div><div class=\"line\">     */</span></div><div class=\"line\">    <span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleSqlSessionFactory\"</span>)</div><div class=\"line\">    <span class=\"meta\">@Primary</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> SqlSessionFactoryBean <span class=\"title\">createSqlSessionFactoryBean</span><span class=\"params\">(@Qualifier(<span class=\"string\">\"singleDataSource\"</span>)</span> DataSource singleDataSource) <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">        SqlSessionFactoryBean sqlSessionFactoryBean = <span class=\"keyword\">new</span> SqlSessionFactoryBean();</div><div class=\"line\">        <span class=\"comment\">//设置mybatis configuration 扫描路径</span></div><div class=\"line\">        sqlSessionFactoryBean.setConfigLocation(<span class=\"keyword\">new</span> ClassPathResource(MYBATIS_CONFIG));</div><div class=\"line\">        sqlSessionFactoryBean.setDataSource(singleDataSource);</div><div class=\"line\"></div><div class=\"line\">        PathMatchingResourcePatternResolver pathMatchingResourcePatternResolver = <span class=\"keyword\">new</span> PathMatchingResourcePatternResolver();</div><div class=\"line\">        sqlSessionFactoryBean.setMapperLocations(pathMatchingResourcePatternResolver.getResources(MAPPER_PATH));</div><div class=\"line\">        <span class=\"keyword\">return</span> sqlSessionFactoryBean;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure></p>\n<p>添加@Bean(name = “singleTransactionManager”)，设置实现事务DataSourceTransactionManager<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * 配置事务管理器</div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"meta\">@Bean</span>(name = <span class=\"string\">\"singleTransactionManager\"</span>)</div><div class=\"line\"><span class=\"meta\">@Primary</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> DataSourceTransactionManager <span class=\"title\">transactionManager</span><span class=\"params\">(@Qualifier(<span class=\"string\">\"singleDataSource\"</span>)</span> DataSource singleDataSource) <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> DataSourceTransactionManager(singleDataSource);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上SingleMyBatisConfig的配置完成，上面主要配置了数据源、SqlSessionFactoryBean、事务（DataSourceTransactionManager）。还差一个MapperScannerConfigurer的配置。</p>\n<h4 id=\"MapperScannerConfig\"><a href=\"#MapperScannerConfig\" class=\"headerlink\" title=\"MapperScannerConfig\"></a>MapperScannerConfig</h4><p>本来主要集中实现各个数据源的MapperScannerConfigurer<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@Configuration</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapperScannerConfig</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</div><div class=\"line\">     * 单数据源配置</div><div class=\"line\">     *</div><div class=\"line\">     * <span class=\"doctag\">@return</span></div><div class=\"line\">     */</span></div><div class=\"line\">    <span class=\"meta\">@Bean</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> MapperScannerConfigurer <span class=\"title\">createSingleMapperScannerConfigurer</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">        System.out.println(<span class=\"string\">\"singleDataSource\"</span>);</div><div class=\"line\">        MapperScannerConfigurer mapperScannerConfigurer = <span class=\"keyword\">new</span> MapperScannerConfigurer();</div><div class=\"line\">        mapperScannerConfigurer.setBasePackage(<span class=\"string\">\"com.yany.dao.single\"</span>);</div><div class=\"line\">        mapperScannerConfigurer.setSqlSessionFactoryBeanName(<span class=\"string\">\"singleSqlSessionFactory\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> mapperScannerConfigurer;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上即为单数据源使用配置，而具体的dao层的编写以及sql的xml编写详情见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>com.yany.dao.single中编写单属于的到接口<br>com.yany.mapper.single中编写对应的sql的xml</p>\n<hr>\n<p>由于贴了比较多的代码，在下一篇多数据中中将直接类比这篇中的代码<br>详情请见：<a href=\"https://github.com/yany8060/SpringDemo.git\">https://github.com/yany8060/SpringDemo.git</a><br>博客：<a href=\"http://yany8060.xyz\">http://yany8060.xyz</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ciz42l2je0000bc0v2xticoxc","category_id":"ciz42l2jn0004bc0v4w6g8n0y","_id":"ciz42l2ka000dbc0vzpxgtvh0"},{"post_id":"ciz42l2jk0002bc0v5m7t3hjv","category_id":"ciz42l2k70009bc0vriq1twrb","_id":"ciz42l2kg000kbc0vlos1bx1z"},{"post_id":"ciz42l2ka000ebc0v6gw89ooi","category_id":"ciz42l2k70009bc0vriq1twrb","_id":"ciz42l2ki000nbc0vnl9x7554"},{"post_id":"ciz42l2kd000hbc0vh8djtkco","category_id":"ciz42l2k70009bc0vriq1twrb","_id":"ciz42l2kj000pbc0v3d0ufxwi"},{"post_id":"ciz42l2js0006bc0va23xrgtt","category_id":"ciz42l2k70009bc0vriq1twrb","_id":"ciz42l2kj000sbc0v76nfcuk4"},{"post_id":"ciz42l2kf000jbc0v4ii6qhr3","category_id":"ciz42l2k70009bc0vriq1twrb","_id":"ciz42l2kk000ubc0varimy382"},{"post_id":"ciz42l2ju0007bc0vfbdhpurk","category_id":"ciz42l2kh000lbc0v2xyg5uif","_id":"ciz42l2kk000xbc0vx17qe7yn"},{"post_id":"ciz42l2jw0008bc0vq3s19b2p","category_id":"ciz42l2kh000lbc0v2xyg5uif","_id":"ciz42l2kk000zbc0v1ddgsf6c"},{"post_id":"ciz42l2k8000cbc0vn8zwi6rg","category_id":"ciz42l2kh000lbc0v2xyg5uif","_id":"ciz42l2km0011bc0vjo5cul41"}],"PostTag":[{"post_id":"ciz42l2je0000bc0v2xticoxc","tag_id":"ciz42l2jr0005bc0vsay1ezee","_id":"ciz42l2k8000bbc0voxbzt1dx"},{"post_id":"ciz42l2jk0002bc0v5m7t3hjv","tag_id":"ciz42l2k7000abc0vzzg2iwtr","_id":"ciz42l2ke000ibc0vf8j5e4ub"},{"post_id":"ciz42l2js0006bc0va23xrgtt","tag_id":"ciz42l2k7000abc0vzzg2iwtr","_id":"ciz42l2ki000obc0vih4g12r2"},{"post_id":"ciz42l2ju0007bc0vfbdhpurk","tag_id":"ciz42l2kh000mbc0vmr148wue","_id":"ciz42l2kj000tbc0v0mif6hi9"},{"post_id":"ciz42l2jw0008bc0vq3s19b2p","tag_id":"ciz42l2kh000mbc0vmr148wue","_id":"ciz42l2kk000ybc0vgd5qd36f"},{"post_id":"ciz42l2k8000cbc0vn8zwi6rg","tag_id":"ciz42l2kk000wbc0v0pqh2s50","_id":"ciz42l2ko0013bc0vhrwtdyw3"},{"post_id":"ciz42l2k8000cbc0vn8zwi6rg","tag_id":"ciz42l2kh000mbc0vmr148wue","_id":"ciz42l2ko0014bc0vc200sv38"},{"post_id":"ciz42l2ka000ebc0v6gw89ooi","tag_id":"ciz42l2kn0012bc0vlv4soa8s","_id":"ciz42l2ko0017bc0vrfmq0ato"},{"post_id":"ciz42l2ka000ebc0v6gw89ooi","tag_id":"ciz42l2ko0015bc0vs9z9d5av","_id":"ciz42l2ko0018bc0vp4qdk2vb"},{"post_id":"ciz42l2kd000hbc0vh8djtkco","tag_id":"ciz42l2kn0012bc0vlv4soa8s","_id":"ciz42l2kp001bbc0vsptfbhq7"},{"post_id":"ciz42l2kd000hbc0vh8djtkco","tag_id":"ciz42l2ko0015bc0vs9z9d5av","_id":"ciz42l2kp001cbc0vx539du3i"},{"post_id":"ciz42l2kf000jbc0v4ii6qhr3","tag_id":"ciz42l2kn0012bc0vlv4soa8s","_id":"ciz42l2kq001ebc0vt3ya0lhj"},{"post_id":"ciz42l2kf000jbc0v4ii6qhr3","tag_id":"ciz42l2ko0015bc0vs9z9d5av","_id":"ciz42l2kq001fbc0vzxxv484k"}],"Tag":[{"name":"kafka","_id":"ciz42l2jr0005bc0vsay1ezee"},{"name":"java-nio","_id":"ciz42l2k7000abc0vzzg2iwtr"},{"name":"spark","_id":"ciz42l2kh000mbc0vmr148wue"},{"name":"sparkstream","_id":"ciz42l2kk000wbc0v0pqh2s50"},{"name":"spring-boot","_id":"ciz42l2kn0012bc0vlv4soa8s"},{"name":"spring","_id":"ciz42l2ko0015bc0vs9z9d5av"}]}}